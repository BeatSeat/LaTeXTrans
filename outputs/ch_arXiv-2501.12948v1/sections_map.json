[
    {
        "section": 0,
        "content": "\\documentclass[11pt, a4paper, logo, copyright, nonumbering]{deepseek}\n\\usepackage[authoryear, sort&compress, round]{natbib}\n\\usepackage{dblfloatfix}\n\\usepackage{ulem}\n\\usepackage{caption}\n\\usepackage{dramatist}\n\\usepackage{xspace}\n\\usepackage{pifont}\n\\usepackage{multirow}\n\\usepackage{tcolorbox}\n\\usepackage{xltabular}\n\\usepackage{longtable}\n\\usepackage{hyperref}\n\\interfootnotelinepenalty=10000\n\n\\usepackage{amsfonts}\n\\usepackage{amsmath}\n\\usepackage{amssymb}\n\\usepackage{lineno}\n\\usepackage{multirow}\n\\usepackage{adjustbox}\n\n\\usepackage[bottom]{footmisc}\n\n\\usepackage{CJKutf8}\n\\usepackage{subfigure}\n\\usepackage{setspace}\n\n\\usepackage{dsfont}\n\\usepackage{array}\n\\usepackage{tabularx}\n\\usepackage{subfigure}\n\\usepackage{xcolor}\n\\usepackage{tabularx}\n\\usepackage{booktabs}\n\n\\usepackage{lipsum}\n\\usepackage{multicol}\n\n\\makeatletter\n\\def\\@BTrule[#1]{\n  \\ifx\\longtable\\undefined\n    \\let\\@BTswitch\\@BTnormal\n  \\else\\ifx\\hline\\LT@hline\n    \\nobreak\n    \\let\\@BTswitch\\@BLTrule\n  \\else\n     \\let\\@BTswitch\\@BTnormal\n  \\fi\\fi\n  \\global\\@thisrulewidth=#1\\relax\n  \\ifnum\\@thisruleclass=\\tw@\\vskip\\@aboverulesep\\else\n  \\ifnum\\@lastruleclass=\\z@\\vskip\\@aboverulesep\\else\n  \\ifnum\\@lastruleclass=\\@ne\\vskip\\doublerulesep\\fi\\fi\\fi\n  \\@BTswitch}\n\\makeatother\n\n\\addto\\extrasenglish{\n    <PLACEHOLDER_NEWCOMMAND_0>\n    <PLACEHOLDER_NEWCOMMAND_1>\n    <PLACEHOLDER_NEWCOMMAND_2>\n    <PLACEHOLDER_NEWCOMMAND_3>\n    <PLACEHOLDER_NEWCOMMAND_4>\n    <PLACEHOLDER_NEWCOMMAND_5>\n    <PLACEHOLDER_NEWCOMMAND_6>\n}\n\n\\newenvironment{indentchunk}[1]\n {<PLACEHOLDER_ENV_1>}\n\n\\bibliographystyle{abbrvnat}\n\n\\reportnumber{001}\n\n<PLACEHOLDER_NEWCOMMAND_7>\n\n<PLACEHOLDER_CAP_1>\n\n\\author[*]{\nDeepSeek-AI\n\\\\\n\\small\n\\texttt{research@deepseek.com}\n}\n<PLACEHOLDER_commands.tex_begin><PLACEHOLDER_NEWCOMMAND_8>\n<PLACEHOLDER_NEWCOMMAND_9>\n<PLACEHOLDER_NEWCOMMAND_10>\n<PLACEHOLDER_NEWCOMMAND_11>\n<PLACEHOLDER_NEWCOMMAND_12>\n<PLACEHOLDER_NEWCOMMAND_13>\n<PLACEHOLDER_NEWCOMMAND_14>\n<PLACEHOLDER_NEWCOMMAND_15>\n<PLACEHOLDER_NEWCOMMAND_16>\n<PLACEHOLDER_NEWCOMMAND_17>\n<PLACEHOLDER_NEWCOMMAND_18>\n<PLACEHOLDER_NEWCOMMAND_19>\n<PLACEHOLDER_NEWCOMMAND_20>\n<PLACEHOLDER_NEWCOMMAND_21>\n<PLACEHOLDER_NEWCOMMAND_22>\n<PLACEHOLDER_NEWCOMMAND_23>\n<PLACEHOLDER_NEWCOMMAND_24>\n<PLACEHOLDER_NEWCOMMAND_25>\n<PLACEHOLDER_NEWCOMMAND_26>\n<PLACEHOLDER_NEWCOMMAND_27>\n<PLACEHOLDER_NEWCOMMAND_28>\n<PLACEHOLDER_NEWCOMMAND_29>\n<PLACEHOLDER_NEWCOMMAND_30>\n<PLACEHOLDER_NEWCOMMAND_31>\n<PLACEHOLDER_NEWCOMMAND_32>\n<PLACEHOLDER_NEWCOMMAND_33>\n<PLACEHOLDER_NEWCOMMAND_34>\n<PLACEHOLDER_NEWCOMMAND_35>\n<PLACEHOLDER_NEWCOMMAND_36>\n<PLACEHOLDER_NEWCOMMAND_37>\n<PLACEHOLDER_NEWCOMMAND_38>\n<PLACEHOLDER_NEWCOMMAND_39>\n<PLACEHOLDER_NEWCOMMAND_40>\n<PLACEHOLDER_NEWCOMMAND_41>\n<PLACEHOLDER_NEWCOMMAND_42>\n<PLACEHOLDER_NEWCOMMAND_43>\n<PLACEHOLDER_NEWCOMMAND_44>\n\n<PLACEHOLDER_NEWCOMMAND_45>\n\n<PLACEHOLDER_NEWCOMMAND_46>\n\n<PLACEHOLDER_NEWCOMMAND_47>\n<PLACEHOLDER_NEWCOMMAND_48>\n<PLACEHOLDER_NEWCOMMAND_49>\n<PLACEHOLDER_NEWCOMMAND_50>\n<PLACEHOLDER_NEWCOMMAND_51>\n<PLACEHOLDER_NEWCOMMAND_52>\n<PLACEHOLDER_NEWCOMMAND_53>\n<PLACEHOLDER_NEWCOMMAND_54>\n<PLACEHOLDER_NEWCOMMAND_55>\n<PLACEHOLDER_NEWCOMMAND_56>\n<PLACEHOLDER_NEWCOMMAND_57>\n<PLACEHOLDER_NEWCOMMAND_58>\n<PLACEHOLDER_NEWCOMMAND_59>\n<PLACEHOLDER_NEWCOMMAND_60>\n<PLACEHOLDER_NEWCOMMAND_61>\n<PLACEHOLDER_NEWCOMMAND_62>\n<PLACEHOLDER_NEWCOMMAND_63>\n<PLACEHOLDER_NEWCOMMAND_64>\n<PLACEHOLDER_NEWCOMMAND_65>\n<PLACEHOLDER_NEWCOMMAND_66>\n<PLACEHOLDER_NEWCOMMAND_67>\n<PLACEHOLDER_NEWCOMMAND_68>\n<PLACEHOLDER_NEWCOMMAND_69>\n<PLACEHOLDER_NEWCOMMAND_70>\n<PLACEHOLDER_NEWCOMMAND_71>\n<PLACEHOLDER_NEWCOMMAND_72>\n<PLACEHOLDER_NEWCOMMAND_73>\n<PLACEHOLDER_NEWCOMMAND_74>\n<PLACEHOLDER_NEWCOMMAND_75>\n<PLACEHOLDER_NEWCOMMAND_76>\n<PLACEHOLDER_NEWCOMMAND_77>\n<PLACEHOLDER_NEWCOMMAND_78>\n<PLACEHOLDER_NEWCOMMAND_79>\n<PLACEHOLDER_NEWCOMMAND_80>\n<PLACEHOLDER_NEWCOMMAND_81>\n<PLACEHOLDER_NEWCOMMAND_82>\n<PLACEHOLDER_NEWCOMMAND_83>\n<PLACEHOLDER_NEWCOMMAND_84>\n<PLACEHOLDER_NEWCOMMAND_85>\n<PLACEHOLDER_NEWCOMMAND_86>\n<PLACEHOLDER_NEWCOMMAND_87>\n<PLACEHOLDER_NEWCOMMAND_88>\n<PLACEHOLDER_NEWCOMMAND_89>\n\n<PLACEHOLDER_NEWCOMMAND_90>\n<PLACEHOLDER_NEWCOMMAND_91>\n<PLACEHOLDER_NEWCOMMAND_92>\n<PLACEHOLDER_NEWCOMMAND_93>\n<PLACEHOLDER_NEWCOMMAND_94>\n<PLACEHOLDER_NEWCOMMAND_95>\n<PLACEHOLDER_NEWCOMMAND_96>\n<PLACEHOLDER_NEWCOMMAND_97>\n<PLACEHOLDER_NEWCOMMAND_98>\n<PLACEHOLDER_NEWCOMMAND_99>\n<PLACEHOLDER_NEWCOMMAND_100>\n<PLACEHOLDER_NEWCOMMAND_101>\n<PLACEHOLDER_NEWCOMMAND_102>\n\n<PLACEHOLDER_NEWCOMMAND_103>\n<PLACEHOLDER_NEWCOMMAND_104>\n<PLACEHOLDER_NEWCOMMAND_105>\n<PLACEHOLDER_NEWCOMMAND_106>\n<PLACEHOLDER_NEWCOMMAND_107>\n<PLACEHOLDER_NEWCOMMAND_108>\n<PLACEHOLDER_NEWCOMMAND_109>\n<PLACEHOLDER_NEWCOMMAND_110>\n<PLACEHOLDER_NEWCOMMAND_111>\n<PLACEHOLDER_NEWCOMMAND_112>\n\n<PLACEHOLDER_NEWCOMMAND_113>\n\\let\\Re\\relax\n\n\\DeclareMathOperator{\\Re}{\\Rr e}\n\\DeclareMathOperator{\\Imag}{\\Ii m}\n\\DeclareMathOperator{\\Ker}{Ker}\n\\DeclareMathOperator{\\Hom}{Hom}\n\\DeclareMathOperator{\\End}{End}\n\\DeclareMathOperator{\\tr}{tr}\n\\DeclareMathOperator{\\Tr}{Tr}\n\\DeclareMathOperator{\\Supp}{Supp}\n\\DeclareMathOperator{\\Sign}{Sign}\n\\let\\Im\\relax\n\\DeclareMathOperator{\\Im}{Im}\n\\DeclareMathOperator{\\Corr}{Corr}\n\\DeclareMathOperator{\\sign}{sign}\n\\DeclareMathOperator{\\supp}{supp}\n\n\\DeclareMathOperator{\\cas}{cas}\n\\DeclareMathOperator{\\sinc}{sinc}\n\\DeclareMathOperator{\\cotan}{cotan}\n\\DeclareMathOperator{\\Card}{Card}\n\\DeclareMathOperator{\\GCD}{GCD}\n\\DeclareMathOperator{\\grad}{grad}\n\\DeclareMathOperator{\\Diag}{Diag}\n\\DeclareMathOperator{\\rank}{rank}\n\\DeclareMathOperator{\\conv}{conv}\n\\DeclareMathOperator{\\interop}{int}\n\n<PLACEHOLDER_NEWCOMMAND_114>\n\n<PLACEHOLDER_NEWCOMMAND_115>\n<PLACEHOLDER_NEWCOMMAND_116>\n<PLACEHOLDER_NEWCOMMAND_117>\n<PLACEHOLDER_NEWCOMMAND_118>\n<PLACEHOLDER_NEWCOMMAND_119>\n<PLACEHOLDER_NEWCOMMAND_120>\n<PLACEHOLDER_NEWCOMMAND_121>\n<PLACEHOLDER_NEWCOMMAND_122>\n<PLACEHOLDER_NEWCOMMAND_123>\n<PLACEHOLDER_NEWCOMMAND_124>\n<PLACEHOLDER_NEWCOMMAND_125>\n<PLACEHOLDER_NEWCOMMAND_126>\n<PLACEHOLDER_NEWCOMMAND_127>\n<PLACEHOLDER_NEWCOMMAND_128>\n<PLACEHOLDER_NEWCOMMAND_129>\n<PLACEHOLDER_NEWCOMMAND_130>\n<PLACEHOLDER_NEWCOMMAND_131>\n<PLACEHOLDER_NEWCOMMAND_132>\n<PLACEHOLDER_NEWCOMMAND_133>\n\n<PLACEHOLDER_NEWCOMMAND_134>\n<PLACEHOLDER_NEWCOMMAND_135>\n<PLACEHOLDER_NEWCOMMAND_136>\n<PLACEHOLDER_NEWCOMMAND_137>\n<PLACEHOLDER_NEWCOMMAND_138>\n<PLACEHOLDER_NEWCOMMAND_139>\n<PLACEHOLDER_NEWCOMMAND_140>\n<PLACEHOLDER_NEWCOMMAND_141>\n<PLACEHOLDER_NEWCOMMAND_142>\n\n<PLACEHOLDER_NEWCOMMAND_143>\n<PLACEHOLDER_NEWCOMMAND_144>\n<PLACEHOLDER_NEWCOMMAND_145>\n<PLACEHOLDER_NEWCOMMAND_146>\n<PLACEHOLDER_NEWCOMMAND_147>\n<PLACEHOLDER_NEWCOMMAND_148>\n<PLACEHOLDER_NEWCOMMAND_149>\n<PLACEHOLDER_NEWCOMMAND_150>\n<PLACEHOLDER_NEWCOMMAND_151>\n<PLACEHOLDER_NEWCOMMAND_152>\n<PLACEHOLDER_NEWCOMMAND_153>\n\\DeclareMathOperator{\\diverg}{div}\n\\DeclareMathOperator{\\Prox}{Prox}\n<PLACEHOLDER_NEWCOMMAND_154>\n<PLACEHOLDER_NEWCOMMAND_155>\n<PLACEHOLDER_NEWCOMMAND_156>\n<PLACEHOLDER_NEWCOMMAND_157>\n<PLACEHOLDER_NEWCOMMAND_158>\n<PLACEHOLDER_NEWCOMMAND_159>\n<PLACEHOLDER_NEWCOMMAND_160>\n\n\\newcommand{\\func}[4]{ {\\left\\{  <PLACEHOLDER_ENV_2>  \\right.} }\n<PLACEHOLDER_NEWCOMMAND_161>\n<PLACEHOLDER_NEWCOMMAND_162>\n\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\n<PLACEHOLDER_NEWCOMMAND_163>\n<PLACEHOLDER_NEWCOMMAND_164>\n\\newcommand{\\choice}[1]{\n\t\\left\\{\n\t\t<PLACEHOLDER_ENV_3>\n\t\\right. }\n<PLACEHOLDER_NEWCOMMAND_165>\n<PLACEHOLDER_NEWCOMMAND_166>\n\n<PLACEHOLDER_NEWCOMMAND_167>\n<PLACEHOLDER_NEWCOMMAND_168>\n<PLACEHOLDER_NEWCOMMAND_169>\n<PLACEHOLDER_NEWCOMMAND_170>\n<PLACEHOLDER_NEWCOMMAND_171>\n<PLACEHOLDER_NEWCOMMAND_172>\n<PLACEHOLDER_NEWCOMMAND_173>\n<PLACEHOLDER_NEWCOMMAND_174>\n<PLACEHOLDER_NEWCOMMAND_175>\n<PLACEHOLDER_NEWCOMMAND_176>\n<PLACEHOLDER_NEWCOMMAND_177>\n<PLACEHOLDER_NEWCOMMAND_178>\n<PLACEHOLDER_NEWCOMMAND_179>\n<PLACEHOLDER_NEWCOMMAND_180>\n<PLACEHOLDER_NEWCOMMAND_181>\n<PLACEHOLDER_NEWCOMMAND_182>\n<PLACEHOLDER_NEWCOMMAND_183>\n<PLACEHOLDER_NEWCOMMAND_184>\n<PLACEHOLDER_NEWCOMMAND_185>\n<PLACEHOLDER_NEWCOMMAND_186>\n\n\\newlength{\\restsubwidth}\n\\newlength{\\restsubheight}\n\\newlength{\\restsubmoreheight}\n\\setlength{\\restsubmoreheight}{4pt}\n<PLACEHOLDER_NEWCOMMAND_187><PLACEHOLDER_commands.tex_end>\n\n<PLACEHOLDER_NEWCOMMAND_188>\n<PLACEHOLDER_NEWCOMMAND_189>\n<PLACEHOLDER_NEWCOMMAND_190>\n<PLACEHOLDER_NEWCOMMAND_191>\n<PLACEHOLDER_NEWCOMMAND_192>\n\n<PLACEHOLDER_ENV_4>",
        "trans_content": "\\documentclass[11pt, a4paper, logo, copyright, nonumbering]{deepseek}\n\\usepackage[authoryear, sort&compress, round]{natbib}\n\\usepackage{dblfloatfix}\n\\usepackage{ulem}\n\\usepackage{caption}\n\\usepackage{dramatist}\n\\usepackage{xspace}\n\\usepackage{pifont}\n\\usepackage{multirow}\n\\usepackage{tcolorbox}\n\\usepackage{xltabular}\n\\usepackage{longtable}\n\\usepackage{hyperref}\n\\interfootnotelinepenalty=10000\n\n\\usepackage{amsfonts}\n\\usepackage{amsmath}\n\\usepackage{amssymb}\n\\usepackage{lineno}\n\\usepackage{multirow}\n\\usepackage{adjustbox}\n\n\\usepackage[bottom]{footmisc}\n\n\\usepackage{CJKutf8}\n\\usepackage{subfigure}\n\\usepackage{setspace}\n\n\\usepackage{dsfont}\n\\usepackage{array}\n\\usepackage{tabularx}\n\\usepackage{subfigure}\n\\usepackage{xcolor}\n\\usepackage{tabularx}\n\\usepackage{booktabs}\n\n\\usepackage{lipsum}\n\\usepackage{multicol}\n\n\\makeatletter\n\\def\\@BTrule[#1]{\n  \\ifx\\longtable\\undefined\n    \\let\\@BTswitch\\@BTnormal\n  \\else\\ifx\\hline\\LT@hline\n    \\nobreak\n    \\let\\@BTswitch\\@BLTrule\n  \\else\n     \\let\\@BTswitch\\@BTnormal\n  \\fi\\fi\n  \\global\\@thisrulewidth=#1\\relax\n  \\ifnum\\@thisruleclass=\\tw@\\vskip\\@aboverulesep\\else\n  \\ifnum\\@lastruleclass=\\z@\\vskip\\@aboverulesep\\else\n  \\ifnum\\@lastruleclass=\\@ne\\vskip\\doublerulesep\\fi\\fi\\fi\n  \\@BTswitch}\n\\makeatother\n\n\\addto\\extrasenglish{\n    <PLACEHOLDER_NEWCOMMAND_0>\n    <PLACEHOLDER_NEWCOMMAND_1>\n    <PLACEHOLDER_NEWCOMMAND_2>\n    <PLACEHOLDER_NEWCOMMAND_3>\n    <PLACEHOLDER_NEWCOMMAND_4>\n    <PLACEHOLDER_NEWCOMMAND_5>\n    <PLACEHOLDER_NEWCOMMAND_6>\n}\n\n\\newenvironment{indentchunk}[1]\n {<PLACEHOLDER_ENV_1>}\n\n\\bibliographystyle{abbrvnat}\n\n\\reportnumber{001}\n\n<PLACEHOLDER_NEWCOMMAND_7>\n\n<PLACEHOLDER_CAP_1>\n\n\\author[*]{\nDeepSeek-AI\n\\\\\n\\small\n\\texttt{research@deepseek.com}\n}\n<PLACEHOLDER_commands.tex_begin><PLACEHOLDER_NEWCOMMAND_8>\n<PLACEHOLDER_NEWCOMMAND_9>\n<PLACEHOLDER_NEWCOMMAND_10>\n<PLACEHOLDER_NEWCOMMAND_11>\n<PLACEHOLDER_NEWCOMMAND_12>\n<PLACEHOLDER_NEWCOMMAND_13>\n<PLACEHOLDER_NEWCOMMAND_14>\n<PLACEHOLDER_NEWCOMMAND_15>\n<PLACEHOLDER_NEWCOMMAND_16>\n<PLACEHOLDER_NEWCOMMAND_17>\n<PLACEHOLDER_NEWCOMMAND_18>\n<PLACEHOLDER_NEWCOMMAND_19>\n<PLACEHOLDER_NEWCOMMAND_20>\n<PLACEHOLDER_NEWCOMMAND_21>\n<PLACEHOLDER_NEWCOMMAND_22>\n<PLACEHOLDER_NEWCOMMAND_23>\n<PLACEHOLDER_NEWCOMMAND_24>\n<PLACEHOLDER_NEWCOMMAND_25>\n<PLACEHOLDER_NEWCOMMAND_26>\n<PLACEHOLDER_NEWCOMMAND_27>\n<PLACEHOLDER_NEWCOMMAND_28>\n<PLACEHOLDER_NEWCOMMAND_29>\n<PLACEHOLDER_NEWCOMMAND_30>\n<PLACEHOLDER_NEWCOMMAND_31>\n<PLACEHOLDER_NEWCOMMAND_32>\n<PLACEHOLDER_NEWCOMMAND_33>\n<PLACEHOLDER_NEWCOMMAND_34>\n<PLACEHOLDER_NEWCOMMAND_35>\n<PLACEHOLDER_NEWCOMMAND_36>\n<PLACEHOLDER_NEWCOMMAND_37>\n<PLACEHOLDER_NEWCOMMAND_38>\n<PLACEHOLDER_NEWCOMMAND_39>\n<PLACEHOLDER_NEWCOMMAND_40>\n<PLACEHOLDER_NEWCOMMAND_41>\n<PLACEHOLDER_NEWCOMMAND_42>\n<PLACEHOLDER_NEWCOMMAND_43>\n<PLACEHOLDER_NEWCOMMAND_44>\n\n<PLACEHOLDER_NEWCOMMAND_45>\n\n<PLACEHOLDER_NEWCOMMAND_46>\n\n<PLACEHOLDER_NEWCOMMAND_47>\n<PLACEHOLDER_NEWCOMMAND_48>\n<PLACEHOLDER_NEWCOMMAND_49>\n<PLACEHOLDER_NEWCOMMAND_50>\n<PLACEHOLDER_NEWCOMMAND_51>\n<PLACEHOLDER_NEWCOMMAND_52>\n<PLACEHOLDER_NEWCOMMAND_53>\n<PLACEHOLDER_NEWCOMMAND_54>\n<PLACEHOLDER_NEWCOMMAND_55>\n<PLACEHOLDER_NEWCOMMAND_56>\n<PLACEHOLDER_NEWCOMMAND_57>\n<PLACEHOLDER_NEWCOMMAND_58>\n<PLACEHOLDER_NEWCOMMAND_59>\n<PLACEHOLDER_NEWCOMMAND_60>\n<PLACEHOLDER_NEWCOMMAND_61>\n<PLACEHOLDER_NEWCOMMAND_62>\n<PLACEHOLDER_NEWCOMMAND_63>\n<PLACEHOLDER_NEWCOMMAND_64>\n<PLACEHOLDER_NEWCOMMAND_65>\n<PLACEHOLDER_NEWCOMMAND_66>\n<PLACEHOLDER_NEWCOMMAND_67>\n<PLACEHOLDER_NEWCOMMAND_68>\n<PLACEHOLDER_NEWCOMMAND_69>\n<PLACEHOLDER_NEWCOMMAND_70>\n<PLACEHOLDER_NEWCOMMAND_71>\n<PLACEHOLDER_NEWCOMMAND_72>\n<PLACEHOLDER_NEWCOMMAND_73>\n<PLACEHOLDER_NEWCOMMAND_74>\n<PLACEHOLDER_NEWCOMMAND_75>\n<PLACEHOLDER_NEWCOMMAND_76>\n<PLACEHOLDER_NEWCOMMAND_77>\n<PLACEHOLDER_NEWCOMMAND_78>\n<PLACEHOLDER_NEWCOMMAND_79>\n<PLACEHOLDER_NEWCOMMAND_80>\n<PLACEHOLDER_NEWCOMMAND_81>\n<PLACEHOLDER_NEWCOMMAND_82>\n<PLACEHOLDER_NEWCOMMAND_83>\n<PLACEHOLDER_NEWCOMMAND_84>\n<PLACEHOLDER_NEWCOMMAND_85>\n<PLACEHOLDER_NEWCOMMAND_86>\n<PLACEHOLDER_NEWCOMMAND_87>\n<PLACEHOLDER_NEWCOMMAND_88>\n<PLACEHOLDER_NEWCOMMAND_89>\n\n<PLACEHOLDER_NEWCOMMAND_90>\n<PLACEHOLDER_NEWCOMMAND_91>\n<PLACEHOLDER_NEWCOMMAND_92>\n<PLACEHOLDER_NEWCOMMAND_93>\n<PLACEHOLDER_NEWCOMMAND_94>\n<PLACEHOLDER_NEWCOMMAND_95>\n<PLACEHOLDER_NEWCOMMAND_96>\n<PLACEHOLDER_NEWCOMMAND_97>\n<PLACEHOLDER_NEWCOMMAND_98>\n<PLACEHOLDER_NEWCOMMAND_99>\n<PLACEHOLDER_NEWCOMMAND_100>\n<PLACEHOLDER_NEWCOMMAND_101>\n<PLACEHOLDER_NEWCOMMAND_102>\n\n<PLACEHOLDER_NEWCOMMAND_103>\n<PLACEHOLDER_NEWCOMMAND_104>\n<PLACEHOLDER_NEWCOMMAND_105>\n<PLACEHOLDER_NEWCOMMAND_106>\n<PLACEHOLDER_NEWCOMMAND_107>\n<PLACEHOLDER_NEWCOMMAND_108>\n<PLACEHOLDER_NEWCOMMAND_109>\n<PLACEHOLDER_NEWCOMMAND_110>\n<PLACEHOLDER_NEWCOMMAND_111>\n<PLACEHOLDER_NEWCOMMAND_112>\n\n<PLACEHOLDER_NEWCOMMAND_113>\n\\let\\Re\\relax\n\n\\DeclareMathOperator{\\Re}{\\Rr e}\n\\DeclareMathOperator{\\Imag}{\\Ii m}\n\\DeclareMathOperator{\\Ker}{Ker}\n\\DeclareMathOperator{\\Hom}{Hom}\n\\DeclareMathOperator{\\End}{End}\n\\DeclareMathOperator{\\tr}{tr}\n\\DeclareMathOperator{\\Tr}{Tr}\n\\DeclareMathOperator{\\Supp}{Supp}\n\\DeclareMathOperator{\\Sign}{Sign}\n\\let\\Im\\relax\n\\DeclareMathOperator{\\Im}{Im}\n\\DeclareMathOperator{\\Corr}{Corr}\n\\DeclareMathOperator{\\sign}{sign}\n\\DeclareMathOperator{\\supp}{supp}\n\n\\DeclareMathOperator{\\cas}{cas}\n\\DeclareMathOperator{\\sinc}{sinc}\n\\DeclareMathOperator{\\cotan}{cotan}\n\\DeclareMathOperator{\\Card}{Card}\n\\DeclareMathOperator{\\GCD}{GCD}\n\\DeclareMathOperator{\\grad}{grad}\n\\DeclareMathOperator{\\Diag}{Diag}\n\\DeclareMathOperator{\\rank}{rank}\n\\DeclareMathOperator{\\conv}{conv}\n\\DeclareMathOperator{\\interop}{int}\n\n<PLACEHOLDER_NEWCOMMAND_114>\n\n<PLACEHOLDER_NEWCOMMAND_115>\n<PLACEHOLDER_NEWCOMMAND_116>\n<PLACEHOLDER_NEWCOMMAND_117>\n<PLACEHOLDER_NEWCOMMAND_118>\n<PLACEHOLDER_NEWCOMMAND_119>\n<PLACEHOLDER_NEWCOMMAND_120>\n<PLACEHOLDER_NEWCOMMAND_121>\n<PLACEHOLDER_NEWCOMMAND_122>\n<PLACEHOLDER_NEWCOMMAND_123>\n<PLACEHOLDER_NEWCOMMAND_124>\n<PLACEHOLDER_NEWCOMMAND_125>\n<PLACEHOLDER_NEWCOMMAND_126>\n<PLACEHOLDER_NEWCOMMAND_127>\n<PLACEHOLDER_NEWCOMMAND_128>\n<PLACEHOLDER_NEWCOMMAND_129>\n<PLACEHOLDER_NEWCOMMAND_130>\n<PLACEHOLDER_NEWCOMMAND_131>\n<PLACEHOLDER_NEWCOMMAND_132>\n<PLACEHOLDER_NEWCOMMAND_133>\n\n<PLACEHOLDER_NEWCOMMAND_134>\n<PLACEHOLDER_NEWCOMMAND_135>\n<PLACEHOLDER_NEWCOMMAND_136>\n<PLACEHOLDER_NEWCOMMAND_137>\n<PLACEHOLDER_NEWCOMMAND_138>\n<PLACEHOLDER_NEWCOMMAND_139>\n<PLACEHOLDER_NEWCOMMAND_140>\n<PLACEHOLDER_NEWCOMMAND_141>\n<PLACEHOLDER_NEWCOMMAND_142>\n\n<PLACEHOLDER_NEWCOMMAND_143>\n<PLACEHOLDER_NEWCOMMAND_144>\n<PLACEHOLDER_NEWCOMMAND_145>\n<PLACEHOLDER_NEWCOMMAND_146>\n<PLACEHOLDER_NEWCOMMAND_147>\n<PLACEHOLDER_NEWCOMMAND_148>\n<PLACEHOLDER_NEWCOMMAND_149>\n<PLACEHOLDER_NEWCOMMAND_150>\n<PLACEHOLDER_NEWCOMMAND_151>\n<PLACEHOLDER_NEWCOMMAND_152>\n<PLACEHOLDER_NEWCOMMAND_153>\n\\DeclareMathOperator{\\diverg}{div}\n\\DeclareMathOperator{\\Prox}{Prox}\n<PLACEHOLDER_NEWCOMMAND_154>\n<PLACEHOLDER_NEWCOMMAND_155>\n<PLACEHOLDER_NEWCOMMAND_156>\n<PLACEHOLDER_NEWCOMMAND_157>\n<PLACEHOLDER_NEWCOMMAND_158>\n<PLACEHOLDER_NEWCOMMAND_159>\n<PLACEHOLDER_NEWCOMMAND_160>\n\n\\newcommand{\\func}[4]{ {\\left\\{  <PLACEHOLDER_ENV_2>  \\right.} }\n<PLACEHOLDER_NEWCOMMAND_161>\n<PLACEHOLDER_NEWCOMMAND_162>\n\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\n<PLACEHOLDER_NEWCOMMAND_163>\n<PLACEHOLDER_NEWCOMMAND_164>\n\\newcommand{\\choice}[1]{\n\t\\left\\{\n\t\t<PLACEHOLDER_ENV_3>\n\t\\right. }\n<PLACEHOLDER_NEWCOMMAND_165>\n<PLACEHOLDER_NEWCOMMAND_166>\n\n<PLACEHOLDER_NEWCOMMAND_167>\n<PLACEHOLDER_NEWCOMMAND_168>\n<PLACEHOLDER_NEWCOMMAND_169>\n<PLACEHOLDER_NEWCOMMAND_170>\n<PLACEHOLDER_NEWCOMMAND_171>\n<PLACEHOLDER_NEWCOMMAND_172>\n<PLACEHOLDER_NEWCOMMAND_173>\n<PLACEHOLDER_NEWCOMMAND_174>\n<PLACEHOLDER_NEWCOMMAND_175>\n<PLACEHOLDER_NEWCOMMAND_176>\n<PLACEHOLDER_NEWCOMMAND_177>\n<PLACEHOLDER_NEWCOMMAND_178>\n<PLACEHOLDER_NEWCOMMAND_179>\n<PLACEHOLDER_NEWCOMMAND_180>\n<PLACEHOLDER_NEWCOMMAND_181>\n<PLACEHOLDER_NEWCOMMAND_182>\n<PLACEHOLDER_NEWCOMMAND_183>\n<PLACEHOLDER_NEWCOMMAND_184>\n<PLACEHOLDER_NEWCOMMAND_185>\n<PLACEHOLDER_NEWCOMMAND_186>\n\n\\newlength{\\restsubwidth}\n\\newlength{\\restsubheight}\n\\newlength{\\restsubmoreheight}\n\\setlength{\\restsubmoreheight}{4pt}\n<PLACEHOLDER_NEWCOMMAND_187><PLACEHOLDER_commands.tex_end>\n\n<PLACEHOLDER_NEWCOMMAND_188>\n<PLACEHOLDER_NEWCOMMAND_189>\n<PLACEHOLDER_NEWCOMMAND_190>\n<PLACEHOLDER_NEWCOMMAND_191>\n<PLACEHOLDER_NEWCOMMAND_192>\n\n<PLACEHOLDER_ENV_4>"
    },
    {
        "section": 0,
        "content": "\\begin{document}\n\n\\maketitle\n<PLACEHOLDER_ENV_5>\n\n\\newpage\n\n<PLACEHOLDER_ENV_6>\n\n\\newpage",
        "trans_content": "\\begin{document}\n\n\\maketitle\n<PLACEHOLDER_ENV_5>\n\n\\newpage\n\n<PLACEHOLDER_ENV_6>\n\n\\newpage"
    },
    {
        "section": "1",
        "content": "\\section{Introduction}\nIn recent years, Large Language Models~(LLMs) have been undergoing rapid iteration and evolution~\\citep{gpt4o,claude35sonnet,gemini1_5}, progressively diminishing the gap towards Artificial General Intelligence~(AGI).\n\nRecently, post-training has emerged as an important component of the full training pipeline. It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. In the context of reasoning capabilities, OpenAI's o1~\\citep{o1} series models were the first to introduce inference-time scaling by increasing the length of the Chain-of-Thought reasoning process. This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge of effective test-time scaling remains an open question for the research community.\nSeveral prior works have explored various approaches, including process-based reward models \\citep{uesato2022solving, lightman2023let,mathshepherd}, reinforcement learning \\citep{kumar2024training}, and search algorithms such as Monte Carlo Tree Search and Beam Search \\citep{feng2024alphazeroliketreesearchguidelarge,xin2024deepseekproverv15harnessingproofassistant,AlphaGeometryTrinh2024}.\nHowever, none of these methods has achieved general reasoning performance comparable to OpenAI's o1 series models.\n\nIn this paper, we take the first step toward improving language model reasoning capabilities using pure reinforcement learning (RL).\nOur goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process.\nSpecifically, we use DeepSeek-V3-Base as the base model and employ GRPO~\\citep{deepseekmath} as the RL framework to improve model performance in reasoning.\nDuring training, \\dsro{} naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, \\dsro{} exhibits super performance on reasoning benchmarks.\nFor instance, the pass@1 score on AIME 2024 increases from 15.6\\% to 71.0\\%, and with majority voting, the score further improves to 86.7\\%, matching the performance of OpenAI-o1-0912.\n\nHowever, \\dsro{} encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce \\dsri{}, which incorporates a small amount of cold-start data and a multi-stage training pipeline.\nSpecifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like \\dsro{}. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model. After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as \\dsri{}, which achieves performance on par with OpenAI-o1-1217.\n\nWe further explore distillation from \\dsri{} to smaller dense models. Using Qwen2.5-32B~\\citep{qwen2_5} as the base model, direct distillation from \\dsri{} outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are crucial for improving reasoning capabilities.  We open-source the distilled Qwen and Llama~\\citep{llama3} series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview~\\citep{QwQ} by a large margin, and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models.",
        "trans_content": "\\section{介绍}\n近年来，大型语言模型（LLMs）正在快速迭代和演化~\\citep{gpt4o,claude35sonnet,gemini1_5}，逐步缩小与人工通用智能（AGI）之间的差距。\n\n最近，后训练已成为完整训练流程中的重要组成部分。研究表明，它在推理任务上提高了准确性，与社会价值观对齐，并适应用户偏好，同时与预训练相比需要相对较少的计算资源。在推理能力的背景下，OpenAI的o1~\\citep{o1}系列模型率先通过增加思维链推理过程的长度引入了推理时间扩展。这种方法在各种推理任务中取得了显著的改进，如数学、编码和科学推理。然而，有效测试时间扩展的挑战仍然是研究界的一个未解问题。若干先前的工作探索了各种方法，包括基于过程的奖励模型\\citep{uesato2022solving, lightman2023let,mathshepherd}、强化学习\\citep{kumar2024training}和搜索算法，如蒙特卡罗树搜索和束搜索\\citep{feng2024alphazeroliketreesearchguidelarge,xin2024deepseekproverv15harnessingproofassistant,AlphaGeometryTrinh2024}。然而，这些方法中没有一个能达到与OpenAI的o1系列模型相当的一般推理性能。\n\n在本文中，我们首次尝试使用纯强化学习（RL）来提高语言模型的推理能力。我们的目标是探索LLMs在不使用任何监督数据的情况下，通过纯RL过程自我进化发展推理能力的潜力。具体而言，我们使用DeepSeek-V3-Base作为基础模型，并采用GRPO~\\citep{deepseekmath}作为RL框架来提高模型在推理方面的表现。在训练过程中，\\dsro{}自然涌现出许多强大而有趣的推理行为。经过数千次RL步骤，\\dsro{}在推理基准上表现出超强性能。例如，AIME 2024的pass@1得分从15.6\\%提高到71.0\\%，并且通过多数投票，得分进一步提高到86.7\\%，匹配OpenAI-o1-0912的表现。\n\n然而，\\dsro{}遇到了一些挑战，如可读性差和语言混合。为了应对这些问题并进一步提高推理性能，我们引入了\\dsri{}，该方法结合了少量的冷启动数据和多阶段训练流程。具体而言，我们首先收集数千条冷启动数据以微调DeepSeek-V3-Base模型。随后，我们执行类似于\\dsro{}的推理导向RL。当RL过程接近收敛时，我们通过在RL检查点上进行拒绝采样创建新的SFT数据，并结合来自DeepSeek-V3的监督数据（如写作、事实问答和自我认知领域），然后重新训练DeepSeek-V3-Base模型。在使用新数据微调后，检查点经历了额外的RL过程，考虑到所有场景的提示。经过这些步骤，我们获得了一个称为\\dsri{}的检查点，其性能可与OpenAI-o1-1217媲美。\n\n我们进一步探索了从\\dsri{}到更小密集模型的蒸馏。以Qwen2.5-32B~\\citep{qwen2_5}作为基础模型，直接从\\dsri{}进行蒸馏优于在其上应用RL。这表明较大基础模型发现的推理模式对于提高推理能力至关重要。我们开源了蒸馏的Qwen和Llama~\\citep{llama3}系列。值得注意的是，我们的蒸馏14B模型远远优于最先进的开源QwQ-32B-Preview~\\citep{QwQ}，而蒸馏的32B和70B模型在密集模型中的推理基准上创下了新纪录。"
    },
    {
        "section": "1_1",
        "content": "\\subsection{Contributions}\n\n\\paragraph{Post-Training: Large-Scale Reinforcement Learning on the Base Model}\n<PLACEHOLDER_ENV_7>\n\n\\paragraph{Distillation: Smaller Models Can Be Powerful Too}\n<PLACEHOLDER_ENV_8>",
        "trans_content": "\\subsection{贡献}\n\n\\paragraph{后训练：在基础模型上进行大规模强化学习}\n<PLACEHOLDER_ENV_7>\n\n\\paragraph{蒸馏：小模型也可以很强大}\n<PLACEHOLDER_ENV_8>"
    },
    {
        "section": "1_2",
        "content": "\\subsection{Summary of Evaluation Results}\n<PLACEHOLDER_ENV_9>",
        "trans_content": "\\subsection{评估结果总结}\n<PLACEHOLDER_ENV_9>"
    },
    {
        "section": "2",
        "content": "\\section{Approach}",
        "trans_content": "\\section{方法}"
    },
    {
        "section": "2_1",
        "content": "\\subsection{Overview}\nPrevious work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) \\dsro{}, which applies RL directly to the base model without any SFT data, and (2) \\dsri{}, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from \\dsri{} to small dense models.",
        "trans_content": "\\subsection{概述}\n之前的工作在很大程度上依赖于大量监督数据来提升模型性能。在这项研究中，我们展示了即使不使用监督微调（SFT）作为冷启动，通过大规模强化学习（RL）也可以显著提高推理能力。此外，通过包含少量冷启动数据可以进一步增强性能。在接下来的部分中，我们介绍：(1) \\dsro{}，它在没有任何 SFT 数据的情况下直接将 RL 应用于基础模型，(2) \\dsri{}，它从经过数千个长链式思维（CoT）示例微调的检查点开始应用 RL，(3) 将 \\dsri{} 的推理能力提炼到小型密集模型中。"
    },
    {
        "section": "2_2",
        "content": "\\subsection{ \\dsro{}: Reinforcement Learning on the Base Model}\n\\label{sec:ds-zero}\nReinforcement learning has demonstrated significant effectiveness in reasoning tasks, as evidenced by our previous works \\citep{mathshepherd,deepseekmath}.\nHowever, these works heavily depended on supervised data, which are time-intensive to gather.\nIn this section, we explore the potential of LLMs to develop reasoning capabilities \\textbf{without any supervised data}, focusing on their self-evolution through a pure reinforcement learning process.\nWe start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights.",
        "trans_content": "\\subsection{ \\dsro{}: 基于模型的强化学习}\n\\label{sec:ds-zero}\n强化学习在推理任务中展示了显著的效果，这在我们之前的研究中得到了证实 \\citep{mathshepherd,deepseekmath}。\n然而，这些研究严重依赖于监督数据，而这些数据的收集需要大量时间。\n在本节中，我们探讨大型语言模型在\\textbf{没有任何监督数据}情况下发展推理能力的潜力，重点关注其通过纯粹的强化学习过程的自我进化。\n我们首先简要概述我们的强化学习算法，然后展示一些激动人心的结果，并希望这能为学术界提供宝贵的见解。"
    },
    {
        "section": "2_2_1",
        "content": "\\subsubsection{Reinforcement Learning Algorithm}\n\\paragraph{Group Relative Policy Optimization} In order to save the training costs of RL, we adopt Group Relative Policy Optimization~(GRPO) \\citep{deepseekmath}, which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead.\nSpecifically, for each question $q$, GRPO samples a group of outputs $\\{o_1, o_2, \\cdots, o_G\\}$ from the old policy $\\pi_{\\theta_{old}}$ and then optimizes the policy model $\\pi_{\\theta}$ by maximizing the following objective:\n<PLACEHOLDER_ENV_10>\n<PLACEHOLDER_ENV_11>\nwhere $\\epsilon$ and $\\beta$ are hyper-parameters, and $A_i$ is the advantage, computed using a group of rewards $\\{r_1, r_2, \\ldots, r_G\\}$ corresponding to the outputs within each group:\n<PLACEHOLDER_ENV_12>\n\n<PLACEHOLDER_ENV_13>",
        "trans_content": "\\subsubsection{强化学习算法}\n\\paragraph{群体相对策略优化} 为了节省强化学习的训练成本，我们采用群体相对策略优化（GRPO） \\citep{deepseekmath}，该方法放弃了通常与策略模型大小相同的评论模型，而是从群体分数中估计基线。\n具体而言，对于每个问题 $q$，GRPO 从旧策略 $\\pi_{\\theta_{old}}$ 中采样一组输出 $\\{o_1, o_2, \\cdots, o_G\\}$，然后通过最大化以下目标来优化策略模型 $\\pi_{\\theta}$：\n<PLACEHOLDER_ENV_10>\n<PLACEHOLDER_ENV_11>\n其中 $\\epsilon$ 和 $\\beta$ 是超参数，$A_i$ 是优势，通过一组对应于每组输出的奖励 $\\{r_1, r_2, \\ldots, r_G\\}$ 计算得到：\n<PLACEHOLDER_ENV_12>\n\n<PLACEHOLDER_ENV_13>"
    },
    {
        "section": "2_2_2",
        "content": "\\subsubsection{Reward Modeling} The reward is the source of the training signal, which decides the optimization direction of RL.\nTo train \\dsro{}, we adopt a rule-based reward system that mainly consists of two types of rewards:\n<PLACEHOLDER_ENV_14>\nWe do not apply the outcome or process neural reward model in developing  \\dsro{}, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline.",
        "trans_content": "\\subsubsection{奖励建模} 奖励是训练信号的来源，决定了强化学习的优化方向。为了训练 \\dsro{}，我们采用了一种基于规则的奖励系统，主要由两种类型的奖励组成：\n<PLACEHOLDER_ENV_14>\n我们在开发 \\dsro{} 时没有应用结果或过程神经奖励模型，因为我们发现神经奖励模型在大规模强化学习过程中可能会遭遇奖励作弊，并且重新训练奖励模型需要额外的训练资源，复杂化了整个训练流程。"
    },
    {
        "section": "2_2_3",
        "content": "\\subsubsection{Training Template}\n\nTo train \\dsro{}, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions. As depicted in Table \\ref{tab:r0_template}, this template requires \\dsro{} to first produce a reasoning process, followed by the final answer. We intentionally limit our constraints to this structural format, avoiding any content-specific biases—such as mandating reflective reasoning or promoting particular problem-solving strategies—to ensure that we can accurately observe the model's natural progression during the RL process.",
        "trans_content": "\\subsubsection{训练模板}\n\n为了训练 \\dsro{}，我们首先设计了一个简单的模板，引导基础模型遵循我们指定的指令。如表 \\ref{tab:r0_template} 所示，该模板要求 \\dsro{} 首先产生一个推理过程，然后给出最终答案。我们故意将约束限制在这种结构格式上，避免任何内容特定的偏见，例如强制反思性推理或促进特定问题解决策略，以确保我们能够准确观察模型在强化学习过程中的自然进展。"
    },
    {
        "section": "2_2_4",
        "content": "\\subsubsection{Performance, Self-evolution Process and Aha Moment of \\dsro{}}\n\n\\paragraph{Performance of \\dsro{}}\n\n<PLACEHOLDER_ENV_15>\n\n<PLACEHOLDER_ENV_16>\n\nFigure \\ref{fig:zero-training-performance} depicts the performance trajectory of \\dsro{} on the AIME 2024 benchmark throughout the RL training process. As illustrated, \\dsro{} demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6\\% to an impressive 71.0\\%, reaching performance levels comparable to OpenAI-o1-0912. This significant improvement highlights the efficacy of our RL algorithm in optimizing the model's performance over time.\n\nTable \\ref{tab:r1-zero} provides a comparative analysis between \\dsro{} and OpenAI's o1-0912 models across a variety of reasoning-related benchmarks. The findings reveal that RL empowers \\dsro{} to attain robust reasoning capabilities without the need for any supervised fine-tuning data.\nThis is a noteworthy achievement, as it underscores the model's ability to learn and generalize effectively through RL alone. Additionally, the performance of \\dsro{} can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, \\dsro{}'s performance escalates from 71.0\\% to 86.7\\%, thereby exceeding the performance of OpenAI-o1-0912.\nThe ability of \\dsro{} to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks.\n\n<PLACEHOLDER_ENV_17>\n\n\\paragraph{Self-evolution Process of \\dsro{}}\nThe self-evolution process of \\dsro{} is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the model's progression without the influence of the supervised fine-tuning stage. This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks.\n\nAs depicted in Figure \\ref{fig:zero-training-length}, the thinking time of \\dsro{} shows consistent improvement throughout the training process. This improvement is not the result of external adjustments but rather an intrinsic development within the model. \\dsro{} naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time computation. This computation ranges from generating hundreds to thousands of reasoning tokens, allowing the model to explore and refine its thought processes in greater depth.\n\nOne of the most remarkable aspects of this self-evolution is the emergence of sophisticated behaviors as the test-time computation increases. Behaviors such as reflection—where the model revisits and reevaluates its previous steps—and the exploration of alternative approaches to problem-solving arise spontaneously. These behaviors are not explicitly programmed but instead emerge as a result of the model's interaction with the reinforcement learning environment. This spontaneous development significantly enhances \\dsro{}'s reasoning capabilities, enabling it to tackle more challenging tasks with greater efficiency and accuracy.\n\n\\paragraph{Aha Moment of \\dsro{}}\nA particularly intriguing phenomenon observed during the training of \\dsro{} is the occurrence of an ``aha moment''. This moment, as illustrated in Table \\ref{tab:aha_moment}, occurs in an intermediate version of the model. During this phase, \\dsro{} learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the model's growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes.\n\nThis moment is not only an ``aha moment'' for the model but also for the researchers observing its behavior. It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies.\nThe ``aha moment'' serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artificial systems, paving the way for more autonomous and adaptive models in the future.\n\n\\paragraph{Drawback of \\dsro{}}\nAlthough \\dsro{} exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, \\dsro{} struggles with challenges like poor readability, and language mixing. To make reasoning processes more readable and share them with the open community, we explore \\dsri{}, a method that utilizes RL with human-friendly cold-start data.\n\n<PLACEHOLDER_ENV_18>",
        "trans_content": "\\subsubsection{\\dsro{} 的性能、自我进化过程和顿悟时刻}\n\n\\paragraph{\\dsro{} 的性能}\n\n<PLACEHOLDER_ENV_15>\n\n<PLACEHOLDER_ENV_16>\n\n图 \\ref{fig:zero-training-performance} 描绘了 \\dsro{} 在整个 RL 训练过程中在 AIME 2024 基准测试上的性能轨迹。如图所示，随着 RL 训练的推进，\\dsro{} 的性能表现出稳定而一致的提升。值得注意的是，AIME 2024 的平均 pass@1 分数显著增加，从最初的 15.6\\% 跃升至令人印象深刻的 71.0\\%，达到了与 OpenAI-o1-0912 相当的性能水平。这一显著提升突出了我们 RL 算法在优化模型性能方面的有效性。\n\n表 \\ref{tab:r1-zero} 提供了 \\dsro{} 与 OpenAI 的 o1-0912 模型在各种推理相关基准测试上的比较分析。研究结果表明，RL 使 \\dsro{} 能够在无需任何监督微调数据的情况下实现强大的推理能力。这是一个值得注意的成就，因为它强调了模型通过 RL 单独学习和有效泛化的能力。此外，\\dsro{} 的性能可以通过应用多数投票进一步增强。例如，当在 AIME 基准测试上采用多数投票时，\\dsro{} 的性能从 71.0\\% 上升到 86.7\\%，从而超过了 OpenAI-o1-0912 的性能。\n\\dsro{} 能够在有无多数投票的情况下实现如此竞争力的表现，突出了其强大的基础能力及其在推理任务中进一步进步的潜力。\n\n<PLACEHOLDER_ENV_17>\n\n\\paragraph{\\dsro{} 的自我进化过程}\n\\dsro{} 的自我进化过程是一个引人入胜的示范，展示了 RL 如何驱动模型自主改善其推理能力。通过直接从基础模型启动 RL，我们可以在没有监督微调阶段影响的情况下密切监测模型的进展。此方法提供了对模型随时间演变的清晰视角，尤其是在处理复杂推理任务的能力方面。\n\n如图 \\ref{fig:zero-training-length} 所示，\\dsro{} 的思考时间在整个训练过程中显示出持续改进。这种改进并非外部调整的结果，而是模型内在发展的结果。\\dsro{} 自然地通过利用扩展的测试时间计算来获取解决日益复杂推理任务的能力。这种计算范围从生成数百到数千个推理符号，使模型能够更深入地探索和完善其思考过程。\n\n这种自我进化最显著的方面之一是随着测试时间计算的增加而出现的复杂行为。诸如反思——模型重新审视并重新评估其先前步骤——以及探索替代问题解决方法等行为自发出现。这些行为不是明确编程的，而是模型与强化学习环境互动的结果。这种自发发展显著增强了 \\dsro{} 的推理能力，使其能够更高效、更准确地应对更具挑战性的任务。\n\n\\paragraph{\\dsro{} 的顿悟时刻}\n在 \\dsro{} 的训练过程中观察到的一个特别引人入胜的现象是“顿悟时刻”的出现。这个时刻，如表 \\ref{tab:aha_moment} 所示，发生在模型的中间版本。在此阶段，\\dsro{} 通过重新评估其初始方法来学习为问题分配更多思考时间。这种行为不仅是模型推理能力增强的证明，也是一个迷人的例子，展示了强化学习如何导致意想不到和复杂的结果。\n\n这一时刻不仅是模型的“顿悟时刻”，也是研究人员观察其行为的“顿悟时刻”。它强调了强化学习的力量和美丽：我们不是明确教导模型如何解决问题，而只是为其提供正确的激励，它就会自主开发高级问题解决策略。\n“顿悟时刻”是一个强有力的提醒，展示了 RL 解锁人工系统新智能水平的潜力，为未来更自主和适应性更强的模型铺平了道路。\n\n\\paragraph{\\dsro{} 的缺陷}\n尽管 \\dsro{} 展现了强大的推理能力，并自主开发了意想不到且强大的推理行为，它仍面临几个问题。例如，\\dsro{} 的挑战包括可读性差以及语言混合。为了使推理过程更具可读性并与开放社群分享，我们探索了 \\dsri{}，这是一种利用 RL 与人类友好冷启动数据的方法。\n\n<PLACEHOLDER_ENV_18>"
    },
    {
        "section": "2_3",
        "content": "\\subsection{\\dsri{}: Reinforcement Learning with Cold Start}\nInspired by the promising results of \\dsro{}, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities?\nTo address these questions, we design a pipeline to train \\dsri{}. The pipeline consists of four stages, outlined as follows.",
        "trans_content": "\\subsection{\\dsri{}: 冷启动下的强化学习}\n受益于\\dsro{}的良好结果，我们自然会提出两个问题：1）通过引入少量高质量数据作为冷启动，能否进一步提高推理性能或加速收敛？2）我们如何训练一个不仅能生成清晰连贯的思维链（CoT），而且表现出强大通用能力的用户友好型模型？\n为了解决这些问题，我们设计了一个用于训练\\dsri{}的流程。该流程包括以下四个阶段。"
    },
    {
        "section": "2_3_1",
        "content": "\\subsubsection{Cold Start}\nUnlike \\dsro{}, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor.\nTo collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering \\dsro{} outputs in a readable format, and refining the results through post-processing by human annotators.\n\nIn this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL.\nCompared to \\dsro{}, the advantages of cold start data include:\n<PLACEHOLDER_ENV_19>",
        "trans_content": "\\subsubsection{冷启动}\n与 \\dsro{} 不同，为了防止 RL 训练初期不稳定的冷启动阶段在基础模型中发生，对于 DeepSeek-R1，我们构建并收集少量长 CoT 数据以微调模型作为初始 RL actor。\n为了收集这些数据，我们探索了几种方法：使用少量示例提示以长 CoT 作为例子，直接提示模型生成带有反思和验证的详细答案，以可读格式收集 \\dsro{} 的输出，并通过人工注释者进行后处理以改进结果。\n\n在这项工作中，我们收集了数千个冷启动数据以微调 DeepSeek-V3-Base 作为 RL 的起始点。\n与 \\dsro{} 相比，冷启动数据的优势包括：\n<PLACEHOLDER_ENV_19>"
    },
    {
        "section": "2_3_2",
        "content": "\\subsubsection{Reasoning-oriented Reinforcement Learning}\nAfter fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in \\dsro. This phase focuses on enhancing the model's reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the model's performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks.",
        "trans_content": "\\subsubsection{面向推理的强化学习}\n在对DeepSeek-V3-Base进行冷启动数据微调之后，我们应用与\\dsro相同的大规模强化学习训练过程。此阶段重点在于增强模型的推理能力，特别是在推理密集型任务中，如编程、数学、科学和逻辑推理，这些任务涉及具有明确解决方案的明确定义的问题。在训练过程中，我们观察到当强化学习提示涉及多种语言时，CoT经常会出现语言混合。为缓解语言混合问题，我们在强化学习训练过程中引入了语言一致性奖励，该奖励计算为CoT中目标语言词汇的比例。尽管消融实验显示这种对齐会导致模型性能的轻微下降，但该奖励符合人类偏好，使其更具可读性。最后，我们通过直接相加推理任务的准确性和语言一致性奖励来形成最终奖励。然后，我们在微调的模型上应用强化学习训练，直到其在推理任务上达到收敛。"
    },
    {
        "section": "2_3_3",
        "content": "\\subsubsection{Rejection Sampling and Supervised Fine-Tuning}\nWhen reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the model's capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below.\n\\label{sec:method:r1:sft}\n\n\\paragraph{Reasoning data}\nWe curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training.\nIn the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment.\nAdditionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long parapraphs, and code blocks.\nFor each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples.\n\n\\paragraph{Non-Reasoning data}\nFor non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as ``hello'' we do not provide a CoT in response.\nIn the end, we collected a total of approximately 200k training samples that are unrelated to reasoning.\n\nWe fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples.",
        "trans_content": "\\subsubsection{拒绝采样与监督微调}\n当面向推理的强化学习（RL）收敛时，我们利用所得的检查点收集下一轮的监督微调（SFT）数据。与最初的冷启动数据主要关注推理不同，这一阶段结合了其他领域的数据，以提升模型在写作、角色扮演和其他通用任务中的能力。具体而言，我们按照以下描述生成数据并微调模型。\n\\label{sec:method:r1:sft}\n\n\\paragraph{推理数据}\n我们通过从上述RL训练的检查点执行拒绝采样来策划推理提示并生成推理轨迹。在前一阶段，我们仅包括可以使用基于规则的奖励进行评估的数据。然而，在这一阶段，我们通过加入额外的数据扩展了数据集，其中一些数据通过将真实值和模型预测输入DeepSeek-V3进行判断，使用生成的奖励模型。此外，由于模型输出有时混乱且难以阅读，我们过滤掉了使用混合语言、长段落和代码块的思维链。对于每个提示，我们采样多个响应并仅保留正确的响应。总共，我们收集了大约60万条与推理相关的训练样本。\n\n\\paragraph{非推理数据}\n对于非推理数据，如写作、事实问答、自我认知和翻译，我们采用DeepSeek-V3管道并重用DeepSeek-V3的部分SFT数据集。对于某些非推理任务，我们调用DeepSeek-V3在回答问题前，通过提示生成潜在的思维链。然而，对于简单的查询，如“hello”，我们不提供思维链响应。最终，我们收集了大约20万条与推理无关的训练样本。\n\n我们使用上述策划的数据集（约80万样本）对DeepSeek-V3-Base进行两次迭代的微调。"
    },
    {
        "section": "2_3_4",
        "content": "\\subsubsection{Reinforcement Learning for all Scenarios}\n\nTo further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model's helpfulness and harmlessness while simultaneously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions.\nFor reasoning data, we adhere to the methodology outlined in \\dsro, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains.\nFor general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process.\nUltimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness.",
        "trans_content": "\\subsubsection{所有场景的强化学习}\n\n为了进一步使模型与人类偏好保持一致，我们实施了一个次级强化学习阶段，旨在提高模型的有用性和无害性，同时改进其推理能力。具体而言，我们使用奖励信号和多样化的提示分布的组合来训练模型。\n对于推理数据，我们遵循 \\dsro 中概述的方法，该方法利用基于规则的奖励来引导数学、代码和逻辑推理领域的学习过程。\n对于一般数据，我们依靠奖励模型在复杂和细微的场景中捕捉人类偏好。我们在 DeepSeek-V3 管道的基础上，采用类似的偏好对和训练提示的分布。对于有用性，我们专注于最终总结，确保评估强调响应对用户的实用性和相关性，同时尽量减少对基础推理过程的干扰。对于无害性，我们评估模型的整个响应，包括推理过程和总结，以识别和减轻生成过程中可能出现的任何潜在风险、偏见或有害内容。\n最终，奖励信号和多样化数据分布的整合使我们能够训练出在推理中表现出色，同时优先考虑有用性和无害性的模型。"
    },
    {
        "section": "2_4",
        "content": "\\subsection{Distillation: Empower Small Models with Reasoning Capability }\n\nTo equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly fine-tuned open-source models like Qwen \\citep{qwen2_5} and Llama \\citep{llama3_1_405b} using the 800k samples curated with DeepSeek-R1, as detailed in \\S \\ref{sec:method:r1:sft}.\nOur findings indicate that this straightforward distillation method significantly enhances the reasoning abilities of smaller models.\nThe base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its reasoning capability is slightly better than that of Llama-3.1.\n\nFor distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community.",
        "trans_content": "\\subsection{蒸馏：赋予小模型推理能力}\n\n为了使更高效的小模型具备像 DeepSeek-R1 这样的推理能力，我们直接使用 DeepSeek-R1 精心挑选的 80 万个样本对开源模型如 Qwen \\citep{qwen2_5} 和 Llama \\citep{llama3_1_405b} 进行了微调，具体细节见 \\S \\ref{sec:method:r1:sft}。我们的研究结果表明，这种直接的蒸馏方法显著增强了小模型的推理能力。我们在此使用的基础模型包括 Qwen2.5-Math-1.5B、Qwen2.5-Math-7B、Qwen2.5-14B、Qwen2.5-32B、Llama-3.1-8B 和 Llama-3.3-70B-Instruct。我们选择 Llama-3.3 是因为其推理能力略优于 Llama-3.1。\n\n对于蒸馏模型，我们仅应用 SFT 并不包括 RL 阶段，尽管结合 RL 可以显著提升模型性能。我们此处的主要目标是展示蒸馏技术的有效性，而将 RL 阶段的探索留给更广泛的研究社区。"
    },
    {
        "section": "3",
        "content": "\\section{Experiment}\n\n\\paragraph{Benchmarks} We evaluate models on MMLU \\citep{mmlu}, MMLU-Redux \\citep{mmlu_redux}, MMLU-Pro \\citep{mmlu_pro}, C-Eval \\citep{ceval}, and CMMLU \\citep{cmmlu}, IFEval~\\citep{IFeval}, FRAMES~\\citep{frames}, GPQA Diamond ~\\citep{gpqa}, SimpleQA~\\citep{simpleqa}, C-SimpleQA~\\citep{csimpleqa}, SWE-Bench Verified~\\citep{swe_verified}, Aider~\\footnote{\\url{https://aider.chat}}, LiveCodeBench~\\citep{livecodebench} (2024-08 -- 2025-01), Codeforces~\\footnote{\\url{https://codeforces.com}}, Chinese National High School Mathematics Olympiad (CNMO 2024)\\footnote{\\url{https://www.cms.org.cn/Home/comp/comp/cid/12.html}}, and American Invitational Mathematics Examination 2024 (AIME 2024)~\\citep{AIME2024}.\nIn addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges.\nSpecifically, we adhere to the original configurations of AlpacaEval 2.0~\\citep{alpaca2.0} and Arena-Hard~\\citep{li2024crowdsourced}, which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we only feed the final summary to evaluation to avoid the length bias.\nFor distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench.\n\n\\paragraph{Evaluation Prompts} Following the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP, GPQA Diamond, and SimpleQA are evaluated using prompts from the simple-evals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format~\\citep{Lin_ZeroEval_A_Unified_2024} in a zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot may hurt the performance of  \\dsri{}.\nOther datasets follow their original evaluation protocols with default prompts provided by their creators.\nFor code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C\\#, JavaScript, TypeScript, PHP, and Bash).\nModel performance on LiveCodeBench is evaluated using  CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases, after which the expected ratings and percentages of competitors are calculated. SWE-Bench verified results are obtained via the agentless framework~\\citep{agentless}. AIDER-related benchmarks are measured using a \"diff\" format.\n\\dsri{} outputs are capped at a maximum of 32,768 tokens for each benchmark.\n\n\\paragraph{Baselines} We conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217. Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its performance based on official reports.\nFor distilled models, we also compare the open-source model QwQ-32B-Preview \\citep{QwQ}.\n\n\\paragraph{Evaluation Setup}\nWe set the maximum generation length to 32,768 tokens for the models.\nWe found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and significant variability across different checkpoints.\nTherefore, we default to pass@$k$ evaluation \\citep{codex} and report pass@1 using a non-zero temperature.\nSpecifically, we use a sampling temperature of $0.6$ and a top-$p$ value of $0.95$ to generate $k$ responses (typically between $4$ and $64$, depending on the test set size) for each question. Pass@1 is then calculated as\n\\[\n\\text{pass@1} = \\frac{1}{k} \\sum_{i=1}^{k} p_i,\n\\]\nwhere $p_i$ denotes the correctness of the $i$-th response. This method provides more reliable performance estimates.\nFor AIME 2024, we also report consensus (majority vote) results \\citep{wang2022self} using $64$ samples, denoted as $\\text{cons}@64$.",
        "trans_content": "\\section{实验}\n\n\\paragraph{基准测试} 我们在 MMLU \\citep{mmlu}, MMLU-Redux \\citep{mmlu_redux}, MMLU-Pro \\citep{mmlu_pro}, C-Eval \\citep{ceval}, 和 CMMLU \\citep{cmmlu}, IFEval~\\citep{IFeval}, FRAMES~\\citep{frames}, GPQA Diamond ~\\citep{gpqa}, SimpleQA~\\citep{simpleqa}, C-SimpleQA~\\citep{csimpleqa}, SWE-Bench Verified~\\citep{swe_verified}, Aider~\\footnote{\\url{https://aider.chat}}, LiveCodeBench~\\citep{livecodebench} (2024-08 -- 2025-01), Codeforces~\\footnote{\\url{https://codeforces.com}}, 中国全国高中数学奥林匹克竞赛 (CNMO 2024)\\footnote{\\url{https://www.cms.org.cn/Home/comp/comp/cid/12.html}}, 和美国邀请数学考试 2024 (AIME 2024)~\\citep{AIME2024} 上评估模型。 除了标准基准测试外，我们还使用 LLMs 作为评判者评估模型在开放式生成任务上的表现。具体而言，我们遵循 AlpacaEval 2.0~\\citep{alpaca2.0} 和 Arena-Hard~\\citep{li2024crowdsourced} 的原始配置，这些配置利用 GPT-4-Turbo-1106 作为成对比较的评判者。 在这里，我们仅将最终摘要输入评估以避免长度偏差。对于压缩模型，我们报告 AIME 2024、MATH-500、GPQA Diamond、Codeforces 和 LiveCodeBench 上的代表性结果。\n\n\\paragraph{评估提示} 按照 DeepSeek-V3 中的设置，MMLU、DROP、GPQA Diamond 和 SimpleQA 等标准基准测试使用 simple-evals 框架中的提示进行评估。对于 MMLU-Redux，我们在零样本设置中采用 Zero-Eval 提示格式~\\citep{Lin_ZeroEval_A_Unified_2024}。至于 MMLU-Pro、C-Eval 和 CLUE-WSC，由于原始提示是少样本的，我们略微修改提示为零样本设置。少样本中的 CoT 可能会影响 \\dsri{} 的性能。其他数据集遵循其创作者提供的默认提示的原始评估协议。对于代码和数学基准测试，HumanEval-Mul 数据集涵盖八种主流编程语言（Python、Java、C++、C\\#、JavaScript、TypeScript、PHP 和 Bash）。 LiveCodeBench 的模型性能使用 CoT 格式进行评估，数据收集时间为 2024 年 8 月至 2025 年 1 月。Codeforces 数据集使用 10 场 Div.2 比赛的问题及专家设计的测试用例进行评估，然后计算预期评分和竞争者百分比。SWE-Bench 验证结果通过无代理框架~\\citep{agentless} 获得。AIDER 相关基准使用“diff”格式进行测量。 \\dsri{} 输出在每个基准上最多限制为 32,768 个标记。\n\n\\paragraph{基线} 我们对多个强大的基线进行了全面评估，包括 DeepSeek-V3、Claude-Sonnet-3.5-1022、GPT-4o-0513、OpenAI-o1-mini 和 OpenAI-o1-1217。由于在中国大陆访问 OpenAI-o1-1217 API 存在困难，我们根据官方报告记录其性能。对于压缩模型，我们还比较了开源模型 QwQ-32B-Preview \\citep{QwQ}。\n\n\\paragraph{评估设置} 我们将模型的最大生成长度设置为 32,768 个标记。我们发现使用贪婪解码评估长输出推理模型会导致更高的重复率，并且在不同检查点之间存在显着的可变性。因此，我们默认使用 pass@$k$ 评估 \\citep{codex} 并使用非零温度报告 pass@1。具体来说，我们使用采样温度 $0.6$ 和 top-$p$ 值 $0.95$ 来为每个问题生成 $k$ 个响应（通常在 $4$ 到 $64$ 之间，具体取决于测试集的大小）。然后计算 pass@1 如下\n\\[\n\\text{pass@1} = \\frac{1}{k} \\sum_{i=1}^{k} p_i,\n\\]\n其中 $p_i$ 表示第 $i$ 个响应的正确性。此方法提供了更可靠的性能估计。对于 AIME 2024，我们还使用 $64$ 个样本报告共识（多数投票）结果 \\citep{wang2022self}，记为 $\\text{cons}@64$。"
    },
    {
        "section": "3_1",
        "content": "\\subsection{\\dsri{} Evaluation}\n\n<PLACEHOLDER_tables/chateval_begin><PLACEHOLDER_ENV_20><PLACEHOLDER_tables/chateval_end>\nFor education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, \\dsri{} demonstrates superior performance compared to DeepSeek-V3. This improvement is primarily attributed to enhanced accuracy in STEM-related questions, where significant gains are achieved through large-scale reinforcement learning. Additionally, \\dsri{} excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities. This highlights the potential of reasoning models in AI-driven search and data analysis tasks. On the factual benchmark SimpleQA, \\dsri{} outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses GPT-4o on this benchmark. However, \\dsri{} performs worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse answering certain queries after safety RL. Without safety RL, \\dsri{} could achieve an accuracy of over 70\\%.\n\n\\dsri{} also delivers impressive results on IF-Eval, a benchmark designed to assess a model's ability to follow format instructions. These improvements can be linked to the inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard, indicating \\dsri{}’s strengths in writing tasks and open-domain question answering. Its significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale RL, which not only boosts reasoning capabilities but also improves performance across diverse domains.\nMoreover, the summary lengths generated by \\dsri{} are concise, with an average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that \\dsri{} avoids introducing length bias during GPT-based evaluations, further solidifying its robustness across multiple tasks.\n\nOn math tasks, \\dsri{} demonstrates performance on par with OpenAI-o1-1217, surpassing other models by a large margin. A similar trend is observed on coding algorithm tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms \\dsri{} on Aider but achieves comparable performance on SWE Verified. We believe the engineering performance of \\dsri{} will improve in the next version, as the amount of related RL training data currently remains very limited.",
        "trans_content": "\\subsection{\\dsri{} 评估}\n\n<PLACEHOLDER_tables/chateval_begin><PLACEHOLDER_ENV_20><PLACEHOLDER_tables/chateval_end>\n对于以教育为导向的知识基准测试，如MMLU、MMLU-Pro和GPQA Diamond，\\dsri{} 展现出比DeepSeek-V3更优越的性能。此改进主要归因于在STEM相关问题中提高了准确性，其中通过大规模强化学习实现了显著进步。此外，\\dsri{} 在FRAMES上表现出色，这是一个长上下文依赖的QA任务，展示了其强大的文档分析能力。这突显了推理模型在AI驱动的搜索和数据分析任务中的潜力。在事实基准SimpleQA上，\\dsri{} 的表现优于DeepSeek-V3，展示了其处理基于事实的查询的能力。在此基准上也观察到类似趋势，OpenAI-o1超越了GPT-4o。然而，在中文SimpleQA基准上，\\dsri{} 的表现不如DeepSeek-V3，主要是因为在安全RL后其倾向于拒绝回答某些查询。如果没有安全RL，\\dsri{} 可以达到超过70\\%的准确率。\n\n\\dsri{} 在IF-Eval上也表现出色，这是一个旨在评估模型遵循格式指令能力的基准。这些改进可以归因于在监督微调（SFT）和RL训练的最后阶段包含了指令遵循数据。此外，在AlpacaEval2.0和ArenaHard上表现出色，表明\\dsri{} 在写作任务和开放域问答中的优势。其显著超过DeepSeek-V3的表现突显了大规模RL的泛化益处，不仅提升了推理能力，也提高了在不同领域的性能。此外，\\dsri{} 生成的摘要长度简洁，在ArenaHard上平均为689个标记，在AlpacaEval 2.0上为2,218个字符。这表明\\dsri{} 在基于GPT的评估中避免了引入长度偏差，进一步巩固了其在多任务中的稳健性。\n\n在数学任务中，\\dsri{} 的表现与OpenAI-o1-1217相当，远超其他模型。在编码算法任务中，如LiveCodeBench和Codeforces，以推理为重点的模型主导了这些基准。在工程导向的编码任务中，OpenAI-o1-1217在Aider上表现优于\\dsri{}，但在SWE Verified上表现相当。我们相信\\dsri{} 的工程性能将在下一个版本中有所提升，因为目前相关的RL训练数据仍然非常有限。"
    },
    {
        "section": "3_2",
        "content": "\\subsection{Distilled Model Evaluation}\n\\label{sec:distilled_model_evaluation}\n<PLACEHOLDER_tables/distill_eval_begin><PLACEHOLDER_ENV_21>\n<PLACEHOLDER_tables/distill_eval_end>\n\nAs shown in Table \\ref{tab:distill}, simply distilling DeepSeek-R1's outputs enables the efficient DeepSeek-R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-reasoning models like GPT-4o-0513 across the board.\nDeepSeek-R1-14B surpasses QwQ-32B-Preview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly exceed o1-mini on most benchmarks.\nThese results demonstrate the strong potential of distillation.\nAdditionally, we found that applying RL to these distilled models yields significant further gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here.",
        "trans_content": "\\subsection{蒸馏模型评估}\n\\label{sec:distilled_model_evaluation}\n<PLACEHOLDER_tables/distill_eval_begin><PLACEHOLDER_ENV_21>\n<PLACEHOLDER_tables/distill_eval_end>\n\n如表 \\ref{tab:distill} 所示，简单地蒸馏 DeepSeek-R1 的输出，使得高效的 DeepSeek-R1-7B（即，DeepSeek-R1-Distill-Qwen-7B，以下简写相同）在各个方面均优于非推理模型如 GPT-4o-0513。DeepSeek-R1-14B 在所有评估指标上超越了 QwQ-32B-Preview，而 DeepSeek-R1-32B 和 DeepSeek-R1-70B 在大多数基准测试上显著超过 o1-mini。这些结果展示了蒸馏的强大潜力。此外，我们发现将强化学习应用于这些蒸馏模型可以带来显著的进一步收益。我们认为这值得进一步探索，因此这里只呈现简单的 SFT 蒸馏模型的结果。"
    },
    {
        "section": "4",
        "content": "\\section{Discussion}",
        "trans_content": "\\section{讨论}"
    },
    {
        "section": "4_1",
        "content": "\\subsection{Distillation v.s. Reinforcement Learning}\n<PLACEHOLDER_tables/distill_vs_rl_begin><PLACEHOLDER_ENV_22><PLACEHOLDER_tables/distill_vs_rl_end>\n\nIn Section \\ref{sec:distilled_model_evaluation}, we can see that by distilling DeepSeek-R1, the small model can achieve impressive results. However, there is still one question left: can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation?\n\nTo answer this question, we conduct large-scale RL training on Qwen-32B-Base using math, code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The experimental results, shown in Table \\ref{tab:distill_vs_rl}, demonstrate that the 32B base model, after large-scale RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-Distill-Qwen-32B, which is distilled from DeepSeek-R1,  performs significantly better than DeepSeek-R1-Zero-Qwen-32B across all benchmarks.\n\nTherefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger-scale reinforcement learning.",
        "trans_content": "\\subsection{蒸馏与强化学习}\n<PLACEHOLDER_tables/distill_vs_rl_begin><PLACEHOLDER_ENV_22><PLACEHOLDER_tables/distill_vs_rl_end>\n\n在第 \\ref{sec:distilled_model_evaluation} 节中，我们可以看到，通过蒸馏 DeepSeek-R1，小模型可以达到令人印象深刻的结果。然而，仍然有一个问题：模型能否通过本文讨论的大规模强化学习训练而不依赖蒸馏实现可比的性能？\n\n为了解答这一问题，我们在 Qwen-32B-Base 上使用数学、代码和 STEM 数据进行了大规模强化学习训练，训练超过 10K 步，得到 DeepSeek-R1-Zero-Qwen-32B。实验结果如表 \\ref{tab:distill_vs_rl} 所示，经过大规模强化学习训练后，32B 基础模型的性能与 QwQ-32B-Preview 相当。然而，从 DeepSeek-R1 蒸馏而来的 DeepSeek-R1-Distill-Qwen-32B 在所有基准测试中显著优于 DeepSeek-R1-Zero-Qwen-32B。\n\n因此，我们可以得出两个结论：首先，将更强大的模型蒸馏成较小的模型能够取得优异的结果，而依赖于本文提到的大规模强化学习的小模型需要巨大的计算能力，甚至可能无法达到蒸馏的性能。其次，虽然蒸馏策略既经济又有效，但要超越智能的界限可能仍然需要更强大的基础模型和更大规模的强化学习。"
    },
    {
        "section": "4_2",
        "content": "\\subsection{Unsuccessful Attempts}\nIn the early stages of developing \\dsri{}, we also encountered failures and setbacks along the way. We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models.\n\n\\paragraph{Process Reward Model (PRM)}\nPRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks~\\citep{uesato2022solving, lightman2023let,mathshepherd}. However, in practice, PRM has three main limitations that may hinder its ultimate success. First, it is challenging to explicitly define a fine-grain step in general reasoning.\nSecond, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not conducive to scaling up.\nThird, once a model-based PRM is introduced, it inevitably leads to reward hacking~\\citep{gao2022scalinglawsrewardmodel},  and retraining the reward model needs additional training resources and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search~\\citep{snell2024scalingllmtesttimecompute}, its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments.\n\n\\paragraph{Monte Carlo Tree Search (MCTS)}\nInspired by AlphaGo~\\citep{alphago} and AlphaZero~\\citep{alphazero}, we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability. This approach involves breaking answers into smaller parts to allow the model to explore the solution space systematically. To facilitate this, we prompt the model to generate multiple tags that correspond to specific reasoning steps necessary for the search. For training, we first use collected prompts to find answers via MCTS guided by a pre-trained value model. Subsequently, we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refining the process.\n\nHowever, this approach encounters several challenges when scaling up the training. First, unlike chess, where the search space is relatively well-defined, token generation presents an exponentially larger search space. To address this, we set a maximum extension limit for each node, but this can lead to the model getting stuck in local optima. Second, the value model directly influences the quality of generation since it guides each step of the search process. Training a fine-grained value model is inherently difficult, which makes it challenging for the model to iteratively improve. While AlphaGo's core success relied on training a value model to progressively enhance its performance, this principle proves difficult to replicate in our setup due to the complexities of token generation.\n\nIn conclusion, while MCTS can improve performance during inference when paired with a pre-trained value model, iteratively boosting model performance through self-search remains a significant challenge.",
        "trans_content": "\\subsection{不成功的尝试}\n在开发 \\dsri{} 的初期阶段，我们也遇到了失败和挫折。我们在此分享我们的失败经验以提供见解，但这并不意味着这些方法无法开发出有效的推理模型。\n\n\\paragraph{过程奖励模型 (PRM)}\nPRM 是一种合理的方法，用于指导模型采用更好的方法解决推理任务~\\citep{uesato2022solving, lightman2023let,mathshepherd}。然而，实际上，PRM 有三个主要限制可能阻碍其最终成功。首先，在一般推理中明确定义一个细粒度步骤是具有挑战性的。其次，判断当前中间步骤是否正确是一项艰巨任务。使用模型进行自动化注释可能无法产生满意的结果，而人工注释不利于扩展。第三，一旦引入基于模型的 PRM，不可避免地会导致奖励黑客~\\citep{gao2022scalinglawsrewardmodel}，并且重新训练奖励模型需要额外的训练资源，复杂化了整个训练流程。总之，尽管 PRM 展示了在模型生成的 top-N 响应中重新排序或协助引导搜索的良好能力~\\citep{snell2024scalingllmtesttimecompute}，但与它在大规模强化学习过程中引入的额外计算开销相比，其优势是有限的。\n\n\\paragraph{蒙特卡罗树搜索 (MCTS)}\n受 AlphaGo~\\citep{alphago} 和 AlphaZero~\\citep{alphazero} 的启发，我们探索了使用蒙特卡罗树搜索 (MCTS) 来增强测试时计算的可扩展性。这种方法涉及将答案分解为更小的部分，以使模型能够系统地探索解空间。为此，我们提示模型生成多个标签，这些标签对应于搜索所需的特定推理步骤。在训练中，我们首先使用收集的提示通过预训练的价值模型引导的 MCTS 找到答案。随后，我们使用所得的问题答案对训练演员模型和价值模型，迭代优化过程。\n\n然而，这种方法在扩大训练规模时遇到了一些挑战。首先，与象棋不同，象棋的搜索空间相对定义明确，而令牌生成呈现出指数级更大的搜索空间。为了解决这个问题，我们为每个节点设置了最大扩展限制，但这可能导致模型陷入局部最优。其次，价值模型直接影响生成质量，因为它指导搜索过程的每一步。训练细粒度价值模型本质上是困难的，这使得模型难以迭代改进。虽然 AlphaGo 的核心成功依赖于训练价值模型以逐步提升其性能，但由于令牌生成的复杂性，这一原则在我们的设置中难以复制。\n\n总之，尽管 MCTS 在与预训练的价值模型配对时可以在推理过程中提高性能，但通过自搜索迭代提升模型性能仍然是一个重大挑战。"
    },
    {
        "section": "5",
        "content": "\\section{Conclusion, Limitations, and Future Work}\n\nIn this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. \\dsro{} represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. \\dsri{} is more powerful, leveraging cold-start data alongside iterative RL fine-tuning. Ultimately, \\dsri{} achieves performance comparable to OpenAI-o1-1217 on a range of tasks.\n\nWe further explore distillation the reasoning capability to small dense models. We use \\dsri{} as the teacher model to generate 800K training samples, and fine-tune several small dense models. The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o and Claude-3.5-Sonnet on math benchmarks with 28.9\\% on AIME and 83.9\\% on MATH. Other dense models also achieve impressive results, significantly outperforming other instruction-tuned models based on the same underlying checkpoints.\n\nIn the future, we plan to invest in research across the following directions for \\dsri{}.\n<PLACEHOLDER_ENV_23>\n\n\\bibliography{main}\n\n\\newpage\n\\appendix",
        "trans_content": "\\section{结论、局限性与未来工作}\n\n在这项工作中，我们分享了通过强化学习增强模型推理能力的历程。 \\dsro{} 代表了一种纯粹的强化学习方法，不依赖于冷启动数据，在各种任务中取得了强劲的性能。 \\dsri{} 更加强大，利用冷启动数据与迭代的强化学习微调。最终，\\dsri{} 在多个任务上达到了与 OpenAI-o1-1217 相当的性能。\n\n我们进一步探索了将推理能力蒸馏到小型密集模型。我们使用 \\dsri{} 作为教师模型生成了 80 万个训练样本，并对几个小型密集模型进行了微调。结果令人振奋：DeepSeek-R1-Distill-Qwen-1.5B 在数学基准测试中以 28.9\\% 的 AIME 和 83.9\\% 的 MATH 成绩超越了 GPT-4o 和 Claude-3.5-Sonnet。其他密集模型也取得了令人印象深刻的结果，显著超越了基于相同底层检查点的其他指令微调模型。\n\n未来，我们计划在以下方向上投入 \\dsri{} 的研究。\n<PLACEHOLDER_ENV_23>\n\n\\bibliography{main}\n\n\\newpage\n\\appendix"
    },
    {
        "section": "6",
        "content": "\\section*{Appendix}",
        "trans_content": "\\section*{附录}"
    },
    {
        "section": "7",
        "content": "\\section{Contributions and Acknowledgments}\n\n\\definecolor{damaiblue}{RGB}{0, 0, 100}\n\\definecolor{damaigreen}{RGB}{0, 100, 0}\n\\definecolor{damaired}{RGB}{100, 0, 0}\n\n\\begin{multicols}{2}\n\\noindent\n\\textbf{\\color{damaired} Core Contributors} \\\\\n\\color{damaired} Daya Guo \\\\\n\\color{damaired} Dejian Yang \\\\\n\\color{damaired} Haowei Zhang \\\\\n\\color{damaired} Junxiao Song \\\\\n\\color{damaired} Ruoyu Zhang \\\\\n\\color{damaired} Runxin Xu \\\\\n\\color{damaired} Qihao Zhu \\\\\n\\color{damaired} Shirong Ma \\\\\n\\color{damaired} Peiyi Wang \\\\\n\\color{damaired} Xiao Bi \\\\\n\\color{damaired} Xiaokang Zhang \\\\\n\\color{damaired} Xingkai Yu \\\\\n\\color{damaired} Yu Wu \\\\\n\\color{damaired} Z.F. Wu \\\\\n\\color{damaired} Zhibin Gou \\\\\n\\color{damaired} Zhihong Shao \\\\\n\\color{damaired} Zhuoshu Li \\\\\n\\color{damaired} Ziyi Gao \\\\\n\n\\noindent\n\\textbf{\\color{damaiblue} Contributors} \\\\\n\\color{damaiblue}\n\\color{damaiblue} Aixin Liu \\\\\n\\color{damaiblue} Bing Xue \\\\\n\\color{damaiblue} Bingxuan Wang \\\\\n\\color{damaiblue} Bochao Wu \\\\\n\\color{damaiblue} Bei Feng \\\\\n\\color{damaiblue} Chengda Lu \\\\\n\\color{damaiblue} Chenggang Zhao \\\\\n\\color{damaiblue} Chengqi Deng \\\\\n\\color{damaiblue} Chong Ruan \\\\\n\\color{damaiblue} Damai Dai \\\\\n\\color{damaiblue} Deli Chen \\\\\n\\color{damaiblue} Dongjie Ji \\\\\n\\color{damaiblue} Erhang Li \\\\\n\\color{damaiblue} Fangyun Lin \\\\\n\\color{damaiblue} Fucong Dai \\\\\n\\color{damaiblue} Fuli Luo* \\\\\n\\color{damaiblue} Guangbo Hao \\\\\n\\color{damaiblue} Guanting Chen \\\\\n\\color{damaiblue} Guowei Li \\\\\n\\color{damaiblue} H. Zhang \\\\\n\\color{damaiblue} Hanwei Xu \\\\\n\\color{damaiblue} Honghui Ding \\\\\n\\color{damaiblue} Huazuo Gao \\\\\n\\color{damaiblue} Hui Qu \\\\\n\\color{damaiblue} Hui Li \\\\\n\\color{damaiblue} Jianzhong Guo \\\\\n\\color{damaiblue} Jiashi Li \\\\\n\\color{damaiblue} Jingchang Chen \\\\\n\\color{damaiblue} Jingyang Yuan \\\\\n\\color{damaiblue} Jinhao Tu \\\\\n\\color{damaiblue} Junjie Qiu \\\\\n\\color{damaiblue} Junlong Li \\\\\n\\color{damaiblue} J.L. Cai \\\\\n\\color{damaiblue} Jiaqi Ni \\\\\n\\color{damaiblue} Jian Liang \\\\\n\\color{damaiblue} Jin Chen \\\\\n\\color{damaiblue} Kai Dong \\\\\n\\color{damaiblue} Kai Hu* \\\\\n\\color{damaiblue} Kaichao You \\\\\n\\color{damaiblue} Kaige Gao \\\\\n\\color{damaiblue} Kang Guan \\\\\n\\color{damaiblue} Kexin Huang \\\\\n\\color{damaiblue} Kuai Yu \\\\\n\\color{damaiblue} Lean Wang \\\\\n\\color{damaiblue} Lecong Zhang \\\\\n\\color{damaiblue} Liang Zhao \\\\\n\\color{damaiblue} Litong Wang \\\\\n\\color{damaiblue} Liyue Zhang \\\\\n\\color{damaiblue} Lei Xu \\\\\n\\color{damaiblue} Leyi Xia \\\\\n\\color{damaiblue} Mingchuan Zhang \\\\\n\\color{damaiblue} Minghua Zhang \\\\\n\\color{damaiblue} Minghui Tang \\\\\n\\color{damaiblue} Mingxu Zhou \\\\\n\\color{damaiblue} Meng Li \\\\\n\\color{damaiblue} Miaojun Wang \\\\\n\\color{damaiblue} Mingming Li \\\\\n\\color{damaiblue} Ning Tian \\\\\n\\color{damaiblue} Panpan Huang \\\\\n\\color{damaiblue} Peng Zhang \\\\\n\\color{damaiblue} Qiancheng Wang \\\\\n\\color{damaiblue} Qinyu Chen \\\\\n\\color{damaiblue} Qiushi Du \\\\\n\\color{damaiblue} Ruiqi Ge* \\\\\n\\color{damaiblue} Ruisong Zhang \\\\\n\\color{damaiblue} Ruizhe Pan \\\\\n\\color{damaiblue} Runji Wang \\\\\n\\color{damaiblue} R.J. Chen \\\\\n\\color{damaiblue} R.L. Jin \\\\\n\\color{damaiblue} Ruyi Chen \\\\\n\\color{damaiblue} Shanghao Lu \\\\\n\\color{damaiblue} Shangyan Zhou \\\\\n\\color{damaiblue} Shanhuang Chen \\\\\n\\color{damaiblue} Shengfeng Ye \\\\\n\\color{damaiblue} Shiyu Wang \\\\\n\\color{damaiblue} Shuiping Yu \\\\\n\\color{damaiblue} Shunfeng Zhou \\\\\n\\color{damaiblue} Shuting Pan \\\\\n\\color{damaiblue} S.S. Li \\\\\n\\color{damaiblue} Shuang Zhou \\\\\n\\color{damaiblue} Shaoqing Wu \\\\\n\\color{damaiblue} Shengfeng Ye \\\\\n\\color{damaiblue} Tao Yun \\\\\n\\color{damaiblue} Tian Pei \\\\\n\\color{damaiblue} Tianyu Sun \\\\\n\\color{damaiblue} T. Wang \\\\\n\\color{damaiblue} Wangding Zeng \\\\\n\\color{damaiblue} Wen Liu \\\\\n\\color{damaiblue} Wenfeng Liang \\\\\n\\color{damaiblue} Wenjun Gao \\\\\n\\color{damaiblue} Wenqin Yu* \\\\\n\\color{damaiblue} Wentao Zhang \\\\\n\\color{damaiblue} W.L. Xiao \\\\\n\\color{damaiblue} Wei An \\\\\n\\color{damaiblue} Xiaodong Liu \\\\\n\\color{damaiblue} Xiaohan Wang \\\\\n\\color{damaiblue} Xiaokang Chen \\\\\n\\color{damaiblue} Xiaotao Nie \\\\\n\\color{damaiblue} Xin Cheng \\\\\n\\color{damaiblue} Xin Liu \\\\\n\\color{damaiblue} Xin Xie \\\\\n\\color{damaiblue} Xingchao Liu \\\\\n\\color{damaiblue} Xinyu Yang \\\\\n\\color{damaiblue} Xinyuan Li \\\\\n\\color{damaiblue} Xuecheng Su \\\\\n\\color{damaiblue} Xuheng Lin \\\\\n\\color{damaiblue} X.Q. Li \\\\\n\\color{damaiblue} Xiangyue Jin \\\\\n\\color{damaiblue} Xiaojin Shen \\\\\n\\color{damaiblue} Xiaosha Chen \\\\\n\\color{damaiblue} Xiaowen Sun \\\\\n\\color{damaiblue} Xiaoxiang Wang \\\\\n\\color{damaiblue} Xinnan Song \\\\\n\\color{damaiblue} Xinyi Zhou \\\\\n\\color{damaiblue} Xianzu Wang \\\\\n\\color{damaiblue} Xinxia Shan \\\\\n\\color{damaiblue} Y.K. Li \\\\\n\\color{damaiblue} Y.Q. Wang \\\\\n\\color{damaiblue} Y.X. Wei \\\\\n\\color{damaiblue} Yang Zhang \\\\\n\\color{damaiblue} Yanhong Xu \\\\\n\\color{damaiblue} Yao Li \\\\\n\\color{damaiblue} Yao Zhao \\\\\n\\color{damaiblue} Yaofeng Sun \\\\\n\\color{damaiblue} Yaohui Wang \\\\\n\\color{damaiblue} Yi Yu \\\\\n\\color{damaiblue} Yichao Zhang \\\\\n\\color{damaiblue} Yifan Shi \\\\\n\\color{damaiblue} Yiliang Xiong \\\\\n\\color{damaiblue} Ying He \\\\\n\\color{damaiblue} Yishi Piao \\\\\n\\color{damaiblue} Yisong Wang \\\\\n\\color{damaiblue} Yixuan Tan \\\\\n\\color{damaiblue} Yiyang Ma* \\\\\n\\color{damaiblue} Yiyuan Liu \\\\\n\\color{damaiblue} Yongqiang Guo \\\\\n\\color{damaiblue} Yuan Ou \\\\\n\\color{damaiblue} Yuduan Wang \\\\\n\\color{damaiblue} Yue Gong \\\\\n\\color{damaiblue} Yuheng Zou \\\\\n\\color{damaiblue} Yujia He \\\\\n\\color{damaiblue} Yunfan Xiong \\\\\n\\color{damaiblue} Yuxiang Luo \\\\\n\\color{damaiblue} Yuxiang You \\\\\n\\color{damaiblue} Yuxuan Liu \\\\\n\\color{damaiblue} Yuyang Zhou \\\\\n\\color{damaiblue} Y.X. Zhu \\\\\n\\color{damaiblue} Yanping Huang \\\\\n\\color{damaiblue} Yaohui Li \\\\\n\\color{damaiblue} Yi Zheng \\\\\n\\color{damaiblue} Yuchen Zhu \\\\\n\\color{damaiblue} Yunxian Ma \\\\\n\\color{damaiblue} Ying Tang \\\\\n\\color{damaiblue} Yukun Zha \\\\\n\\color{damaiblue} Yuting Yan \\\\\n\\color{damaiblue} Z.Z. Ren \\\\\n\\color{damaiblue} Zehui Ren \\\\\n\\color{damaiblue} Zhangli Sha \\\\\n\\color{damaiblue} Zhe Fu \\\\\n\\color{damaiblue} Zhean Xu \\\\\n\\color{damaiblue} Zhenda Xie \\\\\n\\color{damaiblue} Zhengyan Zhang \\\\\n\\color{damaiblue} Zhewen Hao \\\\\n\\color{damaiblue} Zhicheng Ma \\\\\n\\color{damaiblue} Zhigang Yan \\\\\n\\color{damaiblue} Zhiyu Wu \\\\\n\\color{damaiblue} Zihui Gu \\\\\n\\color{damaiblue} Zijia Zhu \\\\\n\\color{damaiblue} Zijun Liu* \\\\\n\\color{damaiblue} Zilin Li \\\\\n\\color{damaiblue} Ziwei Xie \\\\\n\\color{damaiblue} Ziyang Song \\\\\n\\color{damaiblue} Zizheng Pan \\\\\n\\color{damaiblue} Zhen Huang \\\\\n\\color{damaiblue} Zhipeng Xu \\\\\n\\color{damaiblue} Zhongyu Zhang \\\\\n\\color{damaiblue} Zhen Zhang \\\\\n\n\\end{multicols}\n\nWithin each role, authors are listed alphabetically by the first name.\nNames marked with * denote individuals who have departed from our team.\n\n\\setcounter{figure}{0}\n\\makeatletter\n<PLACEHOLDER_NEWCOMMAND_193>\n\\makeatother\n\n\\setcounter{table}{0}\n\\makeatletter\n<PLACEHOLDER_NEWCOMMAND_194>\n\\makeatother\n\n\\end{document}",
        "trans_content": "\\section{贡献与致谢}\n\n\\definecolor{damaiblue}{RGB}{0, 0, 100}\n\\definecolor{damaigreen}{RGB}{0, 100, 0}\n\\definecolor{damaired}{RGB}{100, 0, 0}\n\n\\begin{multicols}{2}\n\\noindent\n\\textbf{\\color{damaired} 核心贡献者} \\\\\n\\color{damaired} Daya Guo \\\\\n\\color{damaired} Dejian Yang \\\\\n\\color{damaired} Haowei Zhang \\\\\n\\color{damaired} Junxiao Song \\\\\n\\color{damaired} Ruoyu Zhang \\\\\n\\color{damaired} Runxin Xu \\\\\n\\color{damaired} Qihao Zhu \\\\\n\\color{damaired} Shirong Ma \\\\\n\\color{damaired} Peiyi Wang \\\\\n\\color{damaired} Xiao Bi \\\\\n\\color{damaired} Xiaokang Zhang \\\\\n\\color{damaired} Xingkai Yu \\\\\n\\color{damaired} Yu Wu \\\\\n\\color{damaired} Z.F. Wu \\\\\n\\color{damaired} Zhibin Gou \\\\\n\\color{damaired} Zhihong Shao \\\\\n\\color{damaired} Zhuoshu Li \\\\\n\\color{damaired} Ziyi Gao \\\\\n\n\\noindent\n\\textbf{\\color{damaiblue} 贡献者} \\\\\n\\color{damaiblue}\n\\color{damaiblue} Aixin Liu \\\\\n\\color{damaiblue} Bing Xue \\\\\n\\color{damaiblue} Bingxuan Wang \\\\\n\\color{damaiblue} Bochao Wu \\\\\n\\color{damaiblue} Bei Feng \\\\\n\\color{damaiblue} Chengda Lu \\\\\n\\color{damaiblue} Chenggang Zhao \\\\\n\\color{damaiblue} Chengqi Deng \\\\\n\\color{damaiblue} Chong Ruan \\\\\n\\color{damaiblue} Damai Dai \\\\\n\\color{damaiblue} Deli Chen \\\\\n\\color{damaiblue} Dongjie Ji \\\\\n\\color{damaiblue} Erhang Li \\\\\n\\color{damaiblue} Fangyun Lin \\\\\n\\color{damaiblue} Fucong Dai \\\\\n\\color{damaiblue} Fuli Luo* \\\\\n\\color{damaiblue} Guangbo Hao \\\\\n\\color{damaiblue} Guanting Chen \\\\\n\\color{damaiblue} Guowei Li \\\\\n\\color{damaiblue} H. Zhang \\\\\n\\color{damaiblue} Hanwei Xu \\\\\n\\color{damaiblue} Honghui Ding \\\\\n\\color{damaiblue} Huazuo Gao \\\\\n\\color{damaiblue} Hui Qu \\\\\n\\color{damaiblue} Hui Li \\\\\n\\color{damaiblue} Jianzhong Guo \\\\\n\\color{damaiblue} Jiashi Li \\\\\n\\color{damaiblue} Jingchang Chen \\\\\n\\color{damaiblue} Jingyang Yuan \\\\\n\\color{damaiblue} Jinhao Tu \\\\\n\\color{damaiblue} Junjie Qiu \\\\\n\\color{damaiblue} Junlong Li \\\\\n\\color{damaiblue} J.L. Cai \\\\\n\\color{damaiblue} Jiaqi Ni \\\\\n\\color{damaiblue} Jian Liang \\\\\n\\color{damaiblue} Jin Chen \\\\\n\\color{damaiblue} Kai Dong \\\\\n\\color{damaiblue} Kai Hu* \\\\\n\\color{damaiblue} Kaichao You \\\\\n\\color{damaiblue} Kaige Gao \\\\\n\\color{damaiblue} Kang Guan \\\\\n\\color{damaiblue} Kexin Huang \\\\\n\\color{damaiblue} Kuai Yu \\\\\n\\color{damaiblue} Lean Wang \\\\\n\\color{damaiblue} Lecong Zhang \\\\\n\\color{damaiblue} Liang Zhao \\\\\n\\color{damaiblue} Litong Wang \\\\\n\\color{damaiblue} Liyue Zhang \\\\\n\\color{damaiblue} Lei Xu \\\\\n\\color{damaiblue} Leyi Xia \\\\\n\\color{damaiblue} Mingchuan Zhang \\\\\n\\color{damaiblue} Minghua Zhang \\\\\n\\color{damaiblue} Minghui Tang \\\\\n\\color{damaiblue} Mingxu Zhou \\\\\n\\color{damaiblue} Meng Li \\\\\n\\color{damaiblue} Miaojun Wang \\\\\n\\color{damaiblue} Mingming Li \\\\\n\\color{damaiblue} Ning Tian \\\\\n\\color{damaiblue} Panpan Huang \\\\\n\\color{damaiblue} Peng Zhang \\\\\n\\color{damaiblue} Qiancheng Wang \\\\\n\\color{damaiblue} Qinyu Chen \\\\\n\\color{damaiblue} Qiushi Du \\\\\n\\color{damaiblue} Ruiqi Ge* \\\\\n\\color{damaiblue} Ruisong Zhang \\\\\n\\color{damaiblue} Ruizhe Pan \\\\\n\\color{damaiblue} Runji Wang \\\\\n\\color{damaiblue} R.J. Chen \\\\\n\\color{damaiblue} R.L. Jin \\\\\n\\color{damaiblue} Ruyi Chen \\\\\n\\color{damaiblue} Shanghao Lu \\\\\n\\color{damaiblue} Shangyan Zhou \\\\\n\\color{damaiblue} Shanhuang Chen \\\\\n\\color{damaiblue} Shengfeng Ye \\\\\n\\color{damaiblue} Shiyu Wang \\\\\n\\color{damaiblue} Shuiping Yu \\\\\n\\color{damaiblue} Shunfeng Zhou \\\\\n\\color{damaiblue} Shuting Pan \\\\\n\\color{damaiblue} S.S. Li \\\\\n\\color{damaiblue} Shuang Zhou \\\\\n\\color{damaiblue} Shaoqing Wu \\\\\n\\color{damaiblue} Shengfeng Ye \\\\\n\\color{damaiblue} Tao Yun \\\\\n\\color{damaiblue} Tian Pei \\\\\n\\color{damaiblue} Tianyu Sun \\\\\n\\color{damaiblue} T. Wang \\\\\n\\color{damaiblue} Wangding Zeng \\\\\n\\color{damaiblue} Wen Liu \\\\\n\\color{damaiblue} Wenfeng Liang \\\\\n\\color{damaiblue} Wenjun Gao \\\\\n\\color{damaiblue} Wenqin Yu* \\\\\n\\color{damaiblue} Wentao Zhang \\\\\n\\color{damaiblue} W.L. Xiao \\\\\n\\color{damaiblue} Wei An \\\\\n\\color{damaiblue} Xiaodong Liu \\\\\n\\color{damaiblue} Xiaohan Wang \\\\\n\\color{damaiblue} Xiaokang Chen \\\\\n\\color{damaiblue} Xiaotao Nie \\\\\n\\color{damaiblue} Xin Cheng \\\\\n\\color{damaiblue} Xin Liu \\\\\n\\color{damaiblue} Xin Xie \\\\\n\\color{damaiblue} Xingchao Liu \\\\\n\\color{damaiblue} Xinyu Yang \\\\\n\\color{damaiblue} Xinyuan Li \\\\\n\\color{damaiblue} Xuecheng Su \\\\\n\\color{damaiblue} Xuheng Lin \\\\\n\\color{damaiblue} X.Q. Li \\\\\n\\color{damaiblue} Xiangyue Jin \\\\\n\\color{damaiblue} Xiaojin Shen \\\\\n\\color{damaiblue} Xiaosha Chen \\\\\n\\color{damaiblue} Xiaowen Sun \\\\\n\\color{damaiblue} Xiaoxiang Wang \\\\\n\\color{damaiblue} Xinnan Song \\\\\n\\color{damaiblue} Xinyi Zhou \\\\\n\\color{damaiblue} Xianzu Wang \\\\\n\\color{damaiblue} Xinxia Shan \\\\\n\\color{damaiblue} Y.K. Li \\\\\n\\color{damaiblue} Y.Q. Wang \\\\\n\\color{damaiblue} Y.X. Wei \\\\\n\\color{damaiblue} Yang Zhang \\\\\n\\color{damaiblue} Yanhong Xu \\\\\n\\color{damaiblue} Yao Li \\\\\n\\color{damaiblue} Yao Zhao \\\\\n\\color{damaiblue} Yaofeng Sun \\\\\n\\color{damaiblue} Yaohui Wang \\\\\n\\color{damaiblue} Yi Yu \\\\\n\\color{damaiblue} Yichao Zhang \\\\\n\\color{damaiblue} Yifan Shi \\\\\n\\color{damaiblue} Yiliang Xiong \\\\\n\\color{damaiblue} Ying He \\\\\n\\color{damaiblue} Yishi Piao \\\\\n\\color{damaiblue} Yisong Wang \\\\\n\\color{damaiblue} Yixuan Tan \\\\\n\\color{damaiblue} Yiyang Ma* \\\\\n\\color{damaiblue} Yiyuan Liu \\\\\n\\color{damaiblue} Yongqiang Guo \\\\\n\\color{damaiblue} Yuan Ou \\\\\n\\color{damaiblue} Yuduan Wang \\\\\n\\color{damaiblue} Yue Gong \\\\\n\\color{damaiblue} Yuheng Zou \\\\\n\\color{damaiblue} Yujia He \\\\\n\\color{damaiblue} Yunfan Xiong \\\\\n\\color{damaiblue} Yuxiang Luo \\\\\n\\color{damaiblue} Yuxiang You \\\\\n\\color{damaiblue} Yuxuan Liu \\\\\n\\color{damaiblue} Yuyang Zhou \\\\\n\\color{damaiblue} Y.X. Zhu \\\\\n\\color{damaiblue} Yanping Huang \\\\\n\\color{damaiblue} Yaohui Li \\\\\n\\color{damaiblue} Yi Zheng \\\\\n\\color{damaiblue} Yuchen Zhu \\\\\n\\color{damaiblue} Yunxian Ma \\\\\n\\color{damaiblue} Ying Tang \\\\\n\\color{damaiblue} Yukun Zha \\\\\n\\color{damaiblue} Yuting Yan \\\\\n\\color{damaiblue} Z.Z. Ren \\\\\n\\color{damaiblue} Zehui Ren \\\\\n\\color{damaiblue} Zhangli Sha \\\\\n\\color{damaiblue} Zhe Fu \\\\\n\\color{damaiblue} Zhean Xu \\\\\n\\color{damaiblue} Zhenda Xie \\\\\n\\color{damaiblue} Zhengyan Zhang \\\\\n\\color{damaiblue} Zhewen Hao \\\\\n\\color{damaiblue} Zhicheng Ma \\\\\n\\color{damaiblue} Zhigang Yan \\\\\n\\color{damaiblue} Zhiyu Wu \\\\\n\\color{damaiblue} Zihui Gu \\\\\n\\color{damaiblue} Zijia Zhu \\\\\n\\color{damaiblue} Zijun Liu* \\\\\n\\color{damaiblue} Zilin Li \\\\\n\\color{damaiblue} Ziwei Xie \\\\\n\\color{damaiblue} Ziyang Song \\\\\n\\color{damaiblue} Zizheng Pan \\\\\n\\color{damaiblue} Zhen Huang \\\\\n\\color{damaiblue} Zhipeng Xu \\\\\n\\color{damaiblue} Zhongyu Zhang \\\\\n\\color{damaiblue} Zhen Zhang \\\\\n\n\\end{multicols}\n\n在每个角色中，作者按名字的字母顺序排列。\n标记有 * 的名字表示已离开我们团队的个人。\n\n\\setcounter{figure}{0}\n\\makeatletter\n<PLACEHOLDER_NEWCOMMAND_193>\n\\makeatother\n\n\\setcounter{table}{0}\n\\makeatletter\n<PLACEHOLDER_NEWCOMMAND_194>\n\\makeatother\n\n\\end{document}"
    }
]