[
    {
        "placeholder": "<PLACEHOLDER_ENV_1>",
        "env_name": "list",
        "content": "\\begin{list}{}\n         {\\setlength{\\leftmargin}{#1}}\n         \\item[]\n }\n {\\end{list}",
        "trans_content": "\\begin{list}{}\n         {\\setlength{\\leftmargin}{#1}}\n         \\item[]\n }\n {\\end{list}",
        "need_trans": true
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_2>",
        "env_name": "array",
        "content": "\\begin{array}{ccc} #1 & \\longrightarrow & #2 \\\\ #3 & \\longmapsto & #4 \\end{array}",
        "trans_content": "\\begin{array}{ccc} #1 & \\longrightarrow & #2 \\\\ #3 & \\longmapsto & #4 \\end{array}",
        "need_trans": true
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_3>",
        "env_name": "array",
        "content": "\\begin{array}{l} #1 \\end{array}",
        "trans_content": "\\begin{array}{l} #1 \\end{array}",
        "need_trans": true
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_4>",
        "env_name": "abstract",
        "content": "\\begin{abstract}\n\nWe introduce our first-generation reasoning models, \\dsro{} and \\dsri{}.\n\\dsro{}, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.\nThrough RL, \\dsro{} naturally emerges with numerous powerful and intriguing reasoning behaviors.\nHowever, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce \\dsri{}, which incorporates multi-stage training and cold-start data before RL.\n\\dsri{} achieves performance comparable to OpenAI-o1-1217 on reasoning tasks.\nTo support the research community, we open-source \\dsro{}, \\dsri{}, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from \\dsri{} based on Qwen and Llama.\n\n\\end{abstract}",
        "trans_content": "\\begin{abstract}\n\n我们介绍了我们的第一代推理模型，\\dsro{} 和 \\dsri{} 。\\dsro{} 是一个通过大规模强化学习 (RL) 训练的模型，没有监督微调 (SFT) 作为初步步骤，展现了非凡的推理能力。通过 RL，\\dsro{} 自然地出现了许多强大且有趣的推理行为。然而，它面临诸如可读性差和语言混合等挑战。为了解决这些问题并进一步提高推理性能，我们引入了 \\dsri{} ，其在 RL 之前结合了多阶段训练和冷启动数据。\\dsri{} 在推理任务中实现了与 OpenAI-o1-1217 相当的性能。为了支持研究社区，我们开源了 \\dsro{} 、 \\dsri{} 和从 \\dsri{} 基于 Qwen 和 Llama 蒸馏出的六个稠密模型 (1.5B, 7B, 8B, 14B, 32B, 70B)。\n\n\\end{abstract}",
        "need_trans": true
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_5>",
        "env_name": "figure",
        "content": "\\begin{figure}[h]\n\\centering\n\\includegraphics[width=1.0\\textwidth]{figures/dsr1_performance.pdf}\n<PLACEHOLDER_CAP_2>\n\\label{fig:dsv3_performance}\n\\end{figure}",
        "trans_content": "\\begin{figure}[h]\n\\centering\n\\includegraphics[width=1.0\\textwidth]{figures/dsr1_performance.pdf}\n<PLACEHOLDER_CAP_2>\n\\label{fig:dsv3_performance}\n\\end{figure}",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_6>",
        "env_name": "spacing",
        "content": "\\begin{spacing}{0.9}\n\\tableofcontents\n\\end{spacing}",
        "trans_content": "\\begin{spacing}{0.9}\n\\tableofcontents\n\\end{spacing}",
        "need_trans": true
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_7>",
        "env_name": "itemize",
        "content": "\\begin{itemize}[topsep=0pt]\n    \\item\n    We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of \\dsro{}. \\dsro{} demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n    \\item\n    We introduce our pipeline to develop \\dsri{}. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models.\n\n\\end{itemize}",
        "trans_content": "\\begin{itemize}[topsep=0pt]\n    \\item\n    我们直接将强化学习（RL）应用于基础模型，而不依赖监督微调（SFT）作为初步步骤。这种方法使得模型可以探索思维链（CoT）来解决复杂问题，最终开发出 \\dsro{}。\\dsro{} 展现了自我验证、反思和生成长思维链等能力，标志着研究界的一个重大里程碑。值得注意的是，这是首次开放研究验证了大型语言模型（LLM）的推理能力可以完全通过RL激励实现，而无需SFT。这一突破为未来在该领域的发展铺平了道路。\n    \\item\n    我们介绍了开发 \\dsri{} 的流程。该流程包含两个RL阶段，旨在发现更好的推理模式并与人类偏好对齐，还有两个SFT阶段，作为模型推理和非推理能力的种子。我们相信这个流程将通过创造更好的模型为行业带来益处。\n\n\\end{itemize}",
        "need_trans": true
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_8>",
        "env_name": "itemize",
        "content": "\\begin{itemize}[topsep=0pt]\n    \\item We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source \\dsri{}, as well as its API, will benefit the research community to distill better smaller models in the future.\n    \\item Using the reasoning data generated by \\dsri{}, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks.\n    DeepSeek-R1-Distill-Qwen-7B achieves 55.5\\% on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6\\% on AIME 2024, 94.3\\% on MATH-500, and 57.2\\% on LiveCodeBench. These results significantly outperform previous open-source models and are comparable to o1-mini.\n    We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\\end{itemize}",
        "trans_content": "\\begin{itemize}[topsep=0pt]\n    \\item 我们证明了较大模型的推理模式可以被蒸馏到较小模型中，从而获得比通过RL在小模型上发现的推理模式更好的性能。开放源码的\\dsri{}及其API将有助于研究社区在未来蒸馏出更好的小模型。\n    \\item 使用\\dsri{}生成的推理数据，我们微调了几个研究社区广泛使用的密集模型。评估结果表明，蒸馏出的较小密集模型在基准测试中表现异常出色。DeepSeek-R1-Distill-Qwen-7B在AIME 2024上取得了55.5\\%的成绩，超过了QwQ-32B-Preview。此外，DeepSeek-R1-Distill-Qwen-32B在AIME 2024上得分72.6\\%，在MATH-500上得分94.3\\%，在LiveCodeBench上得分57.2\\%。这些结果显著优于先前的开源模型，并且与o1-mini相当。我们将基于Qwen2.5和Llama3系列的蒸馏1.5B、7B、8B、14B、32B和70B的检查点向社区开源。\n\\end{itemize}",
        "need_trans": true
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_9>",
        "env_name": "itemize",
        "content": "\\begin{itemize}[topsep=0pt]\n\n    \\item \\textbf{Reasoning tasks}:\n    (1)\n   \\dsri{} achieves a score of 79.8\\% Pass@1 on AIME 2024, slightly surpassing OpenAI-o1-1217. On MATH-500, it attains an impressive score of 97.3\\%, performing on par with OpenAI-o1-1217 and significantly outperforming other models.\n    (2)\n    On coding-related tasks, \\dsri{} demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3\\% human participants in the competition.\n    For engineering-related tasks,  \\dsri{} performs slightly better than DeepSeek-V3, which could help developers in real world tasks.\n\n    \\item \\textbf{Knowledge}:\nOn benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, \\dsri{} achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8\\% on MMLU, 84.0\\% on MMLU-Pro, and 71.5\\% on GPQA Diamond. While its performance is slightly below that of OpenAI-o1-1217 on these benchmarks, \\dsri{} surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, \\dsri{} outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses 4o on this benchmark.\n    \\item \\textbf{Others}: \\dsri{} also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of 87.6\\% on AlpacaEval 2.0 and a win-rate of 92.3\\% on ArenaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries. Additionally, \\dsri{} demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks.\n\\end{itemize}",
        "trans_content": "\\begin{itemize}[topsep=0pt]\n\n    \\item \\textbf{推理任务}：\n    (1)\n    \\dsri{} 在 AIME 2024 上的 Pass@1 得分为 79.8\\%，略微超越了 OpenAI-o1-1217。在 MATH-500 上，它取得了 97.3\\% 的优异成绩，与 OpenAI-o1-1217 表现相当，并显著超过其他模型。\n    (2)\n    在与编码相关的任务中，\\dsri{} 在代码竞赛任务中展示了专家级水平，Codeforces 的 Elo 评分为 2,029，超过了 96.3\\% 的人类参赛者。在工程相关任务中，\\dsri{} 的表现略优于 DeepSeek-V3，这可能有助于开发人员在实际任务中使用。\n\n    \\item \\textbf{知识}：\n    在 MMLU、MMLU-Pro 和 GPQA Diamond 等基准测试中，\\dsri{} 取得了卓越的成绩，显著超过了 DeepSeek-V3，在 MMLU 上得分为 90.8\\%，在 MMLU-Pro 上得分为 84.0\\%，在 GPQA Diamond 上得分为 71.5\\%。虽然在这些基准测试上的表现略低于 OpenAI-o1-1217，但 \\dsri{} 超过了其他闭源模型，显示了其在教育任务中的竞争优势。在事实基准测试 SimpleQA 上，\\dsri{} 的表现优于 DeepSeek-V3，展示了其处理基于事实的查询的能力。在这个基准测试上，OpenAI-o1 的表现也超过了 4o。\n    \\item \\textbf{其他}：\\dsri{} 还在广泛的任务中表现出色，包括创意写作、一般问答、编辑、摘要等。在 AlpacaEval 2.0 中，它取得了 87.6\\% 的长度控制胜率，在 ArenaHard 中取得了 92.3\\% 的胜率，展示了其智能处理非考试导向查询的强大能力。此外，\\dsri{} 在需要长上下文理解的任务中表现出色，在长上下文基准测试中显著超过了 DeepSeek-V3。\n\\end{itemize}",
        "need_trans": true
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_10>",
        "env_name": "equation",
        "content": "\\begin{equation}\n\\begin{split}\n    \\mathcal{J}_{GRPO}(\\theta) &= \\mathbb{E}{[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{old}}(O|q)]}  \\\\\n    & \\frac{1}{G}\\sum_{i=1}^G \\left( \\min \\left( \\frac{\\pi_\\theta(o_i |q)}{\\pi_{\\theta_{old}}(o_i |q)} A_i, \\text{clip} \\left( \\frac{\\pi_\\theta(o_i |q)}{\\pi_{\\theta_{old}}(o_i |q)}, 1 - \\epsilon, 1 + \\epsilon \\right)  A_i \\right) - \\beta \\mathbb{D}_{KL}\\left(\\pi_{\\theta} || \\pi_{ref}\\right)\\right) ,\n\\end{split}\n\\label{eq:GRPO-obj}\n\\end{equation}",
        "trans_content": "\\begin{equation}\n\\begin{split}\n    \\mathcal{J}_{GRPO}(\\theta) &= \\mathbb{E}{[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{old}}(O|q)]}  \\\\\n    & \\frac{1}{G}\\sum_{i=1}^G \\left( \\min \\left( \\frac{\\pi_\\theta(o_i |q)}{\\pi_{\\theta_{old}}(o_i |q)} A_i, \\text{clip} \\left( \\frac{\\pi_\\theta(o_i |q)}{\\pi_{\\theta_{old}}(o_i |q)}, 1 - \\epsilon, 1 + \\epsilon \\right)  A_i \\right) - \\beta \\mathbb{D}_{KL}\\left(\\pi_{\\theta} || \\pi_{ref}\\right)\\right) ,\n\\end{split}\n\\label{eq:GRPO-obj}\n\\end{equation}",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_11>",
        "env_name": "equation",
        "content": "\\begin{equation}\n    \\mathbb{D}_{KL}\\left(\\pi_{\\theta} || \\pi_{ref}\\right) = \\frac{\\pi_{ref}(o_i|q)}{\\pi_{\\theta}(o_i|q)}- \\log\\frac{\\pi_{ref}(o_i|q)}{\\pi_{\\theta}(o_i|q)} - 1,\n\\end{equation}",
        "trans_content": "\\begin{equation}\n    \\mathbb{D}_{KL}\\left(\\pi_{\\theta} || \\pi_{ref}\\right) = \\frac{\\pi_{ref}(o_i|q)}{\\pi_{\\theta}(o_i|q)}- \\log\\frac{\\pi_{ref}(o_i|q)}{\\pi_{\\theta}(o_i|q)} - 1,\n\\end{equation}",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_12>",
        "env_name": "equation",
        "content": "\\begin{equation}\n    A_i = \\frac{r_i - {\\mathrm mean(\\{r_1, r_2, \\cdots, r_G\\})}}{{\\mathrm std(\\{r_1, r_2, \\cdots, r_G\\})}}.\n\\end{equation}",
        "trans_content": "\\begin{equation}\n    A_i = \\frac{r_i - {\\mathrm mean(\\{r_1, r_2, \\cdots, r_G\\})}}{{\\mathrm std(\\{r_1, r_2, \\cdots, r_G\\})}}.\n\\end{equation}",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_13>",
        "env_name": "table",
        "content": "\\begin{table}[t]\n    \\centering\n    \\small\n    \\begin{tabular}{l}\n    \\toprule\n    A conversation between User and Assistant. The user asks a question, and the Assistant solves it. \\\\\n     The assistant first thinks about the reasoning process in the mind and then provides the user \\\\ with the answer.\n     The reasoning process and answer are enclosed within <think> </think> and \\\\<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> \\\\ <answer> answer here </answer>.\n     User: \\textcolor{red}{prompt}. Assistant: \\\\\n     \\bottomrule\n    \\end{tabular}\n    <PLACEHOLDER_CAP_3>\n    \\label{tab:r0_template}\n\\end{table}",
        "trans_content": "\\begin{table}[t]\n    \\centering\n    \\small\n    \\begin{tabular}{l}\n    \\toprule\n    A conversation between User and Assistant. The user asks a question, and the Assistant solves it. \\\\\n     The assistant first thinks about the reasoning process in the mind and then provides the user \\\\ with the answer.\n     The reasoning process and answer are enclosed within <think> </think> and \\\\<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> \\\\ <answer> answer here </answer>.\n     User: \\textcolor{red}{prompt}. Assistant: \\\\\n     \\bottomrule\n    \\end{tabular}\n    <PLACEHOLDER_CAP_3>\n    \\label{tab:r0_template}\n\\end{table}",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_14>",
        "env_name": "itemize",
        "content": "\\begin{itemize}[topsep=0pt]\n    \\item \\textbf{Accuracy rewards}: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases.\n    \\item \\textbf{Format rewards}: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between `<think>' and `</think>' tags.\n\\end{itemize}",
        "trans_content": "\\begin{itemize}[topsep=0pt]\n    \\item \\textbf{准确性奖励}: 准确性奖励模型评估响应是否正确。例如，对于具有确定性结果的数学问题，模型需要以指定格式（例如，在框内）提供最终答案，从而实现可靠的基于规则的正确性验证。同样，对于 LeetCode 问题，可以使用编译器根据预定义的测试用例生成反馈。\n    \\item \\textbf{格式奖励}: 除了准确性奖励模型外，我们还使用格式奖励模型，强制模型将其思维过程放在 `<think>' 和 `</think>' 标签之间。\n\\end{itemize}",
        "need_trans": true
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_15>",
        "env_name": "table",
        "content": "\\begin{table}[t]\n    \\centering\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{@{}l *{6}{c} @{}}\n    \\toprule\n    \\multirow{3}{*}{\\centering\\textbf{Model}} & \\multicolumn{2}{c}{\\multirow{2}{*}{\\textbf{AIME 2024}}} & \\multirow{2}{*}{\\textbf{MATH-500}} & \\textbf{GPQA} & \\textbf{LiveCode} & \\multirow{2}{*}{\\textbf{CodeForces}} \\\\\n     &  & &  & \\textbf{Diamond} & \\textbf{Bench} \\\\\n    \\cmidrule(lr){2-3}\n     & pass@1 & cons@64 & pass@1 &  pass@1 & pass@1 & rating \\\\\n    \\midrule\n    \\textbf{OpenAI-o1-mini} & 63.6 & 80.0 & 90.0  & 60.0 & 53.8 & 1820 \\\\\n    \\textbf{OpenAI-o1-0912} & 74.4  & 83.3  & 94.8  & 77.3 & 63.4 & 1843 \\\\\n    \\midrule\n    \\textbf{\\dsro{}} & 71.0 & 86.7 & 95.9 & 73.3 & 50.0 & 1444 \\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n    <PLACEHOLDER_CAP_4>\n    \\label{tab:r1-zero}\n\\end{table}",
        "trans_content": "\\begin{table}[t]\n    \\centering\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{@{}l *{6}{c} @{}}\n    \\toprule\n    \\multirow{3}{*}{\\centering\\textbf{Model}} & \\multicolumn{2}{c}{\\multirow{2}{*}{\\textbf{AIME 2024}}} & \\multirow{2}{*}{\\textbf{MATH-500}} & \\textbf{GPQA} & \\textbf{LiveCode} & \\multirow{2}{*}{\\textbf{CodeForces}} \\\\\n     &  & &  & \\textbf{Diamond} & \\textbf{Bench} \\\\\n    \\cmidrule(lr){2-3}\n     & pass@1 & cons@64 & pass@1 &  pass@1 & pass@1 & rating \\\\\n    \\midrule\n    \\textbf{OpenAI-o1-mini} & 63.6 & 80.0 & 90.0  & 60.0 & 53.8 & 1820 \\\\\n    \\textbf{OpenAI-o1-0912} & 74.4  & 83.3  & 94.8  & 77.3 & 63.4 & 1843 \\\\\n    \\midrule\n    \\textbf{\\dsro{}} & 71.0 & 86.7 & 95.9 & 73.3 & 50.0 & 1444 \\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n    <PLACEHOLDER_CAP_4>\n    \\label{tab:r1-zero}\n\\end{table}",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_16>",
        "env_name": "figure",
        "content": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.75\\linewidth]{figures/plot_aime_with_maj.png}\n    <PLACEHOLDER_CAP_5>\n    \\label{fig:zero-training-performance}\n\\end{figure}",
        "trans_content": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.75\\linewidth]{figures/plot_aime_with_maj.png}\n    <PLACEHOLDER_CAP_5>\n    \\label{fig:zero-training-performance}\n\\end{figure}",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_17>",
        "env_name": "figure",
        "content": "\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.75\\linewidth]{figures/plot_length.png}\n    <PLACEHOLDER_CAP_6>\n    \\label{fig:zero-training-length}\n\\end{figure}",
        "trans_content": "\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.75\\linewidth]{figures/plot_length.png}\n    <PLACEHOLDER_CAP_6>\n    \\label{fig:zero-training-length}\n\\end{figure}",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_18>",
        "env_name": "table",
        "content": "\\begin{table}[t]\n\\centering\n\\begin{tabular}{l}\n\\toprule\nQuestion: If \\(a > 1\\), then the sum of the real solutions of \\(\\sqrt{a - \\sqrt{a + x}} = x\\) is equal to \\\\\n\\midrule\nResponse: <think> \\\\\nTo solve the equation \\(\\sqrt{a - \\sqrt{a + x}} = x\\), let's start by squaring both $\\cdots$ \\\\\n$\\left(\\sqrt{a - \\sqrt{a + x}}\\right)^2 = x^2 \\implies a - \\sqrt{a + x} = x^2.$ \\\\\nRearrange to isolate the inner square root term: \\\\\n$(a - x^2)^2 = a + x \\implies a^2 - 2ax^2 + (x^2)^2 = a + x \\implies x^4 - 2ax^2 - x + (a^2 - a) = 0$ \\\\\n\\ldots \\\\\n\\textcolor{red}{Wait, wait. Wait. That's an aha moment I can flag here}. \\\\\nLet's reevaluate this step-by-step to identify if the correct sum can be $\\cdots$ \\\\\nWe started with the equation: \\\\\n$\\sqrt{a - \\sqrt{a + x}} = x$ \\\\\nFirst, let's square both sides: \\\\\n$a - \\sqrt{a + x} = x^2 \\implies \\sqrt{a + x} = a - x^2$ \\\\\nNext, I could square both sides again, treating the equation:\n$\\cdots$\n\\\\\n\\ldots \\\\\n\\hline\n\\end{tabular}\n<PLACEHOLDER_CAP_7>\n\\label{tab:aha_moment}\n\\end{table}",
        "trans_content": "\\begin{table}[t]\n\\centering\n\\begin{tabular}{l}\n\\toprule\nQuestion: If \\(a > 1\\), then the sum of the real solutions of \\(\\sqrt{a - \\sqrt{a + x}} = x\\) is equal to \\\\\n\\midrule\nResponse: <think> \\\\\nTo solve the equation \\(\\sqrt{a - \\sqrt{a + x}} = x\\), let's start by squaring both $\\cdots$ \\\\\n$\\left(\\sqrt{a - \\sqrt{a + x}}\\right)^2 = x^2 \\implies a - \\sqrt{a + x} = x^2.$ \\\\\nRearrange to isolate the inner square root term: \\\\\n$(a - x^2)^2 = a + x \\implies a^2 - 2ax^2 + (x^2)^2 = a + x \\implies x^4 - 2ax^2 - x + (a^2 - a) = 0$ \\\\\n\\ldots \\\\\n\\textcolor{red}{Wait, wait. Wait. That's an aha moment I can flag here}. \\\\\nLet's reevaluate this step-by-step to identify if the correct sum can be $\\cdots$ \\\\\nWe started with the equation: \\\\\n$\\sqrt{a - \\sqrt{a + x}} = x$ \\\\\nFirst, let's square both sides: \\\\\n$a - \\sqrt{a + x} = x^2 \\implies \\sqrt{a + x} = a - x^2$ \\\\\nNext, I could square both sides again, treating the equation:\n$\\cdots$\n\\\\\n\\ldots \\\\\n\\hline\n\\end{tabular}\n<PLACEHOLDER_CAP_7>\n\\label{tab:aha_moment}\n\\end{table}",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_19>",
        "env_name": "itemize",
        "content": "\\begin{itemize}[topsep=0pt]\n    \\item\nReadability: A key limitation of \\dsro{} is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for \\dsri{}, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as |special\\_token|<reasoning\\_process>|special\\_token|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results.\n\n \\item\nPotential: By carefully designing the pattern for cold-start data with human priors, we observe better performance against \\dsro{}. We believe the iterative training is a better way for reasoning models.\n\\end{itemize}",
        "trans_content": "\\begin{itemize}[topsep=0pt]\n    \\item\n可读性：\\dsro{} 的一个关键限制是其内容通常不适合阅读。响应可能混合多种语言或缺乏用于突出显示用户答案的 markdown 格式。相比之下，在为 \\dsri{} 创建冷启动数据时，我们设计了一种可读的模式，在每个响应末尾包括摘要，并过滤掉不易阅读的响应。在这里，我们将输出格式定义为 | special\\_token| <reasoning\\_process> | special\\_token| <summary> ，其中推理过程是查询的 CoT，摘要用于总结推理结果。\n\n \\item\n潜力：通过精心设计具有人工先验的冷启动数据模式，我们观察到相较于 \\dsro{} 的更好表现。我们相信迭代训练对于推理模型来说是更好的方式。\n\\end{itemize}",
        "need_trans": true
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_20>",
        "env_name": "table",
        "content": "\\begin{table}[h]\n    \\centering\n    \\footnotesize\n    \\setlength{\\tabcolsep}{1.9pt}\n    \\begin{tabular}{@{}c l | c  c  c | c c |c c@{}}\n    \\toprule\n    & \\multirow{2}{*}{\\centering \\textbf{Benchmark {\\tiny (Metric)}}}  & \\textbf{Claude-3.5-}  & \\textbf{GPT-4o}& \\textbf{DeepSeek} & \\textbf{OpenAI} & \\textbf{OpenAI} & \\textbf{DeepSeek}\\\\\n    & & \\textbf{Sonnet-1022}  & \\textbf{0513} & \\textbf{V3} & \\textbf{o1-mini}& \\textbf{o1-1217} &\\textbf{R1} \\\\\n    \\midrule\n    & Architecture &-&- & MoE &-&-& MoE \\\\\n    & \\# Activated Params& -&-& 37B&-&- & 37B \\\\\n    & \\# Total Params &-&-& 671B&-&- & 671B \\\\\n    \\midrule\n    \\multirow{10}{*}{English}& MMLU {\\tiny (Pass@1)} & 88.3&87.2 & 88.5 & 85.2 & \\textbf{91.8} & 90.8\\\\\n     & MMLU-Redux {\\tiny (EM)}& 88.9& 88.0 & 89.1 & 86.7&- & \\textbf{92.9} \\\\\n    & MMLU-Pro {\\tiny (EM)}  & 78.0 & 72.6 & 75.9 & 80.3 &-& \\textbf{84.0} \\\\\n    & DROP {\\tiny (3-shot F1)}  & 88.3 & 83.7 & 91.6 & 83.9 & 90.2 & \\textbf{92.2}\\\\\n    & IF-Eval {\\tiny (Prompt Strict)}  & \\textbf{86.5} & 84.3 & 86.1 & 84.8&- & 83.3 \\\\\n    & GPQA Diamond {\\tiny (Pass@1)}& 65.0 & 49.9 & 59.1 & 60.0 & \\textbf{75.7} & 71.5&  \\\\\n    & SimpleQA {\\tiny (Correct)} & 28.4 & 38.2& 24.9 & 7.0 & \\textbf{47.0} & 30.1 \\\\\n     & FRAMES {\\tiny (Acc.)}  & 72.5 & 80.5 & 73.3 & 76.9 & -&\\textbf{82.5}\\\\\n      & AlpacaEval2.0 {\\tiny (LC-winrate)}  & 52.0 &  51.1 & 70.0 & 57.8 & - & \\textbf{87.6}\\\\\n       & ArenaHard {\\tiny (GPT-4-1106)}  & 85.2 & 80.4 & 85.5 & 92.0 & - & \\textbf{92.3}\\\\\n    \\midrule\n    \\multirow{4}{*}{Code} & LiveCodeBench {\\tiny (Pass@1-COT)} & 38.9 & 32.9 & 36.2 & 53.8 & 63.4 & \\textbf{65.9} \\\\\n    & Codeforces {\\tiny (Percentile)}& 20.3 & 23.6 & 58.7 & 93.4 & \\textbf{96.6} & 96.3 \\\\\n    & Codeforces {\\tiny (Rating)}& 717 & 759 & 1134 & 1820 & \\textbf{2061} & 2029 \\\\\n    & SWE Verified {\\tiny (Resolved)} & \\textbf{50.8}&38.8&42.0 & 41.6 & 48.9 & 49.2\\\\\n    & Aider-Polyglot {\\tiny (Acc.)} & 45.3&16.0& 49.6 & 32.9 & \\textbf{61.7}&53.3\\\\\n    \\midrule\n    \\multirow{3}{*}{Math} & AIME 2024 {\\tiny (Pass@1)}  & 16.0 & 9.3 & 39.2 & 63.6 & 79.2 & \\textbf{79.8} \\\\\n    & MATH-500 {\\tiny (Pass@1)} &78.3 & 74.6&90.2 & 90.0 & 96.4 & \\textbf{97.3} \\\\\n    & CNMO 2024 {\\tiny (Pass@1)} & 13.1 & 10.8 &43.2 & 67.6 & - & \\textbf{78.8} \\\\\n    \\midrule\n    \\multirow{3}{*}{Chinese} & CLUEWSC {\\tiny (EM)}&  85.4 & 87.9 & 90.9 & 89.9 & - &\\textbf{92.8}\\\\\n    & C-Eval {\\tiny (EM)} & 76.7 & 76.0 & 86.5 & 68.9 & - & \\textbf{91.8}\\\\\n     & C-SimpleQA {\\tiny (Correct)}  & 55.4 & 58.7 & \\textbf{68.0} & 40.3 & -& 63.7 \\\\\n    \\bottomrule\n    \\end{tabular}\n    <PLACEHOLDER_CAP_8>\n    \\label{tab:main}\n\\end{table}",
        "trans_content": "\\begin{table}[h]\n    \\centering\n    \\footnotesize\n    \\setlength{\\tabcolsep}{1.9pt}\n    \\begin{tabular}{@{}c l | c  c  c | c c |c c@{}}\n    \\toprule\n    & \\multirow{2}{*}{\\centering \\textbf{Benchmark {\\tiny (Metric)}}}  & \\textbf{Claude-3.5-}  & \\textbf{GPT-4o}& \\textbf{DeepSeek} & \\textbf{OpenAI} & \\textbf{OpenAI} & \\textbf{DeepSeek}\\\\\n    & & \\textbf{Sonnet-1022}  & \\textbf{0513} & \\textbf{V3} & \\textbf{o1-mini}& \\textbf{o1-1217} &\\textbf{R1} \\\\\n    \\midrule\n    & Architecture &-&- & MoE &-&-& MoE \\\\\n    & \\# Activated Params& -&-& 37B&-&- & 37B \\\\\n    & \\# Total Params &-&-& 671B&-&- & 671B \\\\\n    \\midrule\n    \\multirow{10}{*}{English}& MMLU {\\tiny (Pass@1)} & 88.3&87.2 & 88.5 & 85.2 & \\textbf{91.8} & 90.8\\\\\n     & MMLU-Redux {\\tiny (EM)}& 88.9& 88.0 & 89.1 & 86.7&- & \\textbf{92.9} \\\\\n    & MMLU-Pro {\\tiny (EM)}  & 78.0 & 72.6 & 75.9 & 80.3 &-& \\textbf{84.0} \\\\\n    & DROP {\\tiny (3-shot F1)}  & 88.3 & 83.7 & 91.6 & 83.9 & 90.2 & \\textbf{92.2}\\\\\n    & IF-Eval {\\tiny (Prompt Strict)}  & \\textbf{86.5} & 84.3 & 86.1 & 84.8&- & 83.3 \\\\\n    & GPQA Diamond {\\tiny (Pass@1)}& 65.0 & 49.9 & 59.1 & 60.0 & \\textbf{75.7} & 71.5&  \\\\\n    & SimpleQA {\\tiny (Correct)} & 28.4 & 38.2& 24.9 & 7.0 & \\textbf{47.0} & 30.1 \\\\\n     & FRAMES {\\tiny (Acc.)}  & 72.5 & 80.5 & 73.3 & 76.9 & -&\\textbf{82.5}\\\\\n      & AlpacaEval2.0 {\\tiny (LC-winrate)}  & 52.0 &  51.1 & 70.0 & 57.8 & - & \\textbf{87.6}\\\\\n       & ArenaHard {\\tiny (GPT-4-1106)}  & 85.2 & 80.4 & 85.5 & 92.0 & - & \\textbf{92.3}\\\\\n    \\midrule\n    \\multirow{4}{*}{Code} & LiveCodeBench {\\tiny (Pass@1-COT)} & 38.9 & 32.9 & 36.2 & 53.8 & 63.4 & \\textbf{65.9} \\\\\n    & Codeforces {\\tiny (Percentile)}& 20.3 & 23.6 & 58.7 & 93.4 & \\textbf{96.6} & 96.3 \\\\\n    & Codeforces {\\tiny (Rating)}& 717 & 759 & 1134 & 1820 & \\textbf{2061} & 2029 \\\\\n    & SWE Verified {\\tiny (Resolved)} & \\textbf{50.8}&38.8&42.0 & 41.6 & 48.9 & 49.2\\\\\n    & Aider-Polyglot {\\tiny (Acc.)} & 45.3&16.0& 49.6 & 32.9 & \\textbf{61.7}&53.3\\\\\n    \\midrule\n    \\multirow{3}{*}{Math} & AIME 2024 {\\tiny (Pass@1)}  & 16.0 & 9.3 & 39.2 & 63.6 & 79.2 & \\textbf{79.8} \\\\\n    & MATH-500 {\\tiny (Pass@1)} &78.3 & 74.6&90.2 & 90.0 & 96.4 & \\textbf{97.3} \\\\\n    & CNMO 2024 {\\tiny (Pass@1)} & 13.1 & 10.8 &43.2 & 67.6 & - & \\textbf{78.8} \\\\\n    \\midrule\n    \\multirow{3}{*}{Chinese} & CLUEWSC {\\tiny (EM)}&  85.4 & 87.9 & 90.9 & 89.9 & - &\\textbf{92.8}\\\\\n    & C-Eval {\\tiny (EM)} & 76.7 & 76.0 & 86.5 & 68.9 & - & \\textbf{91.8}\\\\\n     & C-SimpleQA {\\tiny (Correct)}  & 55.4 & 58.7 & \\textbf{68.0} & 40.3 & -& 63.7 \\\\\n    \\bottomrule\n    \\end{tabular}\n    <PLACEHOLDER_CAP_8>\n    \\label{tab:main}\n\\end{table}",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_21>",
        "env_name": "table",
        "content": "\\begin{table}[h]\n    \\centering\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{@{}l *{6}{c} @{}}\n    \\toprule\n    \\multirow{3}{*}{\\centering\\textbf{Model}} & \\multicolumn{2}{c}{\\multirow{2}{*}{\\textbf{AIME 2024}}} & \\multirow{2}{*}{\\textbf{MATH-500}} & \\textbf{GPQA} & \\textbf{LiveCode} & \\multirow{2}{*}{\\textbf{CodeForces}} \\\\\n    &  &  &  & \\textbf{Diamond} & \\textbf{Bench} \\\\\n    \\cmidrule(lr){2-3}\n     & pass@1 & cons@64 & pass@1 & pass@1 & pass@1 & rating \\\\\n    \\midrule\n    \\textbf{GPT-4o-0513} & 9.3 & 13.4 & 74.6  & 49.9 & 32.9 &  759\\\\\n    \\textbf{Claude-3.5-Sonnet-1022} & 16.0 & 26.7 & 78.3  & 65.0 & 38.9 &  717\\\\\n    \\textbf{OpenAI-o1-mini} & 63.6 & 80.0 & 90.0 &  60.0 & 53.8 &  \\textbf{1820}\\\\\n    \\textbf{QwQ-32B-Preview} & 50.0 & 60.0 & 90.6 & 54.5 & 41.9 &  1316 \\\\\n    \\midrule\n    \\textbf{DeepSeek-R1-Distill-Qwen-1.5B} & 28.9 & 52.7 & 83.9 & 33.8 & 16.9 & 954 \\\\\n    \\textbf{DeepSeek-R1-Distill-Qwen-7B} & 55.5 & 83.3 & 92.8 & 49.1 & 37.6 & 1189 \\\\\n    \\textbf{DeepSeek-R1-Distill-Qwen-14B} & 69.7 & 80.0 & 93.9 &  59.1 & 53.1 & 1481 \\\\\n    \\textbf{DeepSeek-R1-Distill-Qwen-32B} & \\textbf{72.6} & {83.3} & {94.3} & {62.1} & {57.2} & 1691 \\\\\n    \\textbf{DeepSeek-R1-Distill-Llama-8B} & 50.4 & 80.0 & 89.1 & 49.0 & 39.6 & 1205 \\\\\n    \\textbf{DeepSeek-R1-Distill-Llama-70B} & 70.0 & \\textbf{86.7} & \\textbf{94.5} & \\textbf{65.2} & \\textbf{57.5} & 1633 \\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n    <PLACEHOLDER_CAP_9>\n    \\label{tab:distill}\n\\end{table}",
        "trans_content": "\\begin{table}[h]\n    \\centering\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{@{}l *{6}{c} @{}}\n    \\toprule\n    \\multirow{3}{*}{\\centering\\textbf{Model}} & \\multicolumn{2}{c}{\\multirow{2}{*}{\\textbf{AIME 2024}}} & \\multirow{2}{*}{\\textbf{MATH-500}} & \\textbf{GPQA} & \\textbf{LiveCode} & \\multirow{2}{*}{\\textbf{CodeForces}} \\\\\n    &  &  &  & \\textbf{Diamond} & \\textbf{Bench} \\\\\n    \\cmidrule(lr){2-3}\n     & pass@1 & cons@64 & pass@1 & pass@1 & pass@1 & rating \\\\\n    \\midrule\n    \\textbf{GPT-4o-0513} & 9.3 & 13.4 & 74.6  & 49.9 & 32.9 &  759\\\\\n    \\textbf{Claude-3.5-Sonnet-1022} & 16.0 & 26.7 & 78.3  & 65.0 & 38.9 &  717\\\\\n    \\textbf{OpenAI-o1-mini} & 63.6 & 80.0 & 90.0 &  60.0 & 53.8 &  \\textbf{1820}\\\\\n    \\textbf{QwQ-32B-Preview} & 50.0 & 60.0 & 90.6 & 54.5 & 41.9 &  1316 \\\\\n    \\midrule\n    \\textbf{DeepSeek-R1-Distill-Qwen-1.5B} & 28.9 & 52.7 & 83.9 & 33.8 & 16.9 & 954 \\\\\n    \\textbf{DeepSeek-R1-Distill-Qwen-7B} & 55.5 & 83.3 & 92.8 & 49.1 & 37.6 & 1189 \\\\\n    \\textbf{DeepSeek-R1-Distill-Qwen-14B} & 69.7 & 80.0 & 93.9 &  59.1 & 53.1 & 1481 \\\\\n    \\textbf{DeepSeek-R1-Distill-Qwen-32B} & \\textbf{72.6} & {83.3} & {94.3} & {62.1} & {57.2} & 1691 \\\\\n    \\textbf{DeepSeek-R1-Distill-Llama-8B} & 50.4 & 80.0 & 89.1 & 49.0 & 39.6 & 1205 \\\\\n    \\textbf{DeepSeek-R1-Distill-Llama-70B} & 70.0 & \\textbf{86.7} & \\textbf{94.5} & \\textbf{65.2} & \\textbf{57.5} & 1633 \\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n    <PLACEHOLDER_CAP_9>\n    \\label{tab:distill}\n\\end{table}",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_22>",
        "env_name": "table",
        "content": "\\begin{table}[h]\n    \\centering\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{@{}l *{6}{c} @{}}\n    \\toprule\n    \\multirow{3}{*}{\\centering\\textbf{Model}} & \\multicolumn{2}{c}{\\textbf{AIME 2024}} & \\textbf{MATH-500} & \\textbf{GPQA Diamond} & \\textbf{LiveCodeBench}  \\\\\n\n    \\cmidrule(lr){2-3}\n     & pass@1 & cons@64 & pass@1& pass@1 & pass@1 \\\\\n    \\midrule\n    \\textbf{QwQ-32B-Preview} & 50.0 & 60.0 & 90.6 & 54.5 & 41.9  \\\\\n    \\textbf{DeepSeek-R1-Zero-Qwen-32B} & 47.0 & 60.0 & 91.6  & 55.0 & 40.2  \\\\\n    \\textbf{DeepSeek-R1-Distill-Qwen-32B} & \\bf{72.6} & \\bf{83.3} & \\bf{94.3}  & \\bf{62.1} & \\bf{57.2}\\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n    <PLACEHOLDER_CAP_10>\n    \\label{tab:distill_vs_rl}\n\\end{table}",
        "trans_content": "\\begin{table}[h]\n    \\centering\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{@{}l *{6}{c} @{}}\n    \\toprule\n    \\multirow{3}{*}{\\centering\\textbf{Model}} & \\multicolumn{2}{c}{\\textbf{AIME 2024}} & \\textbf{MATH-500} & \\textbf{GPQA Diamond} & \\textbf{LiveCodeBench}  \\\\\n\n    \\cmidrule(lr){2-3}\n     & pass@1 & cons@64 & pass@1& pass@1 & pass@1 \\\\\n    \\midrule\n    \\textbf{QwQ-32B-Preview} & 50.0 & 60.0 & 90.6 & 54.5 & 41.9  \\\\\n    \\textbf{DeepSeek-R1-Zero-Qwen-32B} & 47.0 & 60.0 & 91.6  & 55.0 & 40.2  \\\\\n    \\textbf{DeepSeek-R1-Distill-Qwen-32B} & \\bf{72.6} & \\bf{83.3} & \\bf{94.3}  & \\bf{62.1} & \\bf{57.2}\\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n    <PLACEHOLDER_CAP_10>\n    \\label{tab:distill_vs_rl}\n\\end{table}",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_23>",
        "env_name": "itemize",
        "content": "\\begin{itemize}[topsep=0pt]\n    \\item \\textbf{General Capability:}\n  Currently, the capabilities of \\dsri{} fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields.\n    \\item \\textbf{Language Mixing:}\n\\dsri{} is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, \\dsri{} might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates.\n \\item \\textbf{Prompting Engineering:} When evaluating \\dsri{}, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly describe the problem and specify the output format using a zero-shot setting for optimal results.\n\\item  \\textbf{Software Engineering Tasks:}\nDue to the long evaluation times, which impact the efficiency of the RL process, large-scale RL has not been applied extensively in software engineering tasks. As a result, DeepSeek-R1 has not demonstrated a huge improvement over DeepSeek-V3 on software engineering benchmarks. Future versions will address this by implementing rejection sampling on software engineering data or incorporating asynchronous evaluations during the RL process to improve efficiency.\n\n\\end{itemize}",
        "trans_content": "\\begin{itemize}[topsep=0pt]\n    \\item \\textbf{总体能力:}\n  目前，\\dsri{} 的能力在函数调用、多轮对话、复杂角色扮演和 JSON 输出等任务上落后于 DeepSeek-V3。未来，我们计划探索如何利用长 CoT 来增强这些领域的任务。\n    \\item \\textbf{语言混合:}\n\\dsri{} 当前针对中文和英文进行了优化，这可能导致在处理其他语言查询时出现语言混合问题。例如，即使查询是用英语或中文以外的语言进行的，\\dsri{} 可能会使用英语进行推理和响应。我们计划在未来更新中解决这一限制。\n \\item \\textbf{提示工程:} 在评估 \\dsri{} 时，我们发现它对提示非常敏感。几次提示会持续降低其性能。因此，我们建议用户直接描述问题并使用零次提示设置指定输出格式，以获得最佳结果。\n\\item  \\textbf{软件工程任务:}\n由于较长的评估时间影响了 RL 过程的效率，大规模 RL 尚未广泛应用于软件工程任务。因此，DeepSeek-R1 在软件工程基准测试上未能表现出比 DeepSeek-V3 更大的改进。未来版本将通过在软件工程数据上实施拒绝采样或在 RL 过程中结合异步评估来提高效率。\n\n\\end{itemize}",
        "need_trans": true
    }
]