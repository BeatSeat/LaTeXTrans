[
    {
        "placeholder": "<PLACEHOLDER_CAP_1>",
        "cap_type": "title",
        "content": "\\title{\\centering DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}",
        "trans_content": "\\title{\\centering DeepSeek-R1：通过强化学习激励LLMs的推理能力}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_2>",
        "cap_type": "caption",
        "content": "\\caption{\n    \\centering\n    Benchmark performance of \\dsri{}.\n}",
        "trans_content": "\\caption{\n    \\centering\n    \\dsri{} 的基准性能。\n}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_3>",
        "cap_type": "caption",
        "content": "\\caption{Template for \\dsro{}. \\textcolor{red}{prompt} will be replaced with the specific reasoning question during training.}",
        "trans_content": "\\caption{ \\dsro{} 的模板。 \\textcolor{red}{prompt} 将在训练中被特定的推理问题替换。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_4>",
        "cap_type": "caption",
        "content": "\\caption{Comparison of \\dsro{} and OpenAI o1 models on reasoning-related benchmarks.}",
        "trans_content": "\\caption{在推理相关基准测试中 \\dsro{} 和 OpenAI o1 模型的比较。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_5>",
        "cap_type": "caption",
        "content": "\\caption{AIME accuracy of \\dsro{} during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation.}",
        "trans_content": "\\caption{\\dsro{} 在训练期间的 AIME 准确性。对于每个问题，我们采样 16 个回答并计算总体平均准确性，以确保评估的稳定性。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_6>",
        "cap_type": "caption",
        "content": "\\caption{The average response length of \\dsro{} on the training set during the RL process. \\dsro{} naturally learns to solve reasoning tasks with more thinking time.}",
        "trans_content": "\\caption{\\dsro{} 在训练集上的平均响应长度在 RL 过程中的变化。 \\dsro{} 自然地学习在推理任务中使用更多的思考时间。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_7>",
        "cap_type": "caption",
        "content": "\\caption{An interesting ``aha moment'' of an intermediate version of \\dsro{}. The model learns to rethink using an anthropomorphic tone. This is also an aha moment for us, allowing us to witness the power and beauty of reinforcement learning.}",
        "trans_content": "\\caption{一个有趣的 \\dsro{} 中间版本的 ``aha 时刻''。模型学会用拟人化的语气重新思考。这对我们来说也是一个 aha 时刻，让我们见证了强化学习的力量和美妙。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_8>",
        "cap_type": "caption",
        "content": "\\caption{ Comparison between \\dsri{} and other representative models.\n    }",
        "trans_content": "\\caption{ \\dsri{} 与其他典型模型的比较。 }"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_9>",
        "cap_type": "caption",
        "content": "\\caption{Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks.}",
        "trans_content": "\\caption{DeepSeek-R1 蒸馏模型与其他可比较模型在推理相关基准上的比较。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_10>",
        "cap_type": "caption",
        "content": "\\caption{\\centering Comparison of distilled and RL Models on Reasoning-Related Benchmarks.}",
        "trans_content": "\\caption{\\centering 蒸馏模型和 RL 模型在推理相关基准上的比较。}"
    }
]