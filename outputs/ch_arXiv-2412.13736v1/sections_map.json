[
    {
        "section": 0,
        "content": "\\pdfoutput=1\n\n\\documentclass[11pt]{article}\n\n\\usepackage[final]{acl}\n\n\\usepackage{times}\n\\usepackage{latexsym}\n\\usepackage{xcolor}\n\\usepackage{tcolorbox}\n\n\\usepackage{tabularx}\n\\usepackage{booktabs}\n\\usepackage{enumitem}\n\\usepackage[T1]{fontenc}\n\n\\usepackage[utf8]{inputenc}\n\n\\usepackage{microtype}\n\\usepackage{amsmath}\n\\usepackage{amssymb}\n\\usepackage{colortbl}\n\\usepackage{booktabs}\n\\usepackage{pifont}\n\\usepackage{xcolor}\n\\usepackage{multirow}\n\\usepackage{inconsolata}\n\n\\usepackage{graphicx}\n\\definecolor{darkgreen}{rgb}{0.0, 0.5, 0.0}\n<PLACEHOLDER_NEWCOMMAND_0>\n\n\\hypersetup{\n    colorlinks=true,\n    linkcolor=red,\n    citecolor=cyan,\n    filecolor=magenta,\n    urlcolor=cyan,\n    }\n\n<PLACEHOLDER_CAP_1>\n\n\\author{\n \\textbf{Jiaxiang Liu\\textsuperscript{1}}\\ \\ \\\n \\textbf{Yuan Wang\\textsuperscript{1}}\\ \\ \\\n \\textbf{Jiawei Du\\textsuperscript{2, 3}}\\ \\ \\\n \\textbf{Joey Tianyi Zhou\\textsuperscript{2, 3}}\\ \\ \\\n \\textbf{Zuozhu Liu\\textsuperscript{*, 1}}\n\\\\\n\\\\\n \\textsuperscript{1} \\small ZJU-Angelalign R\\&D Center for Intelligence Healthcare, Zhejiang University, China\\\\\n \\textsuperscript{2} \\small Centre for Frontier AI Research (CFAR), Agency for Science, Technology and Research (A*STAR), Singapore\n\\\\\n \\textsuperscript{3} \\small Institute of High Performance Computing (IHPC), Agency for Science, Technology and Research (A*STAR), Singapore\n\\\\\n { \\small\n   {\\tt \\{jiaxiang.21, zuozhuliu\\}@intl.zju.edu.cn}\n }\n}",
        "trans_content": "\\pdfoutput=1\n\n\\documentclass[11pt]{article}\n\n\\usepackage[final]{acl}\n\n\\usepackage{times}\n\\usepackage{latexsym}\n\\usepackage{xcolor}\n\\usepackage{tcolorbox}\n\n\\usepackage{tabularx}\n\\usepackage{booktabs}\n\\usepackage{enumitem}\n\\usepackage[T1]{fontenc}\n\n\\usepackage[utf8]{inputenc}\n\n\\usepackage{microtype}\n\\usepackage{amsmath}\n\\usepackage{amssymb}\n\\usepackage{colortbl}\n\\usepackage{booktabs}\n\\usepackage{pifont}\n\\usepackage{xcolor}\n\\usepackage{multirow}\n\\usepackage{inconsolata}\n\n\\usepackage{graphicx}\n\\definecolor{darkgreen}{rgb}{0.0, 0.5, 0.0}\n<PLACEHOLDER_NEWCOMMAND_0>\n\n\\hypersetup{\n    colorlinks=true,\n    linkcolor=red,\n    citecolor=cyan,\n    filecolor=magenta,\n    urlcolor=cyan,\n    }\n\n<PLACEHOLDER_CAP_1>\n\n\\author{\n \\textbf{Jiaxiang Liu\\textsuperscript{1}}\\ \\ \\\n \\textbf{Yuan Wang\\textsuperscript{1}}\\ \\ \\\n \\textbf{Jiawei Du\\textsuperscript{2, 3}}\\ \\ \\\n \\textbf{Joey Tianyi Zhou\\textsuperscript{2, 3}}\\ \\ \\\n \\textbf{Zuozhu Liu\\textsuperscript{*, 1}}\n\\\\\n\\\\\n \\textsuperscript{1} \\small ZJU-Angelalign R\\&D Center for Intelligence Healthcare, Zhejiang University, China\\\\\n \\textsuperscript{2} \\small Centre for Frontier AI Research (CFAR), Agency for Science, Technology and Research (A*STAR), Singapore\n\\\\\n \\textsuperscript{3} \\small Institute of High Performance Computing (IHPC), Agency for Science, Technology and Research (A*STAR), Singapore\n\\\\\n { \\small\n   {\\tt \\{jiaxiang.21, zuozhuliu\\}@intl.zju.edu.cn}\n }\n}"
    },
    {
        "section": 0,
        "content": "\\begin{document}\n\n\\maketitle\n\n<PLACEHOLDER_ENV_1>",
        "trans_content": "\\begin{document}\n\n\\maketitle\n\n<PLACEHOLDER_ENV_1>"
    },
    {
        "section": "1",
        "content": "\\section{Introduction}\n\n<PLACEHOLDER_ENV_2>\n\nMedical Visual Question Answering (Med-VQA) has recently gained significant attention \\cite{chen2022align, gong2021cross, ren2020cgmvqa, khare2021mmbert}. As a new exploration in the medical domain, Med-VQA aims to answer medical questions in natural language based on input medical images. An effective Med-VQA system can assist clinicians in interpreting medical images, thereby ensuring and accelerating the diagnostic process. For patients, automated Med-VQA services can greatly satisfy the demand for personalized health consultations \\cite{liu2023parameter}.\n\nIn the field of Med-VQA, numerous attempts have been made using deep learning technologies \\cite{tiong2022plug, banerjee2020weaqa, changpinyo2022all, liu2023chatgpt, gai2024medthink}.\nFor instance, \\citet{nguyen2019overcoming} utilized Bilinear Attention Networks (BAN) \\cite{kim2018bilinear} and enhanced them for Med-VQA by incorporating a Mixed Enhanced Visual Feature (MEVF) setup consisting of pre-trained meta-learning modules and Convolutional Denoising Autoencoders (CDAE). Building on this, \\citet{zhan2020medical} designed a conditional reasoning framework to boost the inference capabilities of Med-VQA models. However, these approaches often underperform in many practical scenarios, primarily due to poor capabilities in extracting and integrating features from a limited number of medical images and text data \\cite{eslami2021does, song2022clip, wang2022clip}.\n\\citet{eslami2021does} introduced the CLIP architecture into the framework by deploying it as the visual encoder within MEVF \\cite{nguyen2019overcoming}, pre-trained on the multimodal medical dataset ROCO \\cite{pelka2018radiology}. Their experiments demonstrated significant improvements with the CLIP. \\citet{liu2023parameter} developed VQA-Adapter, which uses a lightweight adapter and label smoothing to efficiently fine-tune the CLIP model for Med-VQA, thus reducing computational costs and mitigating overfitting.\n\\citet{li2024llava} proposed LLaVA-Med, which utilizes GPT-4 and a novel curriculum learning approach to efficiently train LLaVA on biomedical images, significantly enhancing Med-VQA capabilities.\n\nHowever, previous Med-VQA approaches typically focused on the accuracy of the answers \\cite{nguyen2019overcoming,liu2023parameter,zhan2020medical}\n, where most MedVQA responses consist of a simplistic answer lacking detailed explanations or rationale, unlike in real-world scenarios where doctors not only provide answers but also explain their reasoning, professional considerations, and potential contradictions to derive a more comprehensive diagnostic insight.\nBesides, real-world diagnostics often rely on the combined experience of multiple doctors, as a single doctor's diagnosis may be biased by personal experience and may not be sufficiently accurate.\nIn the multimodal Chain of Thought (CoT), answering VQA questions involves providing an answer as well as a corresponding reasoning path (rationale).\nThe generation of this rationale helps to improve the accuracy of the language model.\nInspired by real-world practices and multimodal CoT \\citet{zhang2023multimodal,zheng2023ddcot}, integrating this paradigm into Med-VQA can enhance both the accuracy and interpretability of responses. However, implementing it faces several challenges: (1) Previous CoT methods required manual annotation of fundamental rationales, which is time-consuming, costly, and challenging to ensure consistency and completeness \\cite{zhang2023multimodal,zheng2023ddcot}. (2) Reliance on a single expert model can lead to misleading conclusions. (3) Multimodal CoT has limited depth in understanding the intents of images and texts, which can restrict its effectiveness in medical contexts \\cite{zhang2023multimodal}.\n\nTo address the aforementioned issues, we introduce MedCoT, a hierarchical expert-verified model for Med-VQA. Firstly, the Initial Specialist proposes preliminary diagnostic rationale based on the medical visual and text query. The Follow-up Specialist then reviews these rationales, categorizing them as valid or invalid; valid rationales are retained, while invalid ones are reassessed.\nFinally, the locally implemented Diagnostic Specialist, consisting of a sparse Mixture of Experts (MoE) model functioning as a multimodal language model, casts votes to deliver the definitive diagnosis.\nLeveraging a hierarchy of expertise, MedCoT consistently outperforms state-of-the-art (SoTA) Med-VQA methods across four extensive datasets, demonstrating impressive generalizability and interpretability, as shown in \\autoref{fig1}.\nOur study makes three significant contributions:\n\n<PLACEHOLDER_ENV_3>\n\n\\vspace{1em}\n\n<PLACEHOLDER_ENV_4>",
        "trans_content": "\\section{引言}\n\n<PLACEHOLDER_ENV_2>\n\n医学视觉问答（Med-VQA）最近引起了广泛关注 \\cite{chen2022align, gong2021cross, ren2020cgmvqa, khare2021mmbert}。作为医学领域的新探索，Med-VQA旨在基于输入的医学图像回答自然语言的医学问题。一个有效的Med-VQA系统可以帮助临床医生解释医学图像，从而确保和加速诊断过程。对于患者来说，自动化的Med-VQA服务可以极大地满足个性化健康咨询的需求 \\cite{liu2023parameter}。\n\n在Med-VQA领域，已经有许多基于深度学习技术的尝试 \\cite{tiong2022plug, banerjee2020weaqa, changpinyo2022all, liu2023chatgpt, gai2024medthink}。例如，\\citet{nguyen2019overcoming} 使用双线性注意网络（BAN）\\cite{kim2018bilinear}，并通过结合预训练元学习模块和卷积去噪自编码器（CDAE）组成的混合增强视觉特征（MEVF）设置对其进行了增强。基于此，\\citet{zhan2020medical} 设计了一个条件推理框架，以提升Med-VQA模型的推理能力。然而，这些方法在许多实际场景中往往表现不佳，主要是因为在从有限数量的医学图像和文本数据中提取和整合特征方面能力不足 \\cite{eslami2021does, song2022clip, wang2022clip}。\n\\citet{eslami2021does} 引入了CLIP架构，将其作为MEVF中的视觉编码器 \\cite{nguyen2019overcoming}，并在多模态医学数据集ROCO上进行预训练 \\cite{pelka2018radiology}。他们的实验表明，CLIP显著提高了性能。\\citet{liu2023parameter} 开发了VQA-Adapter，使用轻量级适配器和标签平滑技术高效微调CLIP模型用于Med-VQA，从而降低了计算成本并减轻了过拟合。\n\\citet{li2024llava} 提出了LLaVA-Med，利用GPT-4和一种新的课程学习方法高效训练LLaVA在生物医学图像上的表现，显著增强了Med-VQA的能力。\n\n然而，以往的Med-VQA方法通常关注于答案的准确性 \\cite{nguyen2019overcoming,liu2023parameter,zhan2020medical}，其中大多数MedVQA响应只包含简单的答案，缺乏详细的解释或推理，不同于现实世界中医生不仅提供答案，还解释其推理、专业考虑和可能的矛盾，以得出更全面的诊断见解。\n此外，现实世界的诊断往往依赖于多位医生的综合经验，因为单一医生的诊断可能会受到个人经验的偏见，可能不够准确。\n在多模态思维链（CoT）中，回答VQA问题涉及提供答案以及相应的推理路径（理由）。\n这种理由的生成有助于提高语言模型的准确性。\n受现实世界实践和多模态CoT \\citet{zhang2023multimodal,zheng2023ddcot} 的启发，将这种范式整合到Med-VQA中可以增强答案的准确性和可解释性。然而，实施它面临几个挑战：（1）以前的CoT方法需要手动标注基本理由，这既耗时又昂贵，且难以保证一致性和完整性 \\cite{zhang2023multimodal,zheng2023ddcot}。（2）依赖单一专家模型可能导致误导性结论。（3）多模态CoT在理解图像和文本意图的深度上有限，这可能限制其在医学背景下的有效性 \\cite{zhang2023multimodal}。\n\n为了解决上述问题，我们引入了MedCoT，一种分层的专家验证模型用于Med-VQA。首先，初始专家根据医学视觉和文本查询提出初步诊断理由。随后，后续专家审核这些理由，将其分类为有效或无效；有效的理由被保留，而无效的则需要重新评估。\n最后，局部实现的诊断专家，由一个稀疏专家混合（MoE）模型组成，作为多模态语言模型进行投票，以提供明确的诊断。\n通过利用层级化的专业知识，MedCoT在四个大型数据集上持续优于最先进的（SoTA）Med-VQA方法，展示了令人印象深刻的泛化能力和可解释性，如\\autoref{fig1}所示。\n我们的研究做出了三个重要贡献：\n\n<PLACEHOLDER_ENV_3>\n\n\\vspace{1em}\n\n<PLACEHOLDER_ENV_4>"
    },
    {
        "section": "2",
        "content": "\\section{Related Work}\n\\vspace{-0.5em}",
        "trans_content": "\\section{相关工作}\n\\vspace{-0.5em}"
    },
    {
        "section": "2_1",
        "content": "\\subsection{Med-VQA}\nVQA is a multimodal task in computer vision and natural language processing, aimed at responding to queries about images in natural language \\cite{ben2019vqa,he2020pathvqa,ren2020cgmvqa}. It involves feature extraction, fusion, and inference to comprehend multimodal intents and manage feature processing. Med-VQA extends VQA into the medical domain, where robust medical knowledge is crucial for answering domain-specific questions \\cite{liu2023parameter}, thus complicating feature extraction. Innovations such as Nguyen et al.'s MEVF leverage unsupervised CDAE and meta-learning to initialize weights specifically for Med-VQA \\cite{nguyen2019overcoming}. Zhan et al. built upon this by developing a conditional reasoning framework to handle different types of questions \\cite{zhan2020medical}, while Eslami et al. successfully implemented the CLIP model as a visual encoder, proving its effectiveness in this context \\cite{eslami-etal-2023-pubmedclip}. LLaVA-Med utilizes GPT-4 and a novel curriculum learning approach for training on biomedical images \\citet{li2024llava}, significantly enhancing Med-VQA capabilities. While capable of interactive dialogue, its responses do not focus on the reasoning paths leading to the answers. MedCoT differs from the aforementioned methods by not only providing precise answers but also offering reasoning paths (rationale). Moreover, its validity is confirmed through Hierarchical Expert verification, aligning more closely with real-world medical scenarios.",
        "trans_content": "\\subsection{Med-VQA}\nVQA 是计算机视觉和自然语言处理中的一种多模态任务，旨在用自然语言对图像的查询进行响应 \\cite{ben2019vqa,he2020pathvqa,ren2020cgmvqa}。它涉及特征提取、融合和推理，以理解多模态意图并管理特征处理。Med-VQA 将 VQA 扩展到医学领域，在此领域中，强大的医学知识对于回答特定领域的问题至关重要 \\cite{liu2023parameter}，这使得特征提取更加复杂。Nguyen 等人的创新如 MEVF 利用无监督的 CDAE 和元学习来初始化专门针对 Med-VQA 的权重 \\cite{nguyen2019overcoming}。Zhan 等人基于此开发了一种条件推理框架来处理不同类型的问题 \\cite{zhan2020medical}，而 Eslami 等人成功地将 CLIP 模型作为视觉编码器实施，证明了其在该背景下的有效性 \\cite{eslami-etal-2023-pubmedclip}。LLaVA-Med 利用 GPT-4 和一种新颖的课程学习方法对生物医学图像进行训练 \\citet{li2024llava}，显著增强了 Med-VQA 的能力。虽然能够进行互动对话，但其回答并不注重通向答案的推理路径。MedCoT 不同于上述方法，它不仅提供精确的答案，还提供推理路径（理由）。此外，其有效性通过分层专家验证得到确认，更加符合现实世界的医疗场景。"
    },
    {
        "section": "2_2",
        "content": "\\subsection{Multimodal CoT}\nCoT reasoning with Large Language Models (LLMs) has shown success in natural language processing. Multimodal CoT combines visual information with traditional textual CoT, integrating comprehensive data to perform reasoning tasks \\cite{zhang2023multimodal,zheng2023ddcot}.\nGroundbreaking works in multimodal CoT  \\cite{zheng2023ddcot,zhang2023multimodal,lu2022learn,lu2023chameleon,zhang2023llama} are first examined on the ScienceQA dataset. ScienceQA includes multimodal scientific questions along with annotated rationales \\cite{lu2022learn}.\nMM-CoT developed a two-stage framework based on ScienceQA that trains models to generate rationales from annotations, which are then used to form final answers \\cite{lu2022learn}.\nWith the increasing integration of open-world knowledge in LLMs, research is focusing on equipping these models with visual modalities to tackle complex visual and multimodal challenges. For instance, DDCoT \\cite{zheng2023ddcot}, introduces role-specific Chains of Thought that decompose questions into subproblems and use LLMs to recombine principles, enhancing accuracy and addressing language illusions in multimodal contexts.\nInspired by these advancements, we aim to adapt multimodal CoT reasoning to the medical field, aiming to improve the explainability and accuracy of Med-VQA.",
        "trans_content": "\\subsection{多模态链式推理}\n使用大型语言模型（LLMs）的链式推理（CoT）在自然语言处理方面已显示出成功。多模态链式推理将视觉信息与传统的文本链式推理相结合，整合综合数据以执行推理任务 \\cite{zhang2023multimodal,zheng2023ddcot}。在多模态链式推理的开创性工作 \\cite{zheng2023ddcot,zhang2023multimodal,lu2022learn,lu2023chameleon,zhang2023llama} 中，首先在ScienceQA数据集上进行研究。ScienceQA包括多模态科学问题及带注释的推理 \\cite{lu2022learn}。MM-CoT开发了一个基于ScienceQA的两阶段框架，训练模型从注释中生成推理，随后用于形成最终答案 \\cite{lu2022learn}。 随着开放世界知识在LLMs中越来越多的整合，研究正致力于为这些模型配备视觉模态，以应对复杂的视觉和多模态挑战。例如，DDCoT \\cite{zheng2023ddcot} 引入了角色特定的链式推理，将问题分解为子问题，并使用LLMs重新组合原则，提高精度并在多模态环境中解决语言错觉。 受到这些进展的启发，我们旨在将多模态链式推理适用于医学领域，以提高Med-VQA的可解释性和准确性。"
    },
    {
        "section": "2_3",
        "content": "\\subsection{MoE}\nMoE optimizes learning and prediction by combining multiple expert networks and using a gating network to determine which experts are activated based on the given input \\cite{zhang2024scalable,fedus2022switch}. Sparse MoE, a variant of the MoE model, activates only a few experts during each prediction, thus efficiently utilizing computational resources and enhancing scalability \\cite{shazeer2016outrageously}.\nSparse MoE models have been independently explored within the context of conditional computation in both computer vision and natural language processing domains \\citet{jacobs1991adaptive,fedus2022review}. Conditional computation aims to increase the number of model parameters without proportionally increasing computational costs. This is achieved by selectively activating only the relevant parts of the model based on input-specific factors \\cite{shazeer2016outrageously}. Sparse MoE models employ a learned gating mechanism that activates only a subset of experts, specifically \\( k \\) out of \\( N \\) experts, for a given input. This allows for the selection of either all experts or just a sparse mix, optimizing resource usage \\citet{lepikhin2020gshard}.\n\n<PLACEHOLDER_ENV_5>",
        "trans_content": "\\subsection{MoE}\nMoE通过结合多个专家网络并使用门控网络根据给定输入确定哪些专家被激活来优化学习和预测 \\cite{zhang2024scalable,fedus2022switch}。稀疏MoE是MoE模型的一种变体，在每次预测中仅激活少数专家，从而有效利用计算资源并增强可扩展性 \\cite{shazeer2016outrageously}。\n稀疏MoE模型在计算机视觉和自然语言处理领域的条件计算背景下被独立探索 \\citet{jacobs1991adaptive,fedus2022review}。条件计算旨在增加模型参数的数量而不成比例地增加计算成本。这是通过根据输入特定因素有选择地激活模型的相关部分来实现的 \\cite{shazeer2016outrageously}。稀疏MoE模型采用一种学习的门控机制，仅为给定输入激活一部分专家，特别是从\\( N \\)个专家中选择\\( k \\)个。这允许选择所有专家或仅选择稀疏组合，从而优化资源使用 \\citet{lepikhin2020gshard}。\n\n<PLACEHOLDER_ENV_5>"
    },
    {
        "section": "3",
        "content": "\\section{Methodology}",
        "trans_content": "\\section{方法论}"
    },
    {
        "section": "3_1",
        "content": "\\subsection{Preliminaries}\nThroughout this paper, we model the Med-VQA task within a multimodal CoT framework as follows: The framework takes an image \\( I \\) and a question \\( Q \\) as inputs, and outputs a reasoning rationale \\( R \\). This rationale \\( R \\) is subsequently used to generate an answer \\( A \\). This paradigm ensures that the process is transparent, providing a traceable path from input to conclusion, which is essential for both validating the results and improving user trust in the framework's diagnostic capabilities. We can model the Med-VQA task within a multimodal CoT as follows:\n<PLACEHOLDER_ENV_6>\n\n\\( f \\) is responsible for generating a rational and helpful reasoning rationale \\( R \\) (Initial and Follow-up Specialists), while \\( g \\) uses this rationale to generate the final answer \\( A \\) (Diagnostic Specialist). The rationale \\( R \\) is derived from the Initial Specialist assessments and self-reflection by the Follow-up Specialist. The final answer \\( A \\) is determined by a Diagnostic Specialist through a loss function \\( L \\), which measures the discrepancy between the predicted answer \\( A \\) and the true answer \\( A^* \\).",
        "trans_content": "\\subsection{预备知识}\n在整篇论文中，我们将 Med-VQA 任务建模为一个多模态 CoT 框架，如下所示：该框架以图像 \\( I \\) 和问题 \\( Q \\) 作为输入，并输出推理依据 \\( R \\)。这个推理依据 \\( R \\) 随后用于生成答案 \\( A \\)。这种范式确保了过程的透明性，提供了从输入到结论的可追溯路径，这对于验证结果和提高用户对框架诊断能力的信任至关重要。我们可以将 Med-VQA 任务建模为一个多模态 CoT，如下所示：\n<PLACEHOLDER_ENV_6>\n\n函数 \\( f \\) 负责生成合理且有帮助的推理依据 \\( R \\)（初始和后续专家），而函数 \\( g \\) 使用这个推理依据生成最终答案 \\( A \\)（诊断专家）。推理依据 \\( R \\) 来源于初始专家的评估和后续专家的自我反思。最终答案 \\( A \\) 由诊断专家通过损失函数 \\( L \\) 确定，该损失函数衡量预测答案 \\( A \\) 与真实答案 \\( A^* \\) 之间的差异。"
    },
    {
        "section": "3_2",
        "content": "\\subsection{Initial Specialist}\nIn the initial diagnosis phase, we cue the LLMs to act as the primary rationale Diagnostic Specialist.\nWe prompt the LLMs with the instruction:\n\\textit{\"Please proceed with a step-by-step analysis and provide a rationale\"} ($prompt_{\\hat{i}}$).\nThis is done to guide the LLMs in performing a detailed, step-by-step reasoning process.\nThe textual rationale obtained from this is represented as \\( R_{\\hat{i}} = LLMs(T, I, prompt_{\\hat{i}}) \\), where \\( T \\) and \\( I \\) denote the text and image inputs, respectively. \\( T \\) includes textual context such as the question \\( Q \\) and options.\n\\( prompt_{\\hat{i}} \\) is the specific prompting strategy used to elicit the rationale. For further technical details about the prompt, please refer to the appendix.\n\nFor instance, as shown in \\autoref{MedCoT pipeline}, for the question \"Is there a localized mass?\", we obtain a highly interpretable rationale (for the final diagnostic outcome): \"The provided chest X-ray image shows bilateral interstitial infiltrates, which could indicate the presence of a localized mass\".",
        "trans_content": "\\subsection{初始专家}\n在初始诊断阶段，我们指示 LLMs 作为主要推理诊断专家。\n我们向 LLMs 提示以下指令：\n\\textit{\"请进行逐步分析并提供理由\"} ($prompt_{\\hat{i}}$)。\n这样做是为了指导 LLMs 执行详细的、逐步的推理过程。\n从中获得的文本理由表示为 \\( R_{\\hat{i}} = LLMs(T, I, prompt_{\\hat{i}}) \\)，其中 \\( T \\) 和 \\( I \\) 分别表示文本和图像输入。\\( T \\) 包括问题 \\( Q \\) 和选项等文本上下文。\n\\( prompt_{\\hat{i}} \\) 是用于引出理由的特定提示策略。有关提示的更多技术细节，请参阅附录。\n\n例如，如\\autoref{MedCoT pipeline}所示，对于问题 \"Is there a localized mass?\"，我们获得了一个高度可解释的理由（用于最终诊断结果）：\"The provided chest X-ray image shows bilateral interstitial infiltrates, which could indicate the presence of a localized mass\"。"
    },
    {
        "section": "3_3",
        "content": "\\subsection{Follow-up Specialist}\nIn the follow-up diagnosis phase, we instruct LLMs to conduct self-reflection reasoning and test within the problem's context to identify effective rationales, retain them, and reconstruct ineffective ones to generate accurate rationales. Specifically, we prompt the LLMs with:\n\\textit{\"Please judge whether this rationale is effectively valid for the question and image. If it is effective..., If the existing rationale is Ineffective...\"} ($prompt_{\\hat{f}}$). For the complete prompt, please refer to the appendix. We can define the Self-Reflection reasoning of the Follow-up Specialist using the following formula:\n<PLACEHOLDER_ENV_7>\nwhere $R_{\\hat{f}}$ is Follow-up Specialist rationale. This process helps us obtain the textual rationale needed for the diagnostic analysis, as shown in \\autoref{MedCoT pipeline}.\n\nTo infuse the Diagnostic Specialist with more knowledge and bridge the gap between image and text, we utilize the Follow-up Specialist to generate image captions. This process helps to reduce the modality gap, effectively channeling this knowledge into the Diagnostic Specialist. For detailed caption prompts, please refer to the appendix.",
        "trans_content": "\\subsection{后续专家}\n在后续诊断阶段，我们指导LLMs进行自我反思推理，并在问题的背景下进行测试，以识别有效的推理，保留它们，并重构无效的推理以生成准确的推理。具体来说，我们通过以下提示来引导LLMs：\n\\textit{\"请判断这个推理对问题和图像是否有效。如果有效...，如果现有推理无效...\"} ($prompt_{\\hat{f}}$)。完整的提示内容请参见附录。我们可以使用以下公式定义后续专家的自我反思推理：\n<PLACEHOLDER_ENV_7>\n其中 $R_{\\hat{f}}$ 是后续专家的推理。此过程帮助我们获得诊断分析所需的文本推理，如\\autoref{MedCoT pipeline}所示。\n\n为了为诊断专家注入更多知识并弥合图像和文本之间的差距，我们利用后续专家生成图像说明。此过程有助于减少模态差距，有效地将这些知识传递给诊断专家。有关详细的图像说明提示，请参见附录。"
    },
    {
        "section": "3_4",
        "content": "\\subsection{Diagnostic Specialist}\nWe employ the designed model based on multimodal T5 combined with sparse MoE to serve as the Diagnostic Specialist, as shown in \\autoref{Diagnostic}. The Diagnostic Specialist receives enriched textual context and medical imaging information to generate the final diagnostic outcome.",
        "trans_content": "\\subsection{诊断专家}\n我们使用基于多模态T5结合稀疏MoE的设计模型来作为诊断专家，如 \\autoref{Diagnostic} 所示。诊断专家接收丰富的文本上下文和医学影像信息，以生成最终的诊断结果。"
    },
    {
        "section": "3_4_1",
        "content": "\\subsubsection{Multimodal T5}\n\\autoref{Diagnostic} shows the structure of multimodal T5, including the \\textit{TextualEncoder}, \\textit{VisualEncoder}, \\textit{Cross-Attention Network}, sparse MoE, and the \\textit{TextualDecoder}. Here are the network details:\n\n\\textit{TextualEncoder} transforms natural language input \\( {T} \\) into the textual feature space \\( F_T \\in \\mathbb{R}^{n \\times d} \\), and \\textit{VisualEncoder} converts the input image \\( I \\) into visual features \\( F_I \\in \\mathbb{R}^{m \\times d} \\).\nHere, \\( n \\) signifies the length of the input language text, \\( d \\) the dimensionality of hidden features, and \\( m \\) the count of image patches.\nUpon obtaining the textual representation \\( F_T \\) and visual representation \\( F_I \\), our model leverages the \\textit{Cross-Attention Network} for modality interaction. This network computes the attention-guided visual feature \\( H_{V}^{\\text{att}} \\in \\mathbb{R}^{n \\times d} \\), which selectively captures relevant visual features in response to the textual query, as delineated in the operation:\n<PLACEHOLDER_ENV_8>\nwhere $Q$, $K$, $V$ correspond to the query, key, and value, derived from $F_T$, $F_I$, $F_I$, respectively.\n\nOnce the attention-guided visual feature \\( H_{V}^{\\text{att}} \\) and the textual representation \\( F_T \\) are obtained, we construct the MoE to dynamically amalgamate them, resulting in \\( F_F = \\text{MoE} (H_{V}^{\\text{att}}, F_T) \\). Details of the MoE are provided in the following section.\n$F_{\\text{ F}}$ is input into the \\textit{TextualDecoder} to generate answer\n$A = \\text{TextualDecoder}(F_{\\text{F}})$, as shown in \\autoref{Diagnostic}.\n\nIn the training, refinements enable predicted answers (A) to more accurately approximate label answers. Specifically,The model $f$ with input maximizes the likelihood of the correct sequence $Y = {A}$. The loss function $L$, which is the negative log-likelihood over all tokens, is given by: $L = -\\sum_{n=1}^{N} \\log p(Y_n | X, Y_1^{n-1})$, where $N$ is the number of tokens, and $p(Y_n | X, Y_1^{n-1})$ is the probability of predicting the correct $n$-th token in $Y$.",
        "trans_content": "\\subsubsection{多模态 T5}\n\n\\autoref{Diagnostic} 显示了多模态 T5 的结构，包括 \\textit{TextualEncoder}、\\textit{VisualEncoder}、\\textit{Cross-Attention Network}、稀疏 MoE 和 \\textit{TextualDecoder}。以下是网络的详细信息：\n\n\\textit{TextualEncoder} 将自然语言输入 \\( {T} \\) 转换为文本特征空间 \\( F_T \\in \\mathbb{R}^{n \\times d} \\)，而 \\textit{VisualEncoder} 将输入图像 \\( I \\) 转换为视觉特征 \\( F_I \\in \\mathbb{R}^{m \\times d} \\)。\n这里，\\( n \\) 表示输入语言文本的长度，\\( d \\) 表示隐藏特征的维度，\\( m \\) 表示图像块的数量。\n在获得文本表示 \\( F_T \\) 和视觉表示 \\( F_I \\) 后，我们的模型利用 \\textit{Cross-Attention Network} 进行模态交互。该网络计算注意力引导的视觉特征 \\( H_{V}^{\\text{att}} \\in \\mathbb{R}^{n \\times d} \\)，它选择性地捕获响应文本查询的相关视觉特征，如下操作所示：\n<PLACEHOLDER_ENV_8>\n其中 $Q$, $K$, $V$ 分别从 $F_T$, $F_I$, $F_I$ 中导出，分别对应查询、键和值。\n\n一旦获得注意力引导的视觉特征 \\( H_{V}^{\\text{att}} \\) 和文本表示 \\( F_T \\)，我们构建 MoE 动态地将它们结合，得到 \\( F_F = \\text{MoE} (H_{V}^{\\text{att}}, F_T) \\)。MoE 的详细信息将在以下章节提供。\n$F_{\\text{ F}}$ 被输入到 \\textit{TextualDecoder} 中以生成答案\n$A = \\text{TextualDecoder}(F_{\\text{F}})$，如 \\autoref{Diagnostic} 所示。\n\n在训练中，改进使预测答案 (A) 更准确地逼近标签答案。具体而言，输入模型 $f$ 最大化正确序列 $Y = {A}$ 的似然。损失函数 $L$ 是所有标记的负对数似然，给出为：$L = -\\sum_{n=1}^{N} \\log p(Y_n | X, Y_1^{n-1})$，其中 $N$ 是标记的数量，$p(Y_n | X, Y_1^{n-1})$ 是预测 $Y$ 中正确的第 $n$ 个标记的概率。"
    },
    {
        "section": "3_4_2",
        "content": "\\subsubsection{MoE}\n\nIn the multimodal CoT, a crucial step is understanding the intent of both the image and the text and responding accordingly.\nHowever, previous methods primarily utilized gates for integration, where the gate function \\( \\lambda = \\text{Sigmoid}(W_l F_T + W_v H_V^{\\text{att}}) \\) weights the importance of the image relative to the source text, with \\( W_l \\) and \\( W_v \\) as learnable parameters (see Appendix) \\cite{zhang2023multimodal, zheng2023ddcot}.\nWhich, according to our experiments, shows that the gate is insufficient (\\autoref{ablation}).\nTherefore, MedCoT proposes constructing a MoE for the integration process.\n\nThe Sparse MoE implements a top-k sparse mixture of experts \\cite{fedus2022switch}, leveraging multiple Sparse Experts to specialize in processing complex Med-VQA data.\nThis module dynamically selects the top-k experts for each input based on gating scores, as shown in \\autoref{Diagnostic}.\n\nAfter obtaining the outputs from the experts, we use Feature-level Majority Vote to aggregate their outputs.\nThe weight of each expert is calculated using the following formula:\n<PLACEHOLDER_ENV_9>\n\nwhere \\(\\text{W}_i\\) is the weight of the \\(i\\)-th selected expert, and \\(\\text{V}^{\\text{top k}}_i\\) is the score of the \\(i\\)-th selected expert.\nFor each feature \\( F_f \\), the final result of Feature-level Majority Vote is calculated by weighted averaging the outputs of all selected experts:\n<PLACEHOLDER_ENV_10>\nwhere \\({E}_{F_f}\\) is the value of the final result for feature \\( F_f \\), and \\(E_{i, F_f}\\) is the output of the \\(i\\)-th selected expert for feature \\( F_f \\). Then,\n\\(\\lambda = \\text{Sigmoid}({E}_{F_f}) \\).\nFinally, this results in \\( F_f \\) are as follows:\n<PLACEHOLDER_ENV_11>\n\nThe sparse MoE network allows each selected expert to handle data they specialize in, as demonstrated in \\autoref{rad}, which shows experts proficient in addressing head-related issues.\n\n<PLACEHOLDER_ENV_12>\n\n<PLACEHOLDER_ENV_13>",
        "trans_content": "\\subsubsection{MoE}\n\n在多模态CoT中，一个关键步骤是理解图像和文本的意图并做出相应的响应。\n然而，以前的方法主要使用门控来进行整合，其中门控函数 \\( \\lambda = \\text{Sigmoid}(W_l F_T + W_v H_V^{\\text{att}}) \\) 权重衡量图像相对于源文本的重要性，\\( W_l \\) 和 \\( W_v \\) 是可学习的参数（见附录）\\cite{zhang2023multimodal, zheng2023ddcot}。\n根据我们的实验结果显示，门控是不够的（\\autoref{ablation}）。\n因此，MedCoT 提出了构建一个 MoE 用于整合过程。\n\n稀疏 MoE 实现了一个 top-k 的稀疏专家混合 \\cite{fedus2022switch}，利用多个稀疏专家专门处理复杂的 Med-VQA 数据。\n该模块根据门控得分动态选择每个输入的 top-k 专家，如 \\autoref{Diagnostic} 所示。\n\n在获得专家的输出后，我们使用特征级多数投票来聚合它们的输出。\n每个专家的权重使用下列公式计算：\n<PLACEHOLDER_ENV_9>\n\n其中 \\(\\text{W}_i\\) 是第 \\(i\\) 个被选中专家的权重，\\(\\text{V}^{\\text{top k}}_i\\) 是第 \\(i\\) 个被选中专家的得分。\n对于每个特征 \\( F_f \\)，特征级多数投票的最终结果通过加权平均所有选定专家的输出来计算：\n<PLACEHOLDER_ENV_10>\n其中 \\({E}_{F_f}\\) 是特征 \\( F_f \\) 的最终结果值，\\(E_{i, F_f}\\) 是第 \\(i\\) 个选中专家对特征 \\( F_f \\) 的输出。然后，\n\\(\\lambda = \\text{Sigmoid}({E}_{F_f}) \\)。\n最终，这导致 \\( F_f \\) 结果如下：\n<PLACEHOLDER_ENV_11>\n\n稀疏 MoE 网络允许每个选定专家处理他们擅长的数据，如 \\autoref{rad} 所示，展示了擅长解决头部相关问题的专家。\n\n<PLACEHOLDER_ENV_12>\n\n<PLACEHOLDER_ENV_13>"
    },
    {
        "section": "4",
        "content": "\\section{Experiments}",
        "trans_content": "\\section{实验}"
    },
    {
        "section": "4_1",
        "content": "\\subsection{Experimental Setting}\nIn MedCoT framework, the encoder and decoder from Flan-T5 \\cite{khashabi2020unifiedqa,raffel2020exploring} are integrated as TextualEncoder($\\cdot$) and TextualDecoder($\\cdot$), respectively. Additionally, DETR \\cite{carion2020end} is employed as VisualEncoder($\\cdot$).\nOur Diagnostic Specialist model was trained 100 epochs with a learning rate of $8e-5$ and a batch size of 8.\nTo demonstrate the effects of MedCoT, four benchmark datasets are used for validation in the medical VQA domain: VQA-RAD \\cite{lau2018dataset}, SLAKE-EN \\cite{liu2021slake}, Med-VQA-2019 \\cite{abacha2019vqa}, and PathVQA \\cite{he2020pathvqa}, with detailed statistics provided in Appendix.\nAll experiments were conducted using PyTorch \\cite{paszke2019pytorch} and HuggingFace \\cite{wolf2020transformers}, implemented on 4 NVIDIA GEFORCE RTX 3090 GPUs. Accuracy is utilized as the evaluation metric.\nFor LLMs, Gemini Pro 1.5 version is used for our Initial Specialist and Follow-up Specialist. The more experimental details can be found in the Appendix.\n\n<PLACEHOLDER_ENV_14>\n\n<PLACEHOLDER_ENV_15>",
        "trans_content": "\\subsection{实验设置}\n在MedCoT框架中，Flan-T5 \\cite{khashabi2020unifiedqa,raffel2020exploring}的编码器和解码器分别被集成为TextualEncoder($\\cdot$)和TextualDecoder($\\cdot$)。此外，DETR \\cite{carion2020end}被用作VisualEncoder($\\cdot$)。\n我们的诊断专家模型经过100个epoch的训练，学习率为$8e-5$，批量大小为8。\n为了展示MedCoT的效果，我们在医学VQA领域使用了四个基准数据集进行验证：VQA-RAD \\cite{lau2018dataset}，SLAKE-EN \\cite{liu2021slake}，Med-VQA-2019 \\cite{abacha2019vqa}，和PathVQA \\cite{he2020pathvqa}。详细统计信息见附录。\n所有实验都是使用PyTorch \\cite{paszke2019pytorch}和HuggingFace \\cite{wolf2020transformers}进行的，具体实现是在4个NVIDIA GEFORCE RTX 3090 GPU上完成的。准确率被用作评估指标。\n对于LLMs，我们的初步专家和后续专家使用Gemini Pro 1.5版本。更多的实验细节可以在附录中找到。\n\n<PLACEHOLDER_ENV_14>\n\n<PLACEHOLDER_ENV_15>"
    },
    {
        "section": "4_2",
        "content": "\\subsection{Main Results}\nWe evaluate the performance of MedCoT on the VQA-RAD and SLAKE-EN datasets, benchmarking them against established models like MEVF \\cite{nguyen2019overcoming}, MMBERT \\cite{tiong-etal-2022-plug}, PubMedCLIP \\cite{eslami-etal-2023-pubmedclip}, VQA-Adapter \\cite{liu2023parameter}, MedThink \\cite{gai2024medthink}, LLaVA-Med \\cite{li2024llava}.\n\nOur performance evaluation is divided into two parts, focusing separately on closed-end and open-end questions. Closed-end questions, structured as multiple-choice questions with a single correct answer, are assessed using accuracy as the performance metric, as shown in \\autoref{performance}.\nIn facing closed-end questions,\nMedCoT surpasses a range of SoTA methods on the VQA-RAD and SLAKE-EN datasets. Notably, MedCoT achieved improvements of 27.21\\% and 14.66\\% over Gemini on the two datasets, demonstrating the unreliability of a single model. Besides, MedCoT, with a fine-tuning size of approximately 256M parameters, outperforms the 7B parameter LLaVA-Med (trained on extensive medical data), exceeding it by 5.52\\% and 4.09\\% on two datasets, respectively. Moreover, compared to previous methods, MedCoT clearly displays the reasoning paths (rationale), as illustrated in \\autoref{MedCoT pipeline}.\nMore comparative method results can be seen in  Appendix.\n\nIn contrast, open-end questions allow for a range of answers due to their inherent nature. The answers generated by MedCoT are difficult to match precisely against the dataset. Therefore, we employ text generation metrics such as Rouge and BLEU to evaluate MedCoT's performance.\nWe conducted experiments on the open-end VQA-RAD and SLAKE-EN, with results shown in the Appendix.\nMedCoT demonstrated higher Rouge and BLEU scores on the VQA-RAD and SLAKE-EN dataset, surpassing MedThink \\cite{gai2024medthink}.\nBesides, MedCoT also showed higher scores on the SLAKE-EN.\n\nAdditionally, we evaluated MedCoT's performance on the Med-VQA-2019 and PathVQA datasets, as shown in Appendix. The results indicate that MedCoT consistently achieves SoTA results compared to the majority of SoTA methods.\n\n<PLACEHOLDER_ENV_16>",
        "trans_content": "\\subsection{主要结果}\n我们在 VQA-RAD 和 SLAKE-EN 数据集上评估 MedCoT 的性能，并与 MEVF \\cite{nguyen2019overcoming}、MMBERT \\cite{tiong-etal-2022-plug}、PubMedCLIP \\cite{eslami-etal-2023-pubmedclip}、VQA-Adapter \\cite{liu2023parameter}、MedThink \\cite{gai2024medthink}、LLaVA-Med \\cite{li2024llava} 等已建立的模型进行基准测试。\n\n我们的性能评估分为两个部分，分别关注封闭式问题和开放式问题。封闭式问题结构为单项选择题，使用准确率作为性能指标进行评估，如 \\autoref{performance} 所示。面对封闭式问题，MedCoT 在 VQA-RAD 和 SLAKE-EN 数据集上超越了一系列 SoTA 方法。值得注意的是，MedCoT 在这两个数据集上分别比 Gemini 提高了 27.21\\% 和 14.66\\%，表明单一模型的不可靠性。此外，MedCoT 的微调参数大小约为 256M，优于经过大量医学数据训练的 7B 参数 LLaVA-Med，在两个数据集上分别超过了 5.52\\% 和 4.09\\%。此外，与之前的方法相比，MedCoT 清晰地展示了推理路径（理由），如 \\autoref{MedCoT pipeline} 所示。更多的比较方法结果可以在附录中看到。\n\n相比之下，开放式问题由于其固有的特性允许多种答案。MedCoT 生成的答案很难与数据集精确匹配。因此，我们使用文本生成指标，如 Rouge 和 BLEU 来评估 MedCoT 的性能。我们在开放式 VQA-RAD 和 SLAKE-EN 上进行了实验，结果见附录。MedCoT 在 VQA-RAD 和 SLAKE-EN 数据集上显示出更高的 Rouge 和 BLEU 得分，超越了 MedThink \\cite{gai2024medthink}。此外，MedCoT 在 SLAKE-EN 上也显示出更高的得分。\n\n另外，我们还评估了 MedCoT 在 Med-VQA-2019 和 PathVQA 数据集上的性能，如附录所示。结果表明，MedCoT 相较于大多数 SoTA 方法始终实现 SoTA 结果。\n\n<PLACEHOLDER_ENV_16>"
    },
    {
        "section": "4_3",
        "content": "\\subsection{Ablation Study}\n\\label{ablation}\n\\noindent\\textbf{Effects of Follow-up Specialist}\nTo validate the effectiveness of the Follow-up Specialist, we compared the results of experiments involving only the initial and diagnostic specialists with those from the complete MedCoT. As shown in \\autoref{xiaorong}, across two medical datasets, there is a significant performance loss when the Follow-up Specialist is removed. For instance, on the VQA-RAD dataset, performance dropped from 87.50\\% to 80.88\\%, a decrease of 6.62\\%. This demonstrates the effectiveness of the Follow-up Specialist.\n\nBesides, we conducted experiments involving only the initial and diagnostic specialists,\nbypassing the self-reflection of the Follow-up Specialist.\nIn all cases involving varying numbers of experts, the results without the self-reflection were consistently lower than those with rationales refined by the Follow-up Specialist’s reflection,  and even lower than those from a Diagnostic Specialist that had undergone self-reflection but was lacking the MoE component, as shown in \\autoref{zhexiantu-all}.\nThis underscores the importance of the self-reflection provided by the Follow-up Specialist.\nAdditionally, we conducted zero-shot experiments using both the initial and Follow-up Specialist. As shown in the appendix, these results further confirm the effectiveness of the Follow-up Specialist.\n\n\\noindent\\textbf{Effects of MoE}\nTo validate the effectiveness of the MoE, we compared the performance with and without the MoE. As shown in \\autoref{xiaorong}, there is a significant performance drop across all datasets without MoE. For instance, in the VQA-RAD, the performance decreased from 87.50\\% to 82.72\\%, a loss of 4.78\\%. This indicates that MoE plays a crucial role in Diagnostic Specialist.\nAs can also be seen from \\autoref{zhexiantu-all}, lacking MoE, in most expert number scenarios, the performance is weaker compared to MedCoT equipped with Sparse MoE.\n\nAdditionally, we conducted experiments for each organ-related question category within the VQA-RAD and SLAKE-EN, as shown in \\autoref{rad}.\nIt is evident that in the majority of organ-related questions, methods employing MoE outperform those using the gating mechanism.\nNotably, the Gate mechanism, resembling as a single-expert system, tends to falter with head-related questions, where it performs the worst. For such questions in the VQA-RAD, methods using MoE exceeded those with gates by 10\\%, further emphasizing MoE's effectiveness.\nWe visualized the weights of MoE, as shown in the \\autoref{rad} (right figure), revealing that Experts 0 and 5 primarily handle head-related issues. This demonstrates that these two experts dynamically process and understand the intents of medical images and texts more effectively than the gating.\nSimilar results can also be observed in the experiments conducted on the SLAKE-EN, as shown in Appendix.\n\n\\noindent\\textbf{Grid Search}\nWe conducted a parameter search experiment for the hyperparameters in the sparse MoE, such as the number of experts and the \\( k \\) value. The results are shown in Appendix. The experiment revealed that the optimal number of experts varies for different datasets. Specifically, the best number of experts for VQA-RAD, SLAKE-EN, Med-2019 and PathVQA are 6, 10, 5, and 5, respectively. Regarding the \\( k \\) value, the optimal value for all datasets was consistently 2, as illustrated in Appendix.",
        "trans_content": "\\subsection{消融研究}\n\\label{ablation}\n\\noindent\\textbf{后续专家的效果}\n为了验证后续专家的有效性，我们将仅包含初始和诊断专家的实验结果与完整的MedCoT进行了对比。正如在\\autoref{xiaorong}中所示，在两个医学数据集上，当移除后续专家时，性能显著下降。例如，在VQA-RAD数据集中，性能从87.50\\%下降到80.88\\%，下降了6.62\\%。这证明了后续专家的有效性。\n\n此外，我们进行了仅包含初始和诊断专家的实验，绕过了后续专家的自我反思。在所有涉及不同数量专家的情况下，没有自我反思的结果始终低于经过后续专家反思优化的理由，甚至低于经过自我反思但缺乏MoE组件的诊断专家的结果，如\\autoref{zhexiantu-all}所示。这强调了后续专家提供的自我反思的重要性。此外，我们还进行了使用初始和后续专家的零样本实验。正如附录中所示，这些结果进一步确认了后续专家的有效性。\n\n\\noindent\\textbf{MoE的效果}\n为了验证MoE的有效性，我们比较了有无MoE的性能。正如\\autoref{xiaorong}中所示，所有数据集中没有MoE时性能显著下降。例如，在VQA-RAD中，性能从87.50\\%下降到82.72\\%，损失了4.78\\%。这表明MoE在诊断专家中起着关键作用。从\\autoref{zhexiantu-all}中也可以看出，在大多数专家数量的场景中，缺乏MoE的性能弱于配备稀疏MoE的MedCoT。\n\n此外，我们对VQA-RAD和SLAKE-EN中与器官相关的问题类别进行了实验，如\\autoref{rad}所示。显然，在大多数器官相关的问题中，使用MoE的方法优于使用门控机制的方法。值得注意的是，门控机制类似于单专家系统，它在处理头部相关问题时往往表现不佳。在VQA-RAD中的此类问题中，使用MoE的方法比使用门控的方法高出10\\%，进一步强调了MoE的有效性。我们可视化了MoE的权重，如\\autoref{rad}（右图）所示，显示专家0和5主要处理头部相关问题。这表明这两个专家比门控更有效地动态处理和理解医学图像和文本的意图。在SLAKE-EN上的实验中也可以观察到类似的结果，如附录中所示。\n\n\\noindent\\textbf{网格搜索}\n我们对稀疏MoE中的超参数进行了参数搜索实验，如专家数量和\\( k \\)值。结果如附录中所示。实验表明，不同数据集的最佳专家数量是不同的。具体而言，VQA-RAD、SLAKE-EN、Med-2019和PathVQA的最佳专家数量分别为6、10、5和5。对于\\( k \\)值，所有数据集的最佳值均为2，如附录中所示。"
    },
    {
        "section": "4_4",
        "content": "\\subsection{Discussion}\n\n\\autoref{MedCoT pipeline} and \\autoref{case1} illustrate cases where the Initial Specialist provides a rationale, the Follow-up Specialist makes corrections, and the Diagnostic Specialist delivers the final, accurate diagnosis. For instance, in \\autoref{case1}, the Initial Specialist, influenced by the illusions of the LLMs, mistakenly observes non-existent brain fluid and diagnoses the brain as being affected by gyri. However, after the self-reflection by the Follow-up Specialist, it is clarified that no clear fluid was observed. Ultimately, the Diagnostic Specialist, using the rationale from the Follow-up Specialist and considering the full context, arrives at the correct diagnosis.\n\nAppendix provides an example where the limitations of LLMs affect the ability to accurately diagnose certain cases.\nThe question posed is whether there is pneumomediastinum. The Initial Specialist, based on observations, affirms its presence, and the Follow-up Specialist concurs, leading to a unanimous agreement. However, due to the limitations of the LLMs, these rationales are incorrect, ultimately leading to an erroneous answer.",
        "trans_content": "\\subsection{讨论}\n\n\\autoref{MedCoT pipeline} 和 \\autoref{case1} 展示了初级专家提供理由、随访专家进行纠正、诊断专家给出最终准确诊断的案例。例如，在 \\autoref{case1} 中，初级专家受 LLMs 的错觉影响，错误地观察到不存在的脑液，并诊断大脑受到脑回的影响。然而，经过随访专家的自我反思，澄清并未观察到清晰的液体。最终，诊断专家利用随访专家的理由并考虑完整的上下文，得出了正确的诊断。\n\n附录提供了一个例子，说明 LLMs 的局限性如何影响准确诊断某些病例的能力。提出的问题是是否存在纵隔积气。初级专家基于观察确认其存在，随访专家也表示同意，导致一致的意见。然而，由于 LLMs 的局限性，这些理由是不正确的，最终导致了错误的答案。"
    },
    {
        "section": "5",
        "content": "\\section{Conclusion}\nIn this paper, we propose an effective hierarchical expert reasoning chain method for Med-VQA, named MedCoT. This method is based on two insights: 1) Med-VQA should have a clear reasoning path; 2) Med-VQA scenarios should be reviewed by multiple experts to arrive at a conclusion. Specifically, the process involves initial experts providing preliminary diagnostic rationales based on medical visual questions. Follow-up experts then review these rationales for validity, retaining the effective ones and reassessing the ineffective ones. Finally, a locally deployed Diagnostic Specialist, consisting of a sparse MoE that conducts a vote, then provides the definitive diagnosis. Experimental results on multiple Med-VQA datasets show that MedCoT outperforms existing SoTA techniques, significantly surpasses recent methods, and demonstrates excellent interpretability for final diagnosis.\n\n\\newpage",
        "trans_content": "\\section{结论}\n在本文中，我们提出了一种有效的层次化专家推理链方法，用于医学视觉问答（Med-VQA），称为MedCoT。该方法基于两个见解：1）医学视觉问答应有明确的推理路径；2）医学视觉问答场景应由多位专家审阅以得出结论。具体而言，该过程涉及初始专家根据医学视觉问题提供初步诊断理由。后续专家对这些理由进行有效性审查，保留有效的，重新评估无效的。最后，由本地部署的诊断专家组成的稀疏MoE进行投票，然后提供最终诊断。多个医学视觉问答数据集的实验结果表明，MedCoT优于现有的最先进技术，显著超越了近期方法，并在最终诊断中表现出卓越的可解释性。\n\n\\newpage"
    },
    {
        "section": "6",
        "content": "\\section*{Limitation}\nA limitation is that the performance of MedCoT is influenced by the hallucinations of the LLMs used by the Initial and Follow-up Specialist.\nAlthough self-reflection and Hierarchical Expert design can mitigate some issues with LLMs' hallucinations, it must be acknowledged that the problem is not completely resolved. As shown in Appendix, MedCoT is still susceptible to hallucination risks. Researching methods to suppress hallucinations is a potential topic for further study.\nIn this work, the Gemini-Pro model was employed. If Med-Gemini becomes available, MedCoT could be further enhanced.\nMoreover, MedCoT could inspire future paradigms that integrate proprietary commercial LLMs with local models.\nBy utilizing desensitized information to prompt the extensive knowledge and reasoning capabilities of LLMs, the generated rationales could be combined with local models for further diagnostic analysis, enhancing both interpretability and accuracy.\n\nAnother limitation is that compared to single-model methods, MedCoT may be more time-consuming. However, the hierarchical expert approach aligns more closely with real-world medical diagnostics and provides clear diagnostic pathways as well as more accurate answers, making the additional time worthwhile.",
        "trans_content": "\\section*{局限性}\n一个局限性是，MedCoT 的性能受到初始和后续专家所使用的大型语言模型（LLM）幻觉的影响。尽管自我反思和分层专家设计可以减轻部分 LLM 幻觉问题，但必须承认问题尚未完全解决。如附录所示，MedCoT 仍然容易受到幻觉风险的影响。研究抑制幻觉的方法是进一步研究的一个潜在课题。在这项工作中，使用了 Gemini-Pro 模型。如果 Med-Gemini 可用，MedCoT 可以进一步增强。此外，MedCoT 可能会激发未来将专有商业 LLM 与本地模型集成的范式。通过利用去敏感化信息来提示 LLM 的广泛知识和推理能力，生成的推理可以与本地模型结合进行进一步的诊断分析，提高解释性和准确性。\n\n另一个局限性是，与单模型方法相比，MedCoT 可能更耗时。然而，分层专家方法与现实世界的医疗诊断更加契合，并提供了明确的诊断路径以及更准确的答案，使得额外的时间是值得的。"
    },
    {
        "section": "7",
        "content": "\\section*{Acknowledgements}\nThis work is supported by the National Natural Science Foundation of China (Grant No. 62106222), the Natural Science Foundation of Zhejiang Province, China (Grant No. LZ23F020008), and the Zhejiang University-Angelalign Inc. R\\&D Center for Intelligent Healthcare.\nThis work is also supported by Jiawei Du’s A*STAR Career Development Fund (CDF) C233312004.\n\n\\bibliography{main}\n\\newpage\n\n\\end{document}",
        "trans_content": "\\section*{致谢}\n本工作得到了中国国家自然科学基金（资助号：62106222）、中国浙江省自然科学基金（资助号：LZ23F020008）以及浙江大学-时代天使智能健康研发中心的支持。此项工作还得到了杜嘉伟的A*STAR职业发展基金（CDF）C233312004的支持。\n\n\\bibliography{main}\n\\newpage\n\n\\end{document}"
    }
]